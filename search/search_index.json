{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#cell-spotter-cspot-a-scalable-machine-learning-framework-for-automated-processing-of-highly-multiplexed-tissue-images","title":"CELL SPOTTER (CSPOT): A scalable machine learning framework for automated processing of highly multiplexed tissue images","text":"<p>Highly multiplexed tissue imaging and in situ spatial profiling aims to extract single-cell data from specimens containing closely packed cells having diverse morphologies. This is a challenging problem due to the difficulty of accurately assigning boundaries between cells (the process of segmentation) and then integrating per-cell staining intensities. In addition, existing methods use gating to assign positive and negative scores to individual scores, a common approach in flow cytometry but one that is restrictive in high-resolution imaging. In contrast, human experts identify cells in crowded environments using morphological, neighborhood, and intensity information. Here we describe a computational approach (Cell Spotter or CSPOT) that uses supervised machine learning in combination with classical segmentation to combine human visual review and computation for automated cell type calling.  The end-to-end Python implementation of CSPOT can be integrated into cloud-based image processing pipelines and substantially improves the speed, accuracy, and reproducibility of single-cell spatial data.</p>"},{"location":"Getting%20Started/","title":"\ud83d\udc0a Getting Started with CSPOT","text":"<p>Kindly note that CSPOT is not a plug-and-play solution. It's a framework that requires significant upfront investment of time from potential users for training and validating deep learning models, which can then be utilized in a plug-and-play manner for processing large volumes of similar multiplexed imaging data.</p> <p>There are two ways to set it up based on how you would like to run the program - Using an interactive environment like Jupyter Notebooks - Using Command Line Interface  </p> <p>Before we set up CSPOT, we highly recommend using a environment manager like Conda. Using an environment manager like Conda allows you to create and manage isolated environments with specific package versions and dependencies. </p> <p>Download and Install the right conda based on the opertating system that you are using</p>"},{"location":"Getting%20Started/#create-a-new-conda-environment","title":"Create a new conda environment","text":"<pre><code># use the terminal (mac/linux) and anaconda promt (windows) to run the following command\nconda create --name cspot -y python=3.9\nconda activate cspot\n</code></pre> <p>Install <code>cspot</code> within the conda environment.</p> <pre><code>pip install cspot\n</code></pre>"},{"location":"Getting%20Started/#interactive-mode","title":"Interactive Mode","text":"<p>Using IDE or Jupyter notebooks</p> <pre><code>pip install notebook\n\n# open the notebook and import CSPOT\nimport cspot as cs\n# Go to the tutorial section to follow along\n</code></pre>"},{"location":"Getting%20Started/#command-line-interface","title":"Command Line Interface","text":"<pre><code>wget https://github.com/nirmalLab/cspot/archive/main.zip\nunzip main.zip \ncd cspot-main/cspot \n# Go to the tutorial section to follow along\n</code></pre>"},{"location":"Getting%20Started/#docker-container","title":"Docker Container","text":"<pre><code>docker pull nirmallab/cspot:cspot\n# Go to the tutorial section to follow along\n</code></pre>"},{"location":"Functions/addPredictions/","title":"addPredictions","text":"<p>Short Description</p> <p>The <code>addPredictions</code> function serves as a link between <code>cspot</code> and <code>scimap</code> package.  It's useful for evaluating model performance. The function transforms results  stored in <code>anndata.uns</code> to <code>anndata.obs</code> so they can be visualized using  the <code>scimap</code> package's <code>sm.pl.image viewer</code> function. This displays <code>positive</code>  and <code>negative</code> cells overlaid on the raw image.</p> <p>The <code>addPredictions</code> function can take in two methods.  <code>cspotOutput</code> displays the result of running the <code>cspot</code> function,  while <code>csScore</code> shows the raw output produced by the <code>csScore</code>  function, which returns a probability score. The <code>midpoint</code> parameter,  with a default value of 0.5, can be adjusted to define what is  considered a <code>positive</code> result, when method is set to <code>csScore</code>.</p>"},{"location":"Functions/addPredictions/#cspot.addPredictions--function","title":"Function","text":""},{"location":"Functions/addPredictions/#cspot.addPredictions.addPredictions","title":"<code>addPredictions(csObject, method='cspotOutput', cspotOutput='cspotOutput', csScore='csScore', midpoint=0.5, outputDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObject</code> <code>anndata</code> <p>Single or combined CSPOT object.</p> required <code>method</code> <code>str</code> <p>There are two options: <code>cspotOutput</code> and <code>csScore</code>.  <code>cspotOutput</code> displays the result of running the <code>CSPOT</code> function,  while <code>csScore</code> shows the raw output produced by the <code>csScore</code>  function, which returns a probability score. The <code>midpoint</code> parameter,  with a default value of 0.5, can be adjusted to define what is  considered a <code>positive</code> result, when method is set to <code>csScore</code>.</p> <code>'cspotOutput'</code> <code>cspotOutput</code> <code>str</code> <p>The name under which the <code>cspotOutput</code> is stored.</p> <code>'cspotOutput'</code> <code>csScore</code> <code>str</code> <p>The name under which the <code>csScore</code> is stored.</p> <code>'csScore'</code> <code>midpoint</code> <code>float</code> <p>The threshold for determining positive cells, in conjunction with 'csScore'.</p> <code>0.5</code> <code>outputDir</code> <code>string</code> <p>Provide the path to the output directory. Kindly take note that this particular  output will not be automatically saved in a predetermined directory,  unlike the other outputs. The file will be saved in the directory  specified by the <code>outputDir</code> parameter. If <code>None</code>, the <code>csObject</code> will  be returned to memory.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>If output directory is provided the <code>csObject</code> will  be stored else it will be returned to memory. The results are stored in  <code>anndata.obs</code> with a <code>p_</code> appended to the markers names. So if you would  like to vizulaize <code>CD3</code>, the column that you are looking for is <code>p_CD3</code>.</p> Example <pre><code># Path to projectDir\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# Path to csObject\ncsObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\nadata = cs.addPredictions (csObject, \n                    method='cspotOutput',\n                    cspotOutput='cspotOutput',\n                    csScore='csScore', \n                    midpoint=0.5)\n\n# Same function if the user wants to run it via Command Line Interface\npython addPredictions.py             --csObject Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad        \n</code></pre> Source code in <code>cspot/addPredictions.py</code> <pre><code>def addPredictions (csObject, \n                    method='cspotOutput',\n                    cspotOutput='cspotOutput',\n                    csScore='csScore', \n                    midpoint=0.5,\n                    outputDir=None):\n    \"\"\"\nParameters:\n    csObject (anndata):  \n        Single or combined CSPOT object.\n\n    method (str, optional):  \n        There are two options: `cspotOutput` and `csScore`. \n        `cspotOutput` displays the result of running the `CSPOT` function, \n        while `csScore` shows the raw output produced by the `csScore` \n        function, which returns a probability score. The `midpoint` parameter, \n        with a default value of 0.5, can be adjusted to define what is \n        considered a `positive` result, when method is set to `csScore`.\n\n    cspotOutput (str, optional):  \n        The name under which the `cspotOutput` is stored.\n\n    csScore (str, optional):  \n        The name under which the `csScore` is stored.\n\n    midpoint (float, optional):  \n        The threshold for determining positive cells, in conjunction with 'csScore'.\n\n    outputDir (string, optional):  \n        Provide the path to the output directory. Kindly take note that this particular \n        output will not be automatically saved in a predetermined directory, \n        unlike the other outputs. The file will be saved in the directory \n        specified by the `outputDir` parameter. If `None`, the `csObject` will \n        be returned to memory.\n\nReturns:\n    csObject (anndata):  \n        If output directory is provided the `csObject` will \n        be stored else it will be returned to memory. The results are stored in \n        `anndata.obs` with a `p_` appended to the markers names. So if you would \n        like to vizulaize `CD3`, the column that you are looking for is `p_CD3`.\n\nExample:\n    \t```python    \n\n        # Path to projectDir\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # Path to csObject\n        csObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n        adata = cs.addPredictions (csObject, \n                            method='cspotOutput',\n                            cspotOutput='cspotOutput',\n                            csScore='csScore', \n                            midpoint=0.5)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python addPredictions.py \\\n            --csObject Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad    \t\n        ```\n\n    \"\"\"\n    # Load the adata\n    if isinstance(csObject, str):\n        adata = ad.read(csObject)\n    else: \n        adata = csObject\n\n    # function to convert the prob scores to binary pos or neg\n    def assign_labels(df, midpoint):\n        df = df.applymap(lambda x: 'neg' if x &lt; midpoint else 'pos')\n        return df\n\n    # intialize the data    \n    if method == 'cspotOutput':\n        attach_df = adata.uns[cspotOutput].copy()\n    elif method == 'csScore':\n        df = adata.uns[csScore].copy()\n        attach_df = assign_labels (df, midpoint=midpoint)\n\n\n    obs = adata.obs.copy()\n    columns_to_drop = [col for col in obs.columns if col.startswith('p_')]\n    obs.drop(columns_to_drop, axis=1, inplace=True)\n\n    new_col_names = ['p_{}'.format(idx) for idx in attach_df.columns]\n    attach_df.columns = new_col_names\n    # add to obs\n    final_obs = pd.concat([obs, attach_df], axis=1)\n    adata.obs = final_obs\n\n\n    # Return to adata\n    # Save data if requested\n    if outputDir is not None:    \n        finalPath = pathlib.Path(outputDir)     \n        if not os.path.exists(finalPath):\n            os.makedirs(finalPath)\n        # determine file name\n        if isinstance (csObject, str):\n            imid = pathlib.Path(csObject).stem\n        else:\n            imid = 'addPredictions'    \n        adata.write(finalPath / f'{imid}.h5ad')\n    else:\n        # Return data\n        return adata\n</code></pre>"},{"location":"Functions/cloneFolder/","title":"cloneFolder","text":"<p>Short Description</p> <p>The purpose of the <code>cloneFolder</code> function is to copy user actions from one  folder to another. For example, if a user manually arranges thumbnails in  the <code>localNorm</code> folder, this function can replicate those changes to the  raw thumbnails.</p>"},{"location":"Functions/cloneFolder/#cspot.cloneFolder--function","title":"Function","text":""},{"location":"Functions/cloneFolder/#cspot.cloneFolder.cloneFolder","title":"<code>cloneFolder(copyFolder, applyFolder, TruePos='TruePos', TrueNeg='TrueNeg', PosToNeg='PosToNeg', NegToPos='NegToPos', verbose=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>copyFolder</code> <code>list</code> <p>List of folders from which the user wants to replicate the file structure.</p> required <code>applyFolder</code> <code>list</code> <p>List of folders where the replicated file structure should be applied, in the same order as the <code>copyFolder</code> list.</p> required <code>TruePos</code> <code>str</code> <p>Name of the folder that holds the Thumbnails classified as True Positive.</p> <code>'TruePos'</code> <code>TrueNeg</code> <code>str</code> <p>Name of the folder that holds the Thumbnails classified as True Negative.</p> <code>'TrueNeg'</code> <code>PosToNeg</code> <code>str</code> <p>Name of the folder that holds the Thumbnails that were moved from <code>True Positive</code> to <code>True Negative</code>.</p> <code>'PosToNeg'</code> <code>NegToPos</code> <code>str</code> <p>Name of the folder that holds the Thumbnails that were moved from <code>True Negative</code> to <code>True Positive</code>.</p> <code>'NegToPos'</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <p>Returns:</p> Name Type Description <code>folder</code> <code>cloned folders</code> <p>The file structure of the source Folder is replicated in the destination Folder.</p> Example <pre><code># High level working directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# list of folders to copy settings from\ncopyFolder = [projectDir + '/CSPOT/Thumbnails/localNorm/CD3D',\n              projectDir + '/CSPOT/Thumbnails/localNorm/ECAD']\n# list of folders to apply setting to\napplyFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n               projectDir + '/CSPOT/Thumbnails/ECAD']\n# note: Every copyFolder should have a corresponding applyFolder. The order matters! \n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\ncs.cloneFolder (copyFolder, \n                applyFolder, \n                TruePos='TruePos', TrueNeg='TrueNeg', \n                PosToNeg='PosToNeg', NegToPos='NegToPos')\n\n\n# Same function if the user wants to run it via Command Line Interface\npython cloneFolder.py             --copyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/ECAD             --applyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD\n</code></pre> Source code in <code>cspot/cloneFolder.py</code> <pre><code>def cloneFolder (copyFolder, \n                 applyFolder,\n                 TruePos='TruePos', \n                 TrueNeg='TrueNeg',\n                 PosToNeg='PosToNeg', \n                 NegToPos='NegToPos',\n                 verbose=True):\n    \"\"\"\nParameters:\n    copyFolder (list):\n        List of folders from which the user wants to replicate the file structure.\n\n    applyFolder (list):\n        List of folders where the replicated file structure should be applied,\n        in the same order as the `copyFolder` list.\n\n    TruePos (str, optional):\n        Name of the folder that holds the Thumbnails classified as True Positive.\n\n    TrueNeg (str, optional):\n        Name of the folder that holds the Thumbnails classified as True Negative.\n\n    PosToNeg (str, optional):\n        Name of the folder that holds the Thumbnails that were moved from `True Positive`\n        to `True Negative`.\n\n    NegToPos (str, optional):\n        Name of the folder that holds the Thumbnails that were moved from `True Negative`\n        to `True Positive`.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\nReturns:\n    folder (cloned folders):  \n        The file structure of the source Folder is replicated in the destination Folder.\n\nExample:\n        ```python\n\n        # High level working directory\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # list of folders to copy settings from\n        copyFolder = [projectDir + '/CSPOT/Thumbnails/localNorm/CD3D',\n                      projectDir + '/CSPOT/Thumbnails/localNorm/ECAD']\n        # list of folders to apply setting to\n        applyFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                       projectDir + '/CSPOT/Thumbnails/ECAD']\n        # note: Every copyFolder should have a corresponding applyFolder. The order matters! \n\n        # The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n        cs.cloneFolder (copyFolder, \n                        applyFolder, \n                        TruePos='TruePos', TrueNeg='TrueNeg', \n                        PosToNeg='PosToNeg', NegToPos='NegToPos')\n\n\n        # Same function if the user wants to run it via Command Line Interface\n        python cloneFolder.py \\\n            --copyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/ECAD \\\n            --applyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD\n\n        ```\n\n    \"\"\"\n\n    #TruePos='TruePos'; TrueNeg='TrueNeg'; PosToNeg='PosToNeg'; NegToPos='NegToPos'\n\n    # Convert the path to list\n    if isinstance (copyFolder, str):\n        copyFolder = [copyFolder]\n    if isinstance (applyFolder, str):\n        applyFolder = [applyFolder]\n\n    # Quick Check!\n    if len(copyFolder) is not len(applyFolder):\n        raise ValueError('The number of copyFolder and applyFolder should match, please check!' )\n\n    # function to delete images\n    def deleteFile(files, location):\n        for f in files:\n            # full path\n            #full_path = location  + f\n            full_path = pathlib.Path.joinpath(location, f)\n            if os.path.exists(full_path):\n                os.remove(full_path)\n\n    # Function to move images\n    def moveFile(files, from_loc, to_loc):\n        for f in files:\n            # full path\n            full_path_from = pathlib.Path.joinpath(from_loc, f) # from_loc + f\n            full_path_to = pathlib.Path.joinpath(to_loc, f) # to_loc + f\n            # move file\n            if os.path.exists(full_path_from):\n                shutil.move(full_path_from, full_path_to)\n\n    # path lib of all folder\n    all_folders = [pathlib.Path(p) for p in copyFolder]\n\n    # copy from location\n    pos_aug_location = [pathlib.Path(p + '/' + str(TruePos)) for p in copyFolder]\n    neg_aug_location = [pathlib.Path(p + '/' + str(TrueNeg)) for p in copyFolder]\n    pos2neg_aug_location = [pathlib.Path(p + '/' + str(PosToNeg)) for p in copyFolder]\n    neg2pos_aug_location = [pathlib.Path(p + '/' + str(NegToPos)) for p in copyFolder]\n\n    # copy to location\n    pos_real_location = [pathlib.Path(p + '/' + str(TruePos)) for p in applyFolder]\n    neg_real_location = [pathlib.Path(p + '/' + str(TrueNeg)) for p in applyFolder]\n    pos2neg_real_location = [pathlib.Path(p + '/' + str(PosToNeg)) for p in applyFolder]\n    neg2pos_real_location = [pathlib.Path(p + '/' + str(NegToPos)) for p in applyFolder]\n\n\n\n    # function\n    def processFolder (folderIndex):\n        if verbose is True:\n            print ('Processing: ' + str(all_folders[folderIndex].stem))\n\n        # create a list of all file names in the applyFolder\n        pos_files = next(walk(pos_real_location[folderIndex]), (None, None, []))[2]\n        neg_files = next(walk(neg_real_location[folderIndex]), (None, None, []))[2]\n\n        # Find file names within each of the copyFolder\n        pos = next(walk(pos_aug_location[folderIndex]), (None, None, []))[2]\n        neg = next(walk(neg_aug_location[folderIndex]), (None, None, []))[2]\n        pos2neg = next(walk(pos2neg_aug_location[folderIndex]), (None, None, []))[2]\n        neg2pos = next(walk(neg2pos_aug_location[folderIndex]), (None, None, []))[2]  # [] if no file\n\n        # Find images to delete\n        pos_del = list(set(pos_files).difference(pos + pos2neg))\n        neg_del = list(set(neg_files).difference(neg + neg2pos))\n\n        # delete files\n        deleteFile(files=pos_del, location=pos_real_location[folderIndex])\n        deleteFile(files=neg_del, location=neg_real_location[folderIndex])\n\n        # move files\n        moveFile (files=pos2neg, from_loc=pos_real_location[folderIndex], to_loc=pos2neg_real_location[folderIndex])\n        moveFile (files=neg2pos, from_loc=neg_real_location[folderIndex], to_loc=neg2pos_real_location[folderIndex])\n\n        # print the number of files\n        posaug = len(next(walk(pos_aug_location[folderIndex]), (None, None, []))[2])\n        posreal = len(next(walk(pos_real_location[folderIndex]), (None, None, []))[2])\n        negaug = len(next(walk(neg_aug_location[folderIndex]), (None, None, []))[2])\n        negreal = len(next(walk(neg_real_location[folderIndex]), (None, None, []))[2])\n        postonegaug = len(next(walk(pos2neg_aug_location[folderIndex]), (None, None, []))[2])\n        postonegreal = len(next(walk(pos2neg_real_location[folderIndex]), (None, None, []))[2])\n        negtoposaug = len(next(walk(neg2pos_aug_location[folderIndex]), (None, None, []))[2])\n        negtoposreal = len(next(walk(neg2pos_real_location[folderIndex]), (None, None, []))[2])\n\n        #print ('No of Files in TruePos-&gt; copyFolder: ' + str(posaug) + ' ; applyFolder: '+ str(posreal))\n        #print ('No of Files in TrueNeg-&gt; copyFolder: ' + str(negaug) + ' ; applyFolder: '+ str(negreal))\n        #print ('No of Files in PosToNeg-&gt; copyFolder: ' + str(postonegaug) + ' ; applyFolder: '+ str(postonegreal))\n        #print ('No of Files in NegToPos-&gt; copyFolder: ' + str(negtoposaug) + ' ; applyFolder: '+ str(negtoposreal))\n\n    # apply function to all folders\n    r_processFolder = lambda x: processFolder (folderIndex=x)\n    process_folders = list(map(r_processFolder, list(range(len(copyFolder)))))\n\n    # Finish Job\n    if verbose is True:\n        print('Cloning Folder is complete, head over to /CSPOT/Thumbnails\" to view results')\n</code></pre>"},{"location":"Functions/csExport/","title":"csExport","text":"<p>Short Description</p> <p>Users can utilize the <code>csExport</code> function to store the contents of the csObject to a <code>.CSV</code> file. </p> <p>Keep in mind that the presence of multiple intermediate files in the object will result in the production of several CSV files.</p>"},{"location":"Functions/csExport/#cspot.csExport--function","title":"Function","text":""},{"location":"Functions/csExport/#cspot.csExport.csExport","title":"<code>csExport(csObject, projectDir, fileName=None, raw=False, CellID='CellID', verbose=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObject</code> <code>anndata</code> <p>Pass the <code>csObject</code> loaded into memory or a path to the <code>csObject</code>  file (.h5ad).</p> required <code>projectDir</code> <code>str</code> <p>Provide the path to the output directory. The result will be located at <code>projectDir/CSPOT/csExport/</code>. </p> required <code>fileName</code> <code>str</code> <p>Specify the name of the CSV output file. If you don't provide a file name, t he default name <code>csExport.csv</code> will be assigned.</p> <code>None</code> <code>raw</code> <code>bool</code> <p>If <code>True</code> raw data will be returned instead of the CSPOT scaled data.</p> <code>False</code> <code>CellId</code> <code>str</code> <p>Specify the column name that holds the cell ID (a unique name given to each cell).</p> required <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <p>Returns:</p> Type Description <p>CSV files (.csv): The <code>.csv</code> files can be found under <code>projectDir/CSPOT/csExport/</code></p> Example <pre><code># path to files needed for csExport\nprojectDir = '/Users/aj/Documents/cspotExampleData'\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\ncs.csExport(csObject,\n       projectDir,\n       fileName=None,\n       raw=False,\n       CellID='CellID',\n       verbose=True)\n\n# Same function if the user wants to run it via Command Line Interface\npython csExport.py             --csObject /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/csExport.py</code> <pre><code>def csExport(\n    csObject, projectDir, fileName=None, raw=False, CellID=\"CellID\", verbose=True\n):\n    \"\"\"\nParameters:\n    csObject (anndata):\n        Pass the `csObject` loaded into memory or a path to the `csObject` \n        file (.h5ad).\n\n    projectDir (str, optional):\n        Provide the path to the output directory. The result will be located at\n        `projectDir/CSPOT/csExport/`. \n\n    fileName (str, optional):\n        Specify the name of the CSV output file. If you don't provide a file name, t\n        he default name `csExport.csv` will be assigned.\n\n    raw (bool, optional):\n        If `True` raw data will be returned instead of the CSPOT scaled data.\n\n    CellId (str, optional):\n        Specify the column name that holds the cell ID (a unique name given to each cell).\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\nReturns:\n    CSV files (.csv):\n        The `.csv` files can be found under `projectDir/CSPOT/csExport/`\n\nExample:\n        ```python\n        # path to files needed for csExport\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n        csObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\n        cs.csExport(csObject,\n               projectDir,\n               fileName=None,\n               raw=False,\n               CellID='CellID',\n               verbose=True)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python csExport.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n        ```\n\n    \"\"\"\n\n    # Load the andata object\n    if isinstance(csObject, str):\n        if fileName is None:\n            imid = pathlib.Path(csObject).stem\n        else:\n            imid = str(fileName)\n        csObject = ad.read(csObject)\n    else:\n        if fileName is None:\n            imid = \"csExport\"\n        else:\n            imid = str(fileName)\n        csObject = csObject\n\n    # Expression matrix &amp; obs data\n    if raw is True:\n        data = pd.DataFrame(\n            csObject.raw.X, index=csObject.obs.index, columns=csObject.var.index\n        )\n    else:\n        data = pd.DataFrame(\n            csObject.X, index=csObject.obs.index, columns=csObject.var.index\n        )\n    meta = pd.DataFrame(csObject.obs)\n    # Merge the two dataframes\n    merged = pd.concat([data, meta], axis=1, sort=False)\n\n    # Add a column to save cell-id\n    # make cellID the first column\n    if CellID in merged.columns:\n        first_column = merged.pop(CellID)\n        merged.insert(0, CellID, first_column)\n    else:\n        merged[\"CellID\"] = merged.index\n        first_column = merged.pop(CellID)\n        merged.insert(0, CellID, first_column)\n\n    # reset index\n    merged = merged.reset_index(drop=True)\n\n    # create a folder to hold the results\n    folderPath = pathlib.Path(projectDir + \"/CSPOT/csExport/\")\n    folderPath.mkdir(exist_ok=True, parents=True)\n\n    # extract some of the data stored in .uns and save\n    if hasattr(csObject, \"uns\") and \"cspotOutput\" in csObject.uns:\n        cspotOutput = csObject.uns[\"cspotOutput\"]\n        cspotOutput.index = merged[\"CellID\"]\n        cspotOutput.to_csv(folderPath / \"cspotOutput.csv\")\n    if hasattr(csObject, \"uns\") and \"csScore\" in csObject.uns:\n        csScore = csObject.uns[\"csScore\"]\n        csScore.index = merged[\"CellID\"]\n        csScore.to_csv(folderPath / \"csScore.csv\")\n\n    # scaled data\n    merged.to_csv(folderPath / f\"{imid}.csv\", index=False)\n\n    # Finish Job\n    if verbose is True:\n        print(\n            'Contents of the csObject have been exported to \"'\n            + str(projectDir)\n            + '/CSPOT/csExport\"'\n        )\n</code></pre>"},{"location":"Functions/csObject/","title":"csObject","text":"<p>Short Description</p> <p>The <code>csObject</code> function creates a CSPOT object using the anndata  framework by inputting csScore and a pre-calculated single-cell spatial table.  This centralizes all information into one file, streamlining the data analysis  process and reducing the risk of losing data.</p>"},{"location":"Functions/csObject/#cspot.csObject--function","title":"Function","text":""},{"location":"Functions/csObject/#cspot.csObject.csObject","title":"<code>csObject(spatialTablePath, csScorePath, CellId='CellID', uniqueCellId=True, split='X_centroid', removeDNA=True, remove_string_from_name=None, log=True, dropMarkers=None, verbose=True, projectDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>spatialTablePath</code> <code>list</code> <p>Provide a list of paths to the single-cell spatial feature tables, ensuring each image has a unique path specified.</p> required <code>csScorePath</code> <code>list</code> <p>Supply a list of paths to the DL score tables created using generateCSScore, ensuring they correspond to the image paths specified.</p> required <code>CellId</code> <code>str</code> <p>Specify the column name that holds the cell ID (a unique name given to each cell).</p> <code>'CellID'</code> <code>uniqueCellId</code> <code>bool</code> <p>The function generates a unique name for each cell by combining the CellId and imageid. If you don't want this, pass False. In such case the function will default to using just the CellId. However, make sure CellId is unique especially when loading multiple images together.</p> <code>True</code> <code>split</code> <code>string</code> <p>The spatial feature table generally includes single cell expression data and meta data such as X, Y coordinates, and cell shape size. The CSPOT object separates them. Ensure that the expression data columns come first, followed by meta data columns. Provide the column name that marks the split, i.e the column name immediately following the expression data.</p> <code>'X_centroid'</code> <code>removeDNA</code> <code>bool</code> <p>Exclude DNA channels from the final output. The function searches for column names containing the string <code>dna</code> or <code>dapi</code>. </p> <code>True</code> <code>remove_string_from_name</code> <code>string</code> <p>Cleans up channel names by removing user specified string from all marker names. </p> <code>None</code> <code>log</code> <code>bool</code> <p>Apply log1p transformation to log the data. </p> <code>True</code> <code>dropMarkers</code> <code>list</code> <p>Specify a list of markers to be removed from the analysis, for example: [\"background_channel\", \"CD20\"]. </p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>projectDir</code> <code>string</code> <p>Provide the path to the output directory. The result will be located at <code>projectDir/CSPOT/csObject/</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>If projectDir is provided the CSPOT Object will be saved as a <code>.h5ad</code> file in the provided directory.</p> Example <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# Path to all the files that are necessary files for running csObject function\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\ncsScorePath = projectDir + '/CSPOT/csScore/exampleImage_cspotPredict.ome.csv'\n\n# please note that there are a number of defaults in the below function that assumes certain structure within the spatialTable.\n# Please confirm it is similar with user data or modifiy the parameters accordingly\n# check out the documentation for further details\nadata = cs.csObject (spatialTablePath=spatialTablePath,\n                csScorePath=csScorePath,\n                CellId='CellID',\n                uniqueCellId=True,\n                split='X_centroid',\n                removeDNA=True,\n                remove_string_from_name=None,\n                log=True,\n                dropMarkers=None,\n                projectDir=projectDir)\n\n# Same function if the user wants to run it via Command Line Interface\npython csObject.py             --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv             --csScorePath /Users/aj/Documents/cspotExampleData/CSPOT/csScore/exampleImage_cspotPredict.ome.csv             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/csObject.py</code> <pre><code>def csObject (spatialTablePath,\n                 csScorePath,\n                 CellId='CellID',\n                 uniqueCellId=True,\n                 split='X_centroid',\n                 removeDNA=True,\n                 remove_string_from_name=None,\n                 log=True,\n                 dropMarkers=None,\n                 verbose=True,\n                 projectDir=None):\n    \"\"\"\nParameters:\n    spatialTablePath (list):\n        Provide a list of paths to the single-cell spatial feature tables, ensuring each image has a unique path specified.\n\n    csScorePath (list):\n        Supply a list of paths to the DL score tables created using generateCSScore,\n        ensuring they correspond to the image paths specified.\n\n    CellId (str, optional):\n        Specify the column name that holds the cell ID (a unique name given to each cell).\n\n    uniqueCellId (bool, optional):\n        The function generates a unique name for each cell by combining the CellId and imageid.\n        If you don't want this, pass False. In such case the function will default to using just the CellId.\n        However, make sure CellId is unique especially when loading multiple images together.\n\n    split (string, optional):\n        The spatial feature table generally includes single cell expression data\n        and meta data such as X, Y coordinates, and cell shape size. The CSPOT\n        object separates them. Ensure that the expression data columns come first,\n        followed by meta data columns. Provide the column name that marks the split,\n        i.e the column name immediately following the expression data.\n\n    removeDNA (bool, optional):\n        Exclude DNA channels from the final output. The function searches for\n        column names containing the string `dna` or `dapi`. \n\n    remove_string_from_name (string, optional):\n        Cleans up channel names by removing user specified string from all marker\n        names. \n\n    log (bool, optional):\n        Apply log1p transformation to log the data. \n\n    dropMarkers (list, optional):\n        Specify a list of markers to be removed from the analysis, for\n        example: [\"background_channel\", \"CD20\"]. \n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    projectDir (string, optional):\n        Provide the path to the output directory. The result will be located at\n        `projectDir/CSPOT/csObject/`.\n\nReturns:\n    csObject (anndata):\n        If projectDir is provided the CSPOT Object will be saved as a\n        `.h5ad` file in the provided directory.\n\nExample:\n        ```python\n\n        # set the working directory &amp; set paths to the example data\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # Path to all the files that are necessary files for running csObject function\n        segmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n        csScorePath = projectDir + '/CSPOT/csScore/exampleImage_cspotPredict.ome.csv'\n\n        # please note that there are a number of defaults in the below function that assumes certain structure within the spatialTable.\n        # Please confirm it is similar with user data or modifiy the parameters accordingly\n        # check out the documentation for further details\n        adata = cs.csObject (spatialTablePath=spatialTablePath,\n                        csScorePath=csScorePath,\n                        CellId='CellID',\n                        uniqueCellId=True,\n                        split='X_centroid',\n                        removeDNA=True,\n                        remove_string_from_name=None,\n                        log=True,\n                        dropMarkers=None,\n                        projectDir=projectDir)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python csObject.py \\\n            --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv \\\n            --csScorePath /Users/aj/Documents/cspotExampleData/CSPOT/csScore/exampleImage_cspotPredict.ome.csv \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n        ```\n\n    \"\"\"\n\n    # spatialTablePath list or string\n    if isinstance(spatialTablePath, str):\n        spatialTablePath = [spatialTablePath]\n    spatialTablePath = [pathlib.Path(p) for p in spatialTablePath]\n    # csScorePath list or string\n    if isinstance(csScorePath, str):\n        csScorePath = [csScorePath]\n    csScorePath = [pathlib.Path(p) for p in csScorePath]\n\n    # Import spatialTablePath\n    def load_process_data (image):\n        # Print the data that is being processed\n        if verbose is True:\n            print(f\"Loading {image.name}\")\n        d = pd.read_csv(image)\n        # If the data does not have a unique image ID column, add one.\n        if 'imageid' not in d.columns:\n            imid = image.stem\n            d['imageid'] = imid\n        # Unique name for the data\n        if uniqueCellId is True:\n            d.index = d['imageid'].astype(str)+'_'+d[CellId].astype(str)\n        else:\n            d.index = d[CellId]\n\n        # move image id and cellID column to end\n        cellid_col = [col for col in d.columns if col != CellId] + [CellId]; d = d[cellid_col]\n        imageid_col = [col for col in d.columns if col != 'imageid'] + ['imageid']; d = d[imageid_col]\n        # If there is INF replace with zero\n        d = d.replace([np.inf, -np.inf], 0)\n        # Return data\n        return d\n\n    # Import csScorePath\n    def load_process_probTable (image):\n        d = pd.read_csv(image, index_col=0)\n        # Return data\n        return d\n\n    # Apply function to all spatialTablePath and create a master dataframe\n    r_load_process_data = lambda x: load_process_data(image=x) # Create lamda function\n    all_spatialTable = list(map(r_load_process_data, list(spatialTablePath))) # Apply function\n    # Merge all the spatialTablePath into a single large dataframe\n    for i in range(len(all_spatialTable)):\n        all_spatialTable[i].columns = all_spatialTable[0].columns\n    entire_spatialTable = pd.concat(all_spatialTable, axis=0, sort=False)\n\n    # Apply function to all csScorePath and create a master dataframe\n    r_load_process_probTable = lambda x: load_process_probTable(image=x) # Create lamda function\n    all_probTable = list(map(r_load_process_probTable, list(csScorePath))) # Apply function\n    # Merge all the csScorePath into a single large dataframe\n    for i in range(len(all_probTable)):\n        all_probTable[i].columns = all_probTable[0].columns\n    entire_probTable = pd.concat(all_probTable, axis=0, sort=False)\n    # make the index of entire_probTable same as all_probTable\n    ## NOTE THIS IS A HARD COPY WITHOUT ANY CHECKS! ASSUMES BOTH ARE IN SAME ORDER\n    entire_probTable.index = entire_spatialTable.index\n\n\n    # Split the data into expression data and meta data\n    # Step-1 (Find the index of the column with name X_centroid)\n    split_idx = entire_spatialTable.columns.get_loc(split)\n    meta = entire_spatialTable.iloc [:,split_idx:]\n    # Step-2 (select only the expression values)\n    entire_spatialTable = entire_spatialTable.iloc [:,:split_idx]\n\n    # Rename the columns of the data\n    if remove_string_from_name is not None:\n        entire_spatialTable.columns = entire_spatialTable.columns.str.replace(remove_string_from_name, '')\n\n    # Save a copy of the column names in the uns space of ANNDATA\n    markers = list(entire_spatialTable.columns)\n\n    # Remove DNA channels\n    if removeDNA is True:\n        entire_spatialTable = entire_spatialTable.loc[:,~entire_spatialTable.columns.str.contains('dna', case=False)]\n        entire_spatialTable = entire_spatialTable.loc[:,~entire_spatialTable.columns.str.contains('dapi', case=False)]\n\n    # Drop unnecessary markers\n    if dropMarkers is not None:\n        if isinstance(dropMarkers, str):\n            dropMarkers = [dropMarkers]\n        dropMarkers = list(set(dropMarkers).intersection(entire_spatialTable.columns))\n        entire_spatialTable = entire_spatialTable.drop(columns=dropMarkers)\n\n    # Create an anndata object\n    adata = ad.AnnData(entire_spatialTable, dtype=np.float64)\n    adata.obs = meta\n    adata.uns['all_markers'] = markers\n    adata.uns['csScore'] = entire_probTable\n\n    # Add log data\n    if log is True:\n        adata.raw = adata\n        adata.X = np.log1p(adata.X)\n\n    # Save data if requested\n    if projectDir is not None:\n        finalPath = pathlib.Path(projectDir + '/CSPOT/csObject')\n        if not os.path.exists(finalPath):\n            os.makedirs(finalPath)\n        if len(spatialTablePath) &gt; 1:\n            imid = 'csObject'\n        else:\n            imid = csScorePath[0].stem\n        adata.write(finalPath / f'{imid}.h5ad')\n        # Finish Job\n        if verbose is True:\n            print('CSPOT Object has been created, head over to'+ str(projectDir) + '/CSPOT/csObject\" to view results')\n    else:\n        # Return data\n        if verbose is True:\n            print('CSPOT Object has been created')\n        return adata\n</code></pre>"},{"location":"Functions/csPhenotype/","title":"csPhenotype","text":"<p>Short Description</p> <p>The csPhenotype function requires a phenotype workflow document to guide  the algorithm in performing classification.  </p> <p>The phenotype workflow document is imported as a <code>dataframe</code> and passed to the  <code>phenotype</code> argument. It should follow the following structure:  </p> <p>(1) The <code>first column</code> has to contain the cell that are to be classified. (2) The <code>second column</code> indicates the phenotype a particular cell will be assigned  if it satifies the conditions in the row. (3) <code>Column three</code> and onward represent protein markers. If the protein marker  is known to be expressed for that cell type, then it is denoted by either <code>pos</code>,  <code>allpos</code>. If the protein marker is known to not express for a cell type it  can be denoted by <code>neg</code>, <code>allneg</code>. If the protein marker is irrelevant or  uncertain to express for a cell type, then it is left empty. <code>anypos</code> and  <code>anyneg</code> are options for using a set of markers and if any of the marker is  positive or negative, the cell type is denoted accordingly.  </p> <p>To give users maximum flexibility in identifying desired cell types,  we have implemented various classification arguments as described above  for strategical classification. They include  </p> <ul> <li>allpos</li> <li>allneg</li> <li>anypos</li> <li>anyneg</li> <li>pos</li> <li>neg</li> </ul> <p><code>pos</code> : \"Pos\" looks for cells positive for a given marker. If multiple  markers are annotated as <code>pos</code>, all must be positive to denote the cell type.  For example, a Regulatory T cell can be defined as <code>CD3+CD4+FOXP3+</code> by passing  <code>pos</code> to each marker. If one or more markers don't meet the criteria (e.g. CD4-),  the program will classify it as <code>Likely-Regulatory-T cell</code>, pending user  confirmation. This is useful in cases of technical artifacts or when cell  types (such as cancer cells) are defined by marker loss (e.g. T-cell Lymphomas).  </p> <p><code>neg</code> : Same as <code>pos</code> but looks for negativity of the defined markers.  </p> <p><code>allpos</code> : \"Allpos\" requires all defined markers to be positive. Unlike  <code>pos</code>, it doesn't classify cells as <code>Likely-cellType</code>, but strictly annotates  cells positive for all defined markers.  </p> <p><code>allneg</code> : Same as <code>allpos</code> but looks for negativity of the defined markers.  </p> <p><code>anypos</code> : \"Anypos\" requires only one of the defined markers to be positive.  For example, to define macrophages, a cell could be designated as such if  any of <code>CD68</code>, <code>CD163</code>, or <code>CD206</code> is positive.  </p> <p><code>anyneg</code> : Same as <code>anyneg</code> but looks for negativity of the defined markers. </p>"},{"location":"Functions/csPhenotype/#cspot.csPhenotype--function","title":"Function","text":""},{"location":"Functions/csPhenotype/#cspot.csPhenotype.csPhenotype","title":"<code>csPhenotype(csObject, phenotype, midpoint=0.5, label='phenotype', imageid='imageid', pheno_threshold_percent=None, pheno_threshold_abs=None, fileName=None, verbose=True, projectDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObject</code> <code>anndata</code> <p>Single or combined CSPOT object.</p> required <code>phenotype</code> <code>(dataframe, str)</code> <p>A phenotyping workflow strategy either as a <code>dataframe</code> loaded into memory or  a path to the <code>.csv</code> file. </p> required <code>midpoint</code> <code>float</code> <p>By default, CSPOT normalizes the data in a way that cells with a value above 0.5 are considered positive. However, if you desire more selective threshold, the parameter can be adjusted accordingly. </p> <code>0.5</code> <code>label</code> <code>str</code> <p>Specify the column name under which the final phenotype classification will be saved. </p> <code>'phenotype'</code> <code>imageid</code> <code>str</code> <p>The name of the column that holds the unique image ID. </p> <code>'imageid'</code> <code>pheno_threshold_percent</code> <code>float</code> <p>The user-defined threshold, which can be set between 0-100, is used to recategorize any phenotype that falls below it as 'unknown'. This function is commonly used to address low background false positives.</p> <code>None</code> <code>pheno_threshold_abs</code> <code>int</code> <p>This function serves a similar purpose as the <code>pheno_threshold_percent</code>, but it accepts an absolute number as input. For example, if the user inputs 10, any phenotype that contains less than 10 cells will be recategorized as 'unknown'. </p> <code>None</code> <code>fileName</code> <code>string</code> <p>File Name to be used while saving the CSPOT object.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>projectDir</code> <code>string</code> <p>Provide the path to the output directory.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>Modified CSPOT object with the Phenotypes is returned. If <code>projectDir</code> is  provided, it will be saved in the defined directory.</p> Example <pre><code># set the Project directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n# Path to the CSPOT Object\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\n# load the phenotyping workflow\nphenotype = pd.read_csv(str(projectDir) + '/phenotype_workflow.csv')\n\n# Run Function\nadata = cs.csPhenotype ( csObject=csObject,\n                    phenotype=phenotype,\n                    midpoint = 0.5,\n                    label=\"phenotype\",\n                    imageid='imageid',\n                    pheno_threshold_percent=None,\n                    pheno_threshold_abs=None,\n                    fileName=None,\n                    projectDir=projectDir)\n\n# Same function if the user wants to run it via Command Line Interface\npython csPhenotype.py             --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad             --phenotype /Users/aj/Documents/cspotExampleData/phenotype_workflow.csv             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/csPhenotype.py</code> <pre><code>def csPhenotype (csObject,\n                    phenotype,\n                    midpoint = 0.5,\n                    label=\"phenotype\",\n                    imageid='imageid',\n                    pheno_threshold_percent=None,\n                    pheno_threshold_abs=None,\n                    fileName=None,\n                    verbose=True,\n                    projectDir=None):\n    \"\"\"\nParameters:\n    csObject (anndata):\n        Single or combined CSPOT object.\n\n    phenotype (dataframe, str):\n        A phenotyping workflow strategy either as a `dataframe` loaded into memory or \n        a path to the `.csv` file. \n\n    midpoint (float, optional):\n        By default, CSPOT normalizes the data in a way that cells with a value\n        above 0.5 are considered positive. However, if you desire more selective\n        threshold, the parameter can be adjusted accordingly. \n\n    label (str, optional):\n        Specify the column name under which the final phenotype classification\n        will be saved. \n\n    imageid (str, optional):\n        The name of the column that holds the unique image ID. \n\n    pheno_threshold_percent (float, optional):\n        The user-defined threshold, which can be set between 0-100, is used to\n        recategorize any phenotype that falls below it as 'unknown'.\n        This function is commonly used to address low background false positives.\n\n    pheno_threshold_abs (int, optional):\n        This function serves a similar purpose as the `pheno_threshold_percent`,\n        but it accepts an absolute number as input. For example, if the user\n        inputs 10, any phenotype that contains less than 10 cells will be\n        recategorized as 'unknown'. \n\n    fileName (string, optional):\n        File Name to be used while saving the CSPOT object.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    projectDir (string, optional):\n        Provide the path to the output directory.\n\nReturns:\n    csObject (anndata):\n        Modified CSPOT object with the Phenotypes is returned. If `projectDir` is \n        provided, it will be saved in the defined directory.\n\nExample:\n        ```python\n\n        # set the Project directory\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n        # Path to the CSPOT Object\n        csObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\n        # load the phenotyping workflow\n        phenotype = pd.read_csv(str(projectDir) + '/phenotype_workflow.csv')\n\n        # Run Function\n        adata = cs.csPhenotype ( csObject=csObject,\n                            phenotype=phenotype,\n                            midpoint = 0.5,\n                            label=\"phenotype\",\n                            imageid='imageid',\n                            pheno_threshold_percent=None,\n                            pheno_threshold_abs=None,\n                            fileName=None,\n                            projectDir=projectDir)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python csPhenotype.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad \\\n            --phenotype /Users/aj/Documents/cspotExampleData/phenotype_workflow.csv \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n\n        ```\n\n    \"\"\"\n\n    # Load data\n    if isinstance(csObject, str):\n        adata = ad.read(csObject)\n    else:\n        adata = csObject.copy()\n\n    # load phenotype\n    if isinstance(phenotype, pd.DataFrame):\n        phenotype = phenotype\n    else:\n        phenotype = pd.read_csv(pathlib.Path(phenotype))\n\n    # Create a dataframe from the adata object\n    data = pd.DataFrame(adata.X, columns = adata.var.index, index= adata.obs.index)\n\n    # Function to calculate the phenotype scores\n    def phenotype_cells (data,phenotype,midpoint,group):\n\n        # Subset the phenotype based on the group\n        phenotype = phenotype[phenotype.iloc[:,0] == group]\n\n        # Parser to parse the CSV file into four categories\n        def phenotype_parser (p, cell):\n            # Get the index and subset the phenotype row being passed in\n            location = p.iloc[:,1] == cell\n            idx = [i for i, x in enumerate(location) if x][0]\n            phenotype = p.iloc[idx,:]\n            # Calculate\n            pos = phenotype[phenotype == 'pos'].index.tolist()\n            neg = phenotype[phenotype == 'neg'].index.tolist()\n            anypos = phenotype[phenotype == 'anypos'].index.tolist()\n            anyneg = phenotype[phenotype == 'anyneg'].index.tolist()\n            allpos = phenotype[phenotype == 'allpos'].index.tolist()\n            allneg = phenotype[phenotype == 'allneg'].index.tolist()\n            return {'pos': pos, 'neg': neg ,'anypos': anypos, 'anyneg': anyneg, 'allpos': allpos, 'allneg': allneg}\n            #return pos, neg, anypos, anyneg\n\n        # Run the phenotype_parser function on all rows\n        p_list = phenotype.iloc[:,1].tolist()\n        r_phenotype = lambda x: phenotype_parser(cell=x, p=phenotype) # Create lamda function\n        all_phenotype = list(map(r_phenotype, p_list)) # Apply function\n        all_phenotype = dict(zip(p_list, all_phenotype)) # Name the lists\n\n        # Define function to check if there is any marker that does not satisfy the midpoint\n        def gate_satisfation_lessthan (marker, data, midpoint):\n            fail = np.where(data[marker] &lt; midpoint, 1, 0) # 1 is fail\n            return fail\n        # Corresponding lamda function\n        r_gate_satisfation_lessthan = lambda x: gate_satisfation_lessthan(marker=x, data=data, midpoint=midpoint)\n\n        # Define function to check if there is any marker that does not satisfy the midpoint\n        def gate_satisfation_morethan (marker, data, midpoint):\n            fail = np.where(data[marker] &gt; midpoint, 1, 0)\n            return fail\n        # Corresponding lamda function\n        r_gate_satisfation_morethan = lambda x: gate_satisfation_morethan(marker=x, data=data, midpoint=midpoint)\n\n        def prob_mapper (data, all_phenotype, cell, midpoint):\n            if verbose is True:\n                print(\"Phenotyping \" + str(cell))\n\n            # Get the appropriate dict from all_phenotype\n            p = all_phenotype[cell]\n\n            # Identiy the marker used in each category\n            pos = p.get('pos')\n            neg = p.get('neg')\n            anypos = p.get('anypos')\n            anyneg = p.get('anyneg')\n            allpos = p.get('allpos')\n            allneg = p.get('allneg')\n\n            # Perform computation for each group independently\n            # Positive marker score\n            if len(pos) != 0:\n                pos_score = data[pos].mean(axis=1).values\n                pos_fail = list(map(r_gate_satisfation_lessthan, pos)) if len(pos) &gt; 1 else []\n                pos_fail = np.amax(pos_fail, axis=0) if len(pos) &gt; 1 else []\n            else:\n                pos_score = np.repeat(0, len(data))\n                pos_fail = []\n\n            # Negative marker score\n            if len(neg) != 0:\n                neg_score = (1-data[neg]).mean(axis=1).values\n                neg_fail = list(map(r_gate_satisfation_morethan, neg)) if len(neg) &gt; 1 else []\n                neg_fail = np.amax(neg_fail, axis=0) if len(neg) &gt; 1 else []\n            else:\n                neg_score = np.repeat(0, len(data))\n                neg_fail = []\n\n            # Any positive score\n            anypos_score = np.repeat(0, len(data)) if len(anypos) == 0 else data[anypos].max(axis=1).values\n\n            # Any negative score\n            anyneg_score = np.repeat(0, len(data)) if len(anyneg) == 0 else (1-data[anyneg]).max(axis=1).values\n\n            # All positive score\n            if len(allpos) != 0:\n                allpos_score = data[allpos]\n                allpos_score['score'] = allpos_score.max(axis=1)\n                allpos_score.loc[(allpos_score &lt; midpoint).any(axis = 1), 'score'] = 0\n                allpos_score = allpos_score['score'].values + 0.01 # A small value is added to give an edge over the matching positive cell\n            else:\n                allpos_score = np.repeat(0, len(data))\n\n\n            # All negative score\n            if len(allneg) != 0:\n                allneg_score = 1- data[allneg]\n                allneg_score['score'] = allneg_score.max(axis=1)\n                allneg_score.loc[(allneg_score &lt; midpoint).any(axis = 1), 'score'] = 0\n                allneg_score = allneg_score['score'].values + 0.01\n            else:\n                allneg_score = np.repeat(0, len(data))\n\n\n            # Total score calculation\n            # Account for differences in the number of categories used for calculation of the final score\n            number_of_non_empty_features = np.sum([len(pos) != 0,\n                                                len(neg) != 0,\n                                                len(anypos) != 0,\n                                                len(anyneg) != 0,\n                                                len(allpos) != 0,\n                                                len(allneg) != 0])\n\n            total_score = (pos_score + neg_score + anypos_score + anyneg_score + allpos_score + allneg_score) / number_of_non_empty_features\n\n            return {cell: total_score, 'pos_fail': pos_fail ,'neg_fail': neg_fail}\n            #return total_score, pos_fail, neg_fail\n\n\n        # Apply the fuction to get the total score for all cell types\n        r_prob_mapper = lambda x: prob_mapper (data=data, all_phenotype=all_phenotype, cell=x, midpoint=midpoint) # Create lamda function\n        final_scores = list(map(r_prob_mapper, [*all_phenotype])) # Apply function\n        final_scores = dict(zip([*all_phenotype], final_scores)) # Name the lists\n\n        # Combine the final score to annotate the cells with a label\n        final_score_df = pd.DataFrame()\n        for i in [*final_scores]:\n            df = pd.DataFrame(final_scores[i][i])\n            final_score_df= pd.concat([final_score_df, df], axis=1)\n        # Name the columns\n        final_score_df.columns = [*final_scores]\n        final_score_df.index = data.index\n        # Add a column called unknown if all markers have a value less than the midpoint (0.5)\n        unknown = group + str('-rest')\n        final_score_df[unknown] = (final_score_df &lt; midpoint).all(axis=1).astype(int)\n\n        # Name each cell\n        labels = final_score_df.idxmax(axis=1)\n\n        # Group all failed instances (i.e. when multiple markers were given\n        # any one of the marker fell into neg or pos zones of the midpoint)\n        pos_fail_all = pd.DataFrame()\n        for i in [*final_scores]:\n            df = pd.DataFrame(final_scores[i]['pos_fail'])\n            df.columns = [i] if len(df) != 0 else []\n            pos_fail_all= pd.concat([pos_fail_all, df], axis=1)\n        pos_fail_all.index = data.index if len(pos_fail_all) != 0 else []\n        # Same for Neg\n        neg_fail_all = pd.DataFrame()\n        for i in [*final_scores]:\n            df = pd.DataFrame(final_scores[i]['neg_fail'])\n            df.columns = [i] if len(df) != 0 else []\n            neg_fail_all= pd.concat([neg_fail_all, df], axis=1)\n        neg_fail_all.index = data.index if len(neg_fail_all) != 0 else []\n\n\n        # Modify the labels with the failed annotations\n        if len(pos_fail_all) != 0:\n            for i in pos_fail_all.columns:\n                labels[(labels == i) &amp; (pos_fail_all[i] == 1)] = 'likely-' + i\n        # Do the same for negative\n        if len(neg_fail_all) != 0:\n            for i in neg_fail_all.columns:\n                labels[(labels == i) &amp; (neg_fail_all[i] == 1)] = 'likely-' + i\n\n        # Retun the labels\n        return labels\n\n    # Create an empty dataframe to hold the labeles from each group\n    phenotype_labels = pd.DataFrame()\n\n    # Loop through the groups to apply the phenotype_cells function\n    for i in phenotype.iloc[:,0].unique():\n\n        if phenotype_labels.empty:\n            phenotype_labels = pd.DataFrame(phenotype_cells(data = data, group = i, phenotype=phenotype, midpoint=midpoint))\n            phenotype_labels.columns = [i]\n\n        else:\n            # Find the column with the cell-type of interest\n            column_of_interest = [] # Empty list to hold the column name\n            try:\n                column_of_interest = phenotype_labels.columns[phenotype_labels.eq(i).any()]\n            except:\n                pass\n            # If the cell-type of interest was not found just add NA\n            if len(column_of_interest) == 0:\n                phenotype_labels[i] = np.nan\n            else:\n                #cells_of_interest = phenotype_labels[phenotype_labels[column_of_interest] == i].index\n                cells_of_interest = phenotype_labels[phenotype_labels[column_of_interest].eq(i).any(axis=1)].index\n                d = data.loc[cells_of_interest]\n                if verbose is True:\n                    print(\"-- Subsetting \" + str(i))\n                phenotype_l = pd.DataFrame(phenotype_cells(data = d, group = i, phenotype=phenotype, midpoint=midpoint), columns = [i])\n                phenotype_labels = phenotype_labels.merge(phenotype_l, how='outer', left_index=True, right_index=True)\n\n    # Rearrange the rows back to original\n    phenotype_labels = phenotype_labels.reindex(data.index)\n    phenotype_labels = phenotype_labels.replace('-rest', np.nan, regex=True)\n    if verbose is True:\n        print(\"Consolidating the phenotypes across all groups\")\n        phenotype_labels_Consolidated = phenotype_labels.fillna(method='ffill', axis = 1)\n    phenotype_labels[label] = phenotype_labels_Consolidated.iloc[:,-1].values\n\n    # replace nan to 'other cells'\n    phenotype_labels[label] = phenotype_labels[label].fillna('Unknown')\n\n    # Apply the phenotype threshold if given\n    if pheno_threshold_percent or pheno_threshold_abs is not None:\n        p = pd.DataFrame(phenotype_labels[label])\n        q = pd.DataFrame(adata.obs[imageid])\n        p = q.merge(p, how='outer', left_index=True, right_index=True)\n\n        # Function to remove phenotypes that are less than the given threshold\n        def remove_phenotype(p, ID, pheno_threshold_percent, pheno_threshold_abs):\n            d = p[p[imageid] == ID]\n            x = pd.DataFrame(d.groupby([label]).size())\n            x.columns = ['val']\n            # FInd the phenotypes that are less than the given threshold\n            if pheno_threshold_percent is not None:\n                fail = list(x.loc[x['val'] &lt; x['val'].sum() * pheno_threshold_percent/100].index)\n            if pheno_threshold_abs is not None:\n                fail = list(x.loc[x['val'] &lt; pheno_threshold_abs].index)\n            d[label] = d[label].replace(dict(zip(fail, np.repeat('Unknown',len(fail)))))\n            # Return\n            return d\n\n        # Apply function to all images\n        r_remove_phenotype = lambda x: remove_phenotype (p=p, ID=x,\n                                                         pheno_threshold_percent=pheno_threshold_percent,\n                                                         pheno_threshold_abs=pheno_threshold_abs) # Create lamda function\n        final_phrnotypes= list(map(r_remove_phenotype, list(p[imageid].unique()))) # Apply function\n\n        final_phrnotypes = pd.concat(final_phrnotypes, join='outer')\n        phenotype_labels = final_phrnotypes.reindex(adata.obs.index)\n\n\n    # Return to adata\n    adata.obs[label] = phenotype_labels[label]\n\n    # Save data if requested\n    if projectDir is not None:\n\n        finalPath = pathlib.Path(projectDir + '/CSPOT/csPhenotype/')\n        if not os.path.exists(finalPath):\n            os.makedirs(finalPath)\n        # determine file name\n        if fileName is None:\n            if isinstance (csObject, str):\n                imid = pathlib.Path(csObject).stem\n            else:\n                imid = 'csPhenotype'\n        else:\n            imid = fileName\n\n        adata.write(finalPath / f'{imid}.h5ad')\n        # Print\n        if verbose is True:\n            print('Modified csObject is stored at \"' + str(projectDir) + '/CSPOT/csPhenotype')\n\n    else:\n        # Return data\n        return adata\n    return adata\n</code></pre>"},{"location":"Functions/csPipeline/","title":"csPipeline","text":"<p>Short Description</p> <p>The csPipeline function is simply a wrapper for the following functions: - csPredict - generateCSScore - csObject - cspot  </p> <p>Typically, in production settings, <code>csPipeline</code> would be utilized, whereas  step-by-step analysis would be employed for troubleshooting, model validation,  and similar tasks that necessitate greater granularity or control.</p> <p>Please refer to the individual function documentation for parameter tuning.</p>"},{"location":"Functions/csPipeline/#cspot.csPipeline--function","title":"Function","text":""},{"location":"Functions/csPipeline/#cspot.csPipeline.csPipeline","title":"<code>csPipeline(**kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>imagePath</code> <code>str</code> <p>The path to the .tif file that needs to be processed. </p> required <code>csModelPath</code> <code>str</code> <p>The path to the <code>cspotModel</code> folder. </p> required <code>markerChannelMapPath</code> <code>str</code> <p>The path to the marker panel list, which contains information about the markers used in the image. This argument is required.</p> required <code>segmentationMaskPath</code> <code>str</code> <p>Supply the path of the pre-computed segmentation mask.</p> required <code>spatialTablePath</code> <code>list</code> <p>Provide a list of paths to the single-cell spatial feature tables, ensuring each image has a unique path specified.</p> required <code>projectDir</code> <code>str</code> <p>The path to the output directory where the processed images (<code>probabilityMasks</code>) will be saved.</p> required <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> required <code>markerColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the marker names. The default value is 'marker'.</p> required <code>channelColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the channel names. The default value is 'channel'.</p> required <code>modelColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the model names. The default value is 'cspotmodel'.</p> required <code>GPU</code> <code>int</code> <p>An optional argument to explicitly select the GPU to use. The default value is -1, meaning that the GPU will be selected automatically.</p> required <code>feature</code> <code>str</code> <p>Calculates the <code>mean</code> or <code>median</code> CSPOT Score for each cell.</p> required <code>markerNames</code> <code>list</code> <p>The program searches for marker names in the meta data (description section) of the tiff files created by <code>csPredict</code> by default. If the meta data is lost due to user modifications, provide the marker names for each channel/layer in the <code>probabilityMaskPath</code> here.</p> required <code>CellId</code> <code>str</code> <p>Specify the column name that holds the cell ID (a unique name given to each cell).</p> required <code>uniqueCellId</code> <code>bool</code> <p>The function generates a unique name for each cell by combining the CellId and imageid. If you don't want this, pass False. In such case the function will default to using just the CellId. However, make sure CellId is unique especially when loading multiple images together.</p> required <code>split</code> <code>string</code> <p>The spatial feature table generally includes single cell expression data and meta data such as X, Y coordinates, and cell shape size. The CSPOT object separates them. Ensure that the expression data columns come first, followed by meta data columns. Provide the column name that marks the split, i.e the column name immediately following the expression data.</p> required <code>removeDNA</code> <code>bool</code> <p>Exclude DNA channels from the final output. The function searches for column names containing the string <code>dna</code> or <code>dapi</code>. </p> required <code>remove_string_from_name</code> <code>string</code> <p>Cleans up channel names by removing user specified string from all marker names.</p> required <code>csScore</code> <code>str</code> <p>Include the label used for saving the <code>csScore</code> within the CSPOT object.</p> required <code>minAbundance</code> <code>float</code> <p>Specify the minimum percentage of cells that should express a specific marker in order to determine if the marker is considered a failure. A good approach is to consider the lowest percentage of rare cells expected within the dataset.</p> required <code>percentiles</code> <code>list</code> <p>Specify the interval of percentile levels of the expression utilized to intialize the GMM. The cells falling within these percentiles are utilized to distinguish between negative cells (first two values) and positive cells (last two values).</p> required <code>dropMarkers</code> <code>list</code> <p>Specify a list of markers to be removed from the analysis, for example: <code>[\"background_channel1\", \"background_channel2\"]</code>. </p> required <code>RobustScale</code> <code>bool</code> <p>When set to True, the data will be subject to Robust Scaling before the Gradient Boosting Classifier is trained. </p> required <code>log</code> <code>bool</code> <p>Apply <code>log1p</code> transformation on the data, unless it has already been log transformed in which case set it to <code>False</code>. </p> required <code>stringentThreshold</code> <code>bool</code> <p>The Gaussian Mixture Model (GMM) is utilized to distinguish positive and  negative cells by utilizing csScores. The stringentThreshold can be utilized  to further refine the classification of positive and negative cells.  By setting it to True, cells with csScore below the mean of the negative  distribution and above the mean of the positive distribution will be  labeled as true negative and positive, respectively.</p> required <code>x_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the X coordinates for each cell. </p> required <code>y_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the Y coordinates for each cell.</p> required <code>imageid</code> <code>str</code> <p>The name of the column that holds the unique image ID. </p> required <code>random_state</code> <code>int</code> <p>Seed used by the random number generator. </p> required <code>rescaleMethod</code> <code>string</code> <p>Choose between <code>sigmoid</code> and <code>minmax</code>.</p> required <code>label</code> <code>str</code> <p>Assign a label for the object within <code>adata.uns</code> where the predictions from CSPOT will be stored. </p> required <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>Returns a csObject with predictions of all positve and negative cells. </p> Example <pre><code># Path to all the files that are necessary files for running the \nCSPOT Prediction Algorithm (broken down based on sub functions)\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# gatorPredict related paths\nimagePath = projectDir + '/image/exampleImage.tif'\nmarkerChannelMapPath = projectDir + '/markers.csv'\ncsModelPath = projectDir + '/manuscriptModels/'\n\n# Generate generateGatorScore related paths\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n\n# gatorObject related paths\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\n\n# Run the pipeline\ncs.csPipeline(   \n            # parameters for gatorPredict function\n            imagePath=imagePath,\n            csModelPath=csModelPath,\n            markerChannelMapPath=markerChannelMapPath,\n\n            # parameters for generateGatorScore function\n            segmentationMaskPath=segmentationPath,\n\n            # parameters for gatorObject function\n            spatialTablePath=spatialTablePath,\n\n            # parameters to run gator function\n            # ..\n\n            # common parameters\n            verbose=False,\n            projectDir=projectDir)\n\n# Same function if the user wants to run it via Command Line Interface\npython csPipeline.py                 --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif                 --csModelPath /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/                 --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv                 --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif                 --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv                 --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/csPipeline.py</code> <pre><code>def csPipeline (**kwargs):   \n    \"\"\"\nParameters:\n    imagePath (str):  \n        The path to the .tif file that needs to be processed. \n\n    csModelPath (str):  \n        The path to the `cspotModel` folder. \n\n    markerChannelMapPath (str, optional):  \n        The path to the marker panel list, which contains information about the markers used in the image. This argument is required.\n\n    segmentationMaskPath (str):\n        Supply the path of the pre-computed segmentation mask.\n\n    spatialTablePath (list):\n        Provide a list of paths to the single-cell spatial feature tables, ensuring each image has a unique path specified.\n\n    projectDir (str):  \n        The path to the output directory where the processed images (`probabilityMasks`) will be saved.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    markerColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the marker names. The default value is 'marker'.\n\n    channelColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the channel names. The default value is 'channel'.\n\n    modelColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the model names. The default value is 'cspotmodel'.\n\n    GPU (int, optional):  \n        An optional argument to explicitly select the GPU to use. The default value is -1, meaning that the GPU will be selected automatically.\n\n    feature (str, optional):\n        Calculates the `mean` or `median` CSPOT Score for each cell.\n\n    markerNames (list, optional):\n        The program searches for marker names in the meta data (description section)\n        of the tiff files created by `csPredict` by default. If the meta data\n        is lost due to user modifications, provide the marker names for each\n        channel/layer in the `probabilityMaskPath` here.\n\n    CellId (str, optional):\n        Specify the column name that holds the cell ID (a unique name given to each cell).\n\n    uniqueCellId (bool, optional):\n        The function generates a unique name for each cell by combining the CellId and imageid.\n        If you don't want this, pass False. In such case the function will default to using just the CellId.\n        However, make sure CellId is unique especially when loading multiple images together.\n\n    split (string, optional):\n        The spatial feature table generally includes single cell expression data\n        and meta data such as X, Y coordinates, and cell shape size. The CSPOT\n        object separates them. Ensure that the expression data columns come first,\n        followed by meta data columns. Provide the column name that marks the split,\n        i.e the column name immediately following the expression data.\n\n    removeDNA (bool, optional):\n        Exclude DNA channels from the final output. The function searches for\n        column names containing the string `dna` or `dapi`. \n\n    remove_string_from_name (string, optional):\n        Cleans up channel names by removing user specified string from all marker\n        names.\n\n    csScore (str, optional):\n        Include the label used for saving the `csScore` within the CSPOT object.\n\n    minAbundance (float, optional):\n        Specify the minimum percentage of cells that should express a specific\n        marker in order to determine if the marker is considered a failure.\n        A good approach is to consider the lowest percentage of rare cells\n        expected within the dataset.\n\n    percentiles (list, optional):\n        Specify the interval of percentile levels of the expression utilized to intialize\n        the GMM. The cells falling within these percentiles are utilized to distinguish\n        between negative cells (first two values) and positive cells (last two values).\n\n    dropMarkers (list, optional):\n        Specify a list of markers to be removed from the analysis, for\n        example: `[\"background_channel1\", \"background_channel2\"]`. \n\n    RobustScale (bool, optional):\n        When set to True, the data will be subject to Robust Scaling before the\n        Gradient Boosting Classifier is trained. \n\n    log (bool, optional):\n        Apply `log1p` transformation on the data, unless it has already been log\n        transformed in which case set it to `False`. \n\n    stringentThreshold (bool, optional):\n        The Gaussian Mixture Model (GMM) is utilized to distinguish positive and \n        negative cells by utilizing csScores. The stringentThreshold can be utilized \n        to further refine the classification of positive and negative cells. \n        By setting it to True, cells with csScore below the mean of the negative \n        distribution and above the mean of the positive distribution will be \n        labeled as true negative and positive, respectively.\n\n    x_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        X coordinates for each cell. \n\n    y_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        Y coordinates for each cell.\n\n    imageid (str, optional):\n        The name of the column that holds the unique image ID. \n\n    random_state (int, optional):\n        Seed used by the random number generator. \n\n    rescaleMethod (string, optional):\n        Choose between `sigmoid` and `minmax`.\n\n    label (str, optional):\n        Assign a label for the object within `adata.uns` where the predictions\n        from CSPOT will be stored. \n\n\nReturns:\n    csObject (anndata):\n        Returns a csObject with predictions of all positve and negative cells. \n\nExample:\n        ```python\n\n        # Path to all the files that are necessary files for running the \n        CSPOT Prediction Algorithm (broken down based on sub functions)\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # gatorPredict related paths\n        imagePath = projectDir + '/image/exampleImage.tif'\n        markerChannelMapPath = projectDir + '/markers.csv'\n        csModelPath = projectDir + '/manuscriptModels/'\n\n        # Generate generateGatorScore related paths\n        segmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n\n        # gatorObject related paths\n        spatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\n\n        # Run the pipeline\n        cs.csPipeline(   \n                    # parameters for gatorPredict function\n                    imagePath=imagePath,\n                    csModelPath=csModelPath,\n                    markerChannelMapPath=markerChannelMapPath,\n\n                    # parameters for generateGatorScore function\n                    segmentationMaskPath=segmentationPath,\n\n                    # parameters for gatorObject function\n                    spatialTablePath=spatialTablePath,\n\n                    # parameters to run gator function\n                    # ..\n\n                    # common parameters\n                    verbose=False,\n                    projectDir=projectDir)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python csPipeline.py \\\n                --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif \\\n                --csModelPath /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/ \\\n                --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv \\\n                --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif \\\n                --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv \\\n                --projectDir /Users/aj/Documents/cspotExampleData\n        ```\n\n\n    \"\"\"\n\n    ##########################################################################\n    # STEP: 1 :- PREDICT\n    ##########################################################################\n    function1_args = inspect.signature(csPredict).parameters.keys()\n    # Extract only the arguments that csPredict expects from the keyword arguments\n    function1_kwargs = {k: kwargs[k] for k in kwargs if k in function1_args}\n    # Call csPredict with the extracted arguments\n    csPredict (**function1_kwargs)\n\n\n    ##########################################################################\n    # STEP: 2 :- generateCSScore\n    ##########################################################################\n\n    # derive the probability mask path\n    probPath = pathlib.Path(kwargs['projectDir'] + '/CSPOT/csPredict/')\n    fileName = os.path.basename(kwargs['imagePath'])\n    fileNamePrefix = fileName.split(os.extsep, 1)\n    probabilityMaskPath = str(probPath / (fileNamePrefix[0] + '_cspotPredict.ome.tif'))\n\n    # extract key words for generateCSScore\n    function2_args = inspect.signature(generateCSScore).parameters.keys()\n    function2_kwargs = {k: kwargs[k] for k in kwargs if k in function2_args}\n    generateCSScore (probabilityMaskPath=probabilityMaskPath, **function2_kwargs)\n\n\n    ##########################################################################\n    # STEP: 3 :- Generate csObject\n    ##########################################################################\n\n    # derive the path to CSPOT scores\n    gPath = pathlib.Path(kwargs['projectDir'] + '/CSPOT/csScore/')\n    file_name = pathlib.Path(probabilityMaskPath).stem + '.csv'\n    csScorePath = str(gPath / file_name)\n\n    # extract key words for csObject\n    function3_args = inspect.signature(csObject).parameters.keys()\n    function3_kwargs = {k: kwargs[k] for k in kwargs if k in function3_args}\n    csObject (csScorePath=csScorePath, **function3_kwargs)\n\n    ##########################################################################\n    # STEP: 4 :- Run CSPOT Algorithm\n    ##########################################################################\n\n    # derive the path to CSPOT object\n    oPath = pathlib.Path(kwargs['projectDir'] + '/CSPOT/csObject/')\n    file_name = pathlib.Path(csScorePath).stem + '.h5ad'\n    csObjectPath = str(oPath / file_name)\n\n    # extract key words for running CSPOT\n    function4_args = inspect.signature(cspot).parameters.keys()\n    function4_kwargs = {k: kwargs[k] for k in kwargs if k in function4_args}    \n    cspot (csObject=csObjectPath, **function4_kwargs)\n</code></pre>"},{"location":"Functions/csPredict/","title":"csPredict","text":"<p>Short Description</p> <p>The function <code>csPredict</code> is employed to make predictions about the  expression of a specified marker on cells in new images using the models  generated by <code>csTrain</code>. This calculation is done at the pixel level,  resulting in an output image where the number of channels corresponds to  the number of models applied to the input image. The parameter <code>markerChannelMapPath</code>  is used to associate the image channel number with the relevant model to be applied.</p>"},{"location":"Functions/csPredict/#cspot.csPredict--function","title":"Function","text":""},{"location":"Functions/csPredict/#cspot.csPredict.csPredict","title":"<code>csPredict(imagePath, csModelPath, projectDir, markerChannelMapPath, markerColumnName='marker', channelColumnName='channel', modelColumnName='cspotmodel', verbose=True, GPU=-1, dsFactor=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>imagePath</code> <code>str</code> <p>The path to the .tif file that needs to be processed. </p> required <code>csModelPath</code> <code>str</code> <p>The path to the <code>cspotModel</code> folder. </p> required <code>projectDir</code> <code>str</code> <p>The path to the output directory where the processed images (<code>probabilityMasks</code>) will be saved.</p> required <code>markerChannelMapPath</code> <code>str</code> <p>The path to the marker panel list, which contains information about the markers used in the image.</p> required <code>markerColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the marker names. </p> <code>'marker'</code> <code>channelColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the channel names. </p> <code>'channel'</code> <code>modelColumnName</code> <code>str</code> <p>The name of the column in the marker panel list that contains the model names. </p> <code>'cspotmodel'</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>GPU</code> <code>int</code> <p>An optional argument to explicitly select the GPU to use. The default value is -1, meaning that the GPU will be selected automatically.</p> <code>-1</code> <code>dsFactor</code> <code>float</code> <p>An optional argument to downsample image before inference. The default value is 1, meaning that the image is not downsampled. Use it to modify image pixel size to match training data in the model.</p> <code>1</code> <p>Returns:</p> Type Description <p>Predicted Probability Masks (images): The result will be located at <code>projectDir/CSPOT/csPredict/</code>.</p> Example <pre><code># Path to all the files that are necessary files for running csPredict\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# csPredict related paths\nimagePath = projectDir + '/image/exampleImage.tif'\nmarkerChannelMapPath = projectDir + '/markers.csv'\ncsModelPath = projectDir + '/manuscriptModels/'\n\n# Run the function\ncs.csPredict( imagePath=imagePath,\n         csModelPath=csModelPath,\n         projectDir=projectDir,\n         markerChannelMapPath=markerChannelMapPath, \n         markerColumnName='marker', \n         channelColumnName='channel', \n         modelColumnName='cspotmodel')\n\n# Same function if the user wants to run it via Command Line Interface\npython csPredict.py             --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif             --csModelPath /Users/aj/Documents/cspotExampleData/manuscriptModels             --projectDir /Users/aj/Documents/cspotExampleData             --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv\n</code></pre> Source code in <code>cspot/csPredict.py</code> <pre><code>def csPredict (imagePath,\n                 csModelPath,\n                 projectDir, \n                 markerChannelMapPath, \n                 markerColumnName='marker', \n                 channelColumnName='channel', \n                 modelColumnName='cspotmodel', \n                 verbose=True,\n                 GPU=-1,\n                 dsFactor=1):\n\n    \"\"\"\nParameters:\n    imagePath (str):  \n        The path to the .tif file that needs to be processed. \n\n    csModelPath (str):  \n        The path to the `cspotModel` folder. \n\n    projectDir (str):  \n        The path to the output directory where the processed images (`probabilityMasks`) will be saved.\n\n    markerChannelMapPath (str):  \n        The path to the marker panel list, which contains information about the markers used in the image.\n\n    markerColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the marker names. \n\n    channelColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the channel names. \n\n    modelColumnName (str, optional):  \n        The name of the column in the marker panel list that contains the model names. \n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    GPU (int, optional):  \n        An optional argument to explicitly select the GPU to use. The default value is -1, meaning that the GPU will be selected automatically.\n\n    dsFactor (float, optional):\n        An optional argument to downsample image before inference. The default value is 1, meaning that the image is not downsampled. Use it to modify image pixel size to match training data in the model.\n\nReturns:\n    Predicted Probability Masks (images):  \n        The result will be located at `projectDir/CSPOT/csPredict/`.\n\nExample:\n    \t```python    \n        # Path to all the files that are necessary files for running csPredict\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # csPredict related paths\n        imagePath = projectDir + '/image/exampleImage.tif'\n        markerChannelMapPath = projectDir + '/markers.csv'\n        csModelPath = projectDir + '/manuscriptModels/'\n\n        # Run the function\n        cs.csPredict( imagePath=imagePath,\n                 csModelPath=csModelPath,\n                 projectDir=projectDir,\n                 markerChannelMapPath=markerChannelMapPath, \n                 markerColumnName='marker', \n                 channelColumnName='channel', \n                 modelColumnName='cspotmodel')\n\n        # Same function if the user wants to run it via Command Line Interface\n        python csPredict.py \\\n            --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif \\\n            --csModelPath /Users/aj/Documents/cspotExampleData/manuscriptModels \\\n            --projectDir /Users/aj/Documents/cspotExampleData \\\n            --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv\n\n    \t```\n\n     \"\"\"\n\n    #fileName = pathlib.Path(imagePath).stem\n    fileName = os.path.basename(imagePath)\n    fileName = fileName.split(os.extsep, 1)[0]\n\n    # read the markers.csv\n    maper = pd.read_csv(pathlib.Path(markerChannelMapPath))\n    columnnames =  [word.lower() for word in maper.columns]\n    maper.columns = columnnames\n\n    # making it compatable with mcmicro when no channel info is provided\n    if not set(['channel', 'channels', channelColumnName]).intersection(set(columnnames)):\n        # add a column called 'channel'\n        maper['channel'] = [i + 1 for i in range(len(maper))]\n        columnnames = list(maper.columns)\n\n\n    # identify the marker column name (doing this to make it easier for people who confuse between marker and markers)\n    if markerColumnName not in columnnames:\n        # ckeck if 'markers' or 'marker_name' or 'marker_names' is in columnnames\n        # if so assign that match to markerCol\n        for colname in columnnames:\n            if 'marker' in colname or 'markers' in colname or 'marker_name' in colname or 'marker_names' in colname:\n                markerCol = colname\n                break\n        else:\n            raise ValueError('markerColumnName not found in markerChannelMap, please check')\n    else:\n        markerCol = markerColumnName\n\n\n   # identify the channel column name (doing this to make it easier for people who confuse between channel and channels)\n    if channelColumnName not in columnnames:\n        if channelColumnName != 'channel':\n            raise ValueError('channelColumnName not found in markerChannelMap, please check')\n        if 'channels' in columnnames:\n            channelCol = 'channels'\n        else:\n            raise ValueError('channelColumnName not found in markerChannelMap, please check')\n    else:\n        channelCol = channelColumnName\n\n\n    # identify the CSPOT model column name (doing this to make it easier for people who confuse between cspotmodel and cspotmodels)\n    if modelColumnName not in columnnames:\n        if modelColumnName != 'cspotmodel':\n            raise ValueError('modelColumnName not found in markerChannelMap, please check')\n        if 'cspotmodels' in columnnames:\n            modelCol = 'cspotmodels'\n        else:\n            raise ValueError('modelColumnName not found in markerChannelMap, please check')\n    else:\n        modelCol = modelColumnName\n\n    # remove rowa that have nans in modelCol\n    runMenu = maper.dropna(subset=[modelCol], inplace=False)[[channelCol,markerCol,modelCol]]\n\n    # shortcuts\n    numMarkers = len(runMenu)\n\n    I = skio.imread(imagePath, img_num=0, plugin='tifffile')\n\n\n    probPath = pathlib.Path(projectDir + '/CSPOT/csPredict/')\n    modelPath = pathlib.Path(csModelPath)\n\n    if not os.path.exists(probPath):\n        os.makedirs(probPath,exist_ok=True)\n\n\n    def data(runMenu, \n             imagePath, \n             modelPath, \n             projectDir, \n             dsFactor=dsFactor, \n             GPU=GPU):\n\n        # Loop through the rows of the DataFrame\n        for index, row in runMenu.iterrows():\n            channel = row[channelCol]\n            markerName = row[markerCol]\n            cspotmodel = row[modelCol]\n            if verbose is True:\n                print('Running CSPOT model ' + str(cspotmodel) + ' on channel ' + str(channel) + ' corresponding to marker ' + str(markerName) )\n\n\n            tf.reset_default_graph()\n            UNet2D.singleImageInferenceSetup(pathlib.Path(modelPath / cspotmodel), GPU, -1, -1)\n\n            fileName = os.path.basename(imagePath)\n            fileNamePrefix = fileName.split(os.extsep, 1)\n            fileType = fileNamePrefix[1]\n            if fileType == 'ome.tif' or fileType == 'ome.tiff' or fileType == 'btf':\n                I = skio.imread(imagePath, img_num=int(channel-1), plugin='tifffile')\n            elif fileType == 'tif':\n                I = tifffile.imread(imagePath, key=int(channel-1))\n\n            if I.dtype == 'float32':\n                I = im2double(I) * 255\n            elif I.dtype == 'uint16':\n                I = im2double(I) * 255\n\n            rawVert = I.shape[0]\n            rawHorz = I.shape[1]\n            hsize = int(float(rawVert * float(dsFactor)))\n            vsize = int(float(rawHorz * float(dsFactor)))\n            I = resize(I, (hsize, vsize),preserve_range=True)\n\n            append_kwargs = {\n                'bigtiff': True,\n                'metadata': None,\n                'append': True,\n            }\n            save_kwargs = {\n                'bigtiff': True,\n                'metadata': None,\n                'append': False,\n            }\n\n            PM = np.uint8(255 * UNet2D.singleImageInference(I, 'accumulate',1))\n            PM = resize(PM, (rawVert, rawHorz))\n            yield np.uint8(255 * PM)\n\n    with tifffile.TiffWriter(probPath / (fileName + '_cspotPredict.ome.tif'), bigtiff=True) as tiff:\n        tiff.write(data(runMenu, imagePath, modelPath, probPath, dsFactor=dsFactor, GPU=GPU), shape=(numMarkers,I.shape[0],I.shape[1]), dtype='uint8', metadata={'Channel': {'Name': runMenu[markerCol].tolist()}, 'axes': 'CYX'})\n        UNet2D.singleImageInferenceCleanup()\n</code></pre>"},{"location":"Functions/csTrain/","title":"csTrain","text":"<p>Short Description</p> <p>The function trains a deep learning model for each marker in the provided  training data. To train the <code>cspotModel</code>, simply direct the function to the  <code>TrainingData</code> folder. To train only specific models, specify the folder names  using the <code>trainMarkers</code> parameter. The <code>projectDir</code> remains constant and the  program will automatically create subfolders to save the trained models.</p>"},{"location":"Functions/csTrain/#cspot.csTrain--function","title":"Function","text":""},{"location":"Functions/csTrain/#cspot.csTrain.csTrain","title":"<code>csTrain(trainingDataPath, projectDir, trainMarkers=None, artefactPath=None, imSize=64, nChannels=1, nClasses=2, nExtraConvs=0, nLayers=3, featMapsFact=2, downSampFact=2, ks=3, nOut0=16, stdDev0=0.03, batchSize=16, epochs=100, verbose=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>trainingDataPath</code> <code>str</code> <p>The file path leading to the directory that holds the training data.</p> required <code>projectDir</code> <code>str</code> <p>Path to output directory. The result will be located at <code>projectDir/CSPOT/cspotModel/</code>.</p> required <code>trainMarkers</code> <code>list</code> <p>Generate models for a specified list of markers. By default, models are c reated for all data in the TrainingData folder. If the user wants to limit it to a specific list, they can pass in the folder names (e.g. ['CD3D', 'CD4'])</p> <code>None</code> <code>artefactPath</code> <code>str</code> <p>Path to the directory where the artefacts data is loaded from.</p> <code>None</code> <code>imSize</code> <code>int</code> <p>Image size (assumed to be square).</p> <code>64</code> <code>nChannels</code> <code>int</code> <p>Number of channels in the input image.</p> <code>1</code> <code>nClasses</code> <code>int</code> <p>Number of classes in the classification problem.</p> <code>2</code> <code>nExtraConvs</code> <code>int</code> <p>Number of extra convolutional layers to add to the model.</p> <code>0</code> <code>nLayers</code> <code>int</code> <p>Total number of layers in the model.</p> <code>3</code> <code>featMapsFact</code> <code>int</code> <p>Factor to multiply the number of feature maps by in each layer.</p> <code>2</code> <code>downSampFact</code> <code>int</code> <p>Factor to down-sample the feature maps by in each layer.</p> <code>2</code> <code>ks</code> <code>int</code> <p>Kernel size for the convolutional layers.</p> <code>3</code> <code>nOut0</code> <code>int</code> <p>Number of filters in the first layer.</p> <code>16</code> <code>stdDev0</code> <code>float</code> <p>Standard deviation for the initializer for the first layer.</p> <code>0.03</code> <code>batchSize</code> <code>int</code> <p>Batch size for training.</p> <code>16</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>100</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>images and model</code> <p>The result will be located at <code>projectDir/CSPOT/cspotModel/</code>.</p> Example <pre><code># High level working directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\ntrainingDataPath = projectDir + '/CSPOT/TrainingData'\n\ncs.csTrain(trainingDataPath=trainingDataPath,\n               projectDir=projectDir,\n               trainMarkers=None,\n               artefactPath=None,\n               imSize=64,\n               nChannels=1,\n               nClasses=2,\n               nExtraConvs=0,\n               nLayers=3,\n               featMapsFact=2,\n               downSampFact=2,\n               ks=3,\n               nOut0=16,\n               stdDev0=0.03,\n               batchSize=16,\n               epochs=1)\n\n# Same function if the user wants to run it via Command Line Interface\npython csTrain.py         --trainingDataPath /Users/aj/Documents/cspotExampleData/CSPOT/TrainingData         --projectDir /Users/aj/Documents/cspotExampleData/         --epochs 1\n</code></pre> Source code in <code>cspot/csTrain.py</code> <pre><code>def csTrain(trainingDataPath,\n               projectDir,\n               trainMarkers=None,\n               artefactPath=None,\n               imSize=64,\n               nChannels=1,\n               nClasses=2,\n               nExtraConvs=0,\n               nLayers=3,\n               featMapsFact=2,\n               downSampFact=2,\n               ks=3,\n               nOut0=16,\n               stdDev0=0.03,\n               batchSize=16,\n               epochs=100,\n               verbose=True):\n    \"\"\"\n\nParameters:\n    trainingDataPath (str):\n        The file path leading to the directory that holds the training data.\n\n    projectDir (str):\n        Path to output directory. The result will be located at `projectDir/CSPOT/cspotModel/`.\n\n    trainMarkers (list):\n        Generate models for a specified list of markers. By default, models are c\n        reated for all data in the TrainingData folder. If the user wants to\n        limit it to a specific list, they can pass in the folder names (e.g. ['CD3D', 'CD4'])\n\n    artefactPath (str):\n        Path to the directory where the artefacts data is loaded from.\n\n    imSize (int, optional):\n        Image size (assumed to be square).\n\n    nChannels (int, optional):\n        Number of channels in the input image.\n\n    nClasses (int, optional):\n        Number of classes in the classification problem.\n\n    nExtraConvs (int, optional):\n        Number of extra convolutional layers to add to the model.\n\n    nLayers (int, optional):\n        Total number of layers in the model.\n\n    featMapsFact (int, optional):\n        Factor to multiply the number of feature maps by in each layer.\n\n    downSampFact (int, optional):\n        Factor to down-sample the feature maps by in each layer.\n\n    ks (int, optional):\n        Kernel size for the convolutional layers.\n\n    nOut0 (int, optional):\n        Number of filters in the first layer.\n\n    stdDev0 (float, optional):\n        Standard deviation for the initializer for the first layer.\n\n    batchSize (int, optional):\n        Batch size for training.\n\n    epochs (int, optional):\n        Number of training epochs.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\nReturns:\n    Model (images and model):  \n        The result will be located at `projectDir/CSPOT/cspotModel/`.\n\n\nExample:\n    ```python\n\n    # High level working directory\n    projectDir = '/Users/aj/Documents/cspotExampleData'\n\n    trainingDataPath = projectDir + '/CSPOT/TrainingData'\n\n    cs.csTrain(trainingDataPath=trainingDataPath,\n                   projectDir=projectDir,\n                   trainMarkers=None,\n                   artefactPath=None,\n                   imSize=64,\n                   nChannels=1,\n                   nClasses=2,\n                   nExtraConvs=0,\n                   nLayers=3,\n                   featMapsFact=2,\n                   downSampFact=2,\n                   ks=3,\n                   nOut0=16,\n                   stdDev0=0.03,\n                   batchSize=16,\n                   epochs=1)\n\n    # Same function if the user wants to run it via Command Line Interface\n    python csTrain.py \\\n        --trainingDataPath /Users/aj/Documents/cspotExampleData/CSPOT/TrainingData \\\n        --projectDir /Users/aj/Documents/cspotExampleData/ \\\n        --epochs 1\n\n    ```\n\n\n    \"\"\"\n\n    # Start here\n    # convert to path\n    trainingDataPath = pathlib.Path(trainingDataPath)\n    # identify all the data folders within the given TrainingData folder\n    directories = [x for x in trainingDataPath.iterdir() if x.is_dir()]\n    # keep only folders that the user have requested\n    if trainMarkers is not None:\n        if isinstance(trainMarkers, str):\n            trainMarkers = [trainMarkers]\n        directories = [x for x in directories if x.stem in trainMarkers]\n\n    # optional artifacts\n    if artefactPath is not None:\n        artefactPath = pathlib.Path(artefactPath)\n        artefactTrainPath = pathlib.Path(artefactPath / 'training')\n        artefactValidPath = pathlib.Path(artefactPath / 'validation')\n    else:\n        artefactPath = ''\n        artefactTrainPath = ''\n        artefactValidPath = ''\n    # Need to run the training for each marker\n\n    def csTrainInternal(trainingDataPath,\n                           projectDir,\n                           artefactPath,\n                           imSize,\n                           nChannels,\n                           nClasses,\n                           nExtraConvs,\n                           nLayers,\n                           featMapsFact,\n                           downSampFact,\n                           ks,\n                           nOut0,\n                           stdDev0,\n                           batchSize,\n                           epochs):\n        # process the file name\n        finalName = trainingDataPath.stem\n\n        # paths for loading data\n        trainPath = pathlib.Path(trainingDataPath / 'training')\n        validPath = pathlib.Path(trainingDataPath / 'validation')\n        testPath = pathlib.Path(trainingDataPath / 'test')\n\n        # Paths for saving data\n        logPath = pathlib.Path(\n            projectDir + '/CSPOT/csTrain/' + finalName + '/tempTFLogs/')\n        modelPath = pathlib.Path(projectDir + '/CSPOT/cspotModel/' + finalName)\n        pmPath = pathlib.Path(projectDir + '/CSPOT/csTrain/' +\n                              finalName + '/TFprobMaps/')\n\n        # set up the model\n        UNet2D.setup(imSize=imSize,\n                     nClasses=nClasses,\n                     nChannels=nChannels,\n                     nExtraConvs=nExtraConvs,\n                     nDownSampLayers=nLayers,\n                     featMapsFact=featMapsFact,\n                     downSampFact=downSampFact,\n                     kernelSize=ks,\n                     nOut0=nOut0,\n                     stdDev0=stdDev0,\n                     batchSize=batchSize)\n\n        # train the model\n        UNet2D.train(trainPath=trainPath,\n                     validPath=validPath,\n                     testPath=testPath,\n                     artTrainPath=artefactTrainPath,\n                     artValidPath=artefactValidPath,\n                     logPath=logPath,\n                     modelPath=modelPath,\n                     pmPath=pmPath,\n                     restoreVariables=False,\n                     nSteps=epochs,\n                     gpuIndex=0,\n                     testPMIndex=2)\n\n    # Run the function on all markers\n    def r_csTrainInternal(x): return csTrainInternal(trainingDataPath=x,\n                                                           projectDir=projectDir,\n                                                           artefactPath=artefactPath,\n                                                           imSize=imSize,\n                                                           nChannels=nChannels,\n                                                           nClasses=nClasses,\n                                                           nExtraConvs=nExtraConvs,\n                                                           nLayers=nLayers,\n                                                           featMapsFact=featMapsFact,\n                                                           downSampFact=downSampFact,\n                                                           ks=ks,\n                                                           nOut0=nOut0,\n                                                           stdDev0=stdDev0,\n                                                           batchSize=batchSize,\n                                                           epochs=epochs)\n\n    csTrainInternal_result = list(\n        map(r_csTrainInternal,  directories))  # Apply function\n\n    # Finish Job\n    if verbose is True:\n        print('CSPOT Models have been generated, head over to \"' + str(projectDir) + '/CSPOT/cspotModel\" to view results')\n</code></pre>"},{"location":"Functions/cspot/","title":"cspot","text":"<p>Short Description</p> <p>The cspot function identifies positive and negative cells for a marker. To  get optimal results, consider adjusting the following parameters:  </p> <ol> <li>The <code>csObject</code> parameter can accept either the loaded csObject or a path to the <code>.h5ad</code> file.  </li> <li>The <code>minAbundance</code> parameter determines the minimum percentage of a marker's abundance to consider it a failure.  </li> <li>It is suggested to drop background markers with the <code>dropMarkers</code> option as they can interfere with classifiers.  </li> <li><code>RobustScale</code>: Scaling the data before training the classifier model has been shown to improve results.  However, in our experience a simple log transformation was found to be sufficient.   </li> </ol>"},{"location":"Functions/cspot/#cspot.cspot--function","title":"Function","text":""},{"location":"Functions/cspot/#cspot.cspot.cspot","title":"<code>cspot(csObject, csScore='csScore', minAbundance=0.005, percentiles=[1, 20, 80, 99], dropMarkers=None, RobustScale=False, log=True, stringentThreshold=False, x_coordinate='X_centroid', y_coordinate='Y_centroid', imageid='imageid', random_state=0, rescaleMethod='minmax', label='cspotOutput', verbose=True, projectDir=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObject</code> <code>anndata</code> <p>Pass the <code>csObject</code> loaded into memory or a path to the <code>csObject</code>  file (.h5ad).</p> required <code>csScore</code> <code>str</code> <p>Include the label used for saving the <code>csScore</code> within the CSPOT object.</p> <code>'csScore'</code> <code>minAbundance</code> <code>float</code> <p>Specify the minimum percentage of cells that should express a specific marker in order to determine if the marker is considered a failure. A good approach is to consider the lowest percentage of rare cells expected within the dataset.</p> <code>0.005</code> <code>percentiles</code> <code>list</code> <p>Specify the interval of percentile levels of the expression utilized to intialize the GMM. The cells falling within these percentiles are utilized to distinguish between negative cells (first two values) and positive cells (last two values).</p> <code>[1, 20, 80, 99]</code> <code>dropMarkers</code> <code>list</code> <p>Specify a list of markers to be removed from the analysis, for example: <code>[\"background_channel1\", \"background_channel2\"]</code>. </p> <code>None</code> <code>RobustScale</code> <code>bool</code> <p>When set to True, the data will be subject to Robust Scaling before the Gradient Boosting Classifier is trained. </p> <code>False</code> <code>log</code> <code>bool</code> <p>Apply <code>log1p</code> transformation on the data, unless it has already been log transformed in which case set it to <code>False</code>. </p> <code>True</code> <code>stringentThreshold</code> <code>bool</code> <p>The Gaussian Mixture Model (GMM) is utilized to distinguish positive and  negative cells by utilizing csScores. The stringentThreshold can be utilized  to further refine the classification of positive and negative cells.  By setting it to True, cells with csScore below the mean of the negative  distribution and above the mean of the positive distribution will be  labeled as true negative and positive, respectively.</p> <code>False</code> <code>x_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the X coordinates for each cell. </p> <code>'X_centroid'</code> <code>y_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the Y coordinates for each cell.</p> <code>'Y_centroid'</code> <code>imageid</code> <code>str</code> <p>The name of the column that holds the unique image ID. </p> <code>'imageid'</code> <code>random_state</code> <code>int</code> <p>Seed used by the random number generator. </p> <code>0</code> <code>rescaleMethod</code> <code>string</code> <p>Choose between <code>sigmoid</code> and <code>minmax</code>.</p> <code>'minmax'</code> <code>label</code> <code>str</code> <p>Assign a label for the object within <code>adata.uns</code> where the predictions from CSPOT will be stored. </p> <code>'cspotOutput'</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>projectDir</code> <code>str</code> <p>Provide the path to the output directory. The result will be located at <code>projectDir/CSPOT/cspotOutput/</code>. </p> <code>None</code> <code>**kwargs</code> <code>keyword parameters</code> <p>Additional arguments to pass to the <code>HistGradientBoostingClassifier()</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>If projectDir is provided the updated CSPOT Object will saved within the provided projectDir.</p> Example <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Documents/cspotExampleData'\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\n# Run the function\nadata = cs.cspot ( csObject=csObject,\n            csScore='csScore',\n            minAbundance=0.005,\n            percentiles=[1, 20, 80, 99],\n            dropMarkers = None,\n            RobustScale=False,\n            log=True,\n            x_coordinate='X_centroid',\n            y_coordinate='Y_centroid',\n            imageid='imageid',\n            random_state=0,\n            rescaleMethod='sigmoid',\n            label='cspotOutput',\n            verbose=False,\n           projectDir=projectDir)\n\n\n# Same function if the user wants to run it via Command Line Interface\npython cspot.py             --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/cspot.py</code> <pre><code>def cspot (csObject,\n           csScore='csScore',\n           minAbundance=0.005,\n           percentiles=[1, 20, 80, 99],\n           dropMarkers = None,\n           RobustScale=False,\n           log=True,\n           stringentThreshold=False,\n           x_coordinate='X_centroid',\n           y_coordinate='Y_centroid',\n           imageid='imageid',\n           random_state=0,\n           rescaleMethod='minmax',\n           label='cspotOutput',\n           verbose=True,\n           projectDir=None, **kwargs):\n    \"\"\"\nParameters:\n    csObject (anndata):\n        Pass the `csObject` loaded into memory or a path to the `csObject` \n        file (.h5ad).\n\n    csScore (str, optional):\n        Include the label used for saving the `csScore` within the CSPOT object.\n\n    minAbundance (float, optional):\n        Specify the minimum percentage of cells that should express a specific\n        marker in order to determine if the marker is considered a failure.\n        A good approach is to consider the lowest percentage of rare cells\n        expected within the dataset.\n\n    percentiles (list, optional):\n        Specify the interval of percentile levels of the expression utilized to intialize\n        the GMM. The cells falling within these percentiles are utilized to distinguish\n        between negative cells (first two values) and positive cells (last two values).\n\n    dropMarkers (list, optional):\n        Specify a list of markers to be removed from the analysis, for\n        example: `[\"background_channel1\", \"background_channel2\"]`. \n\n    RobustScale (bool, optional):\n        When set to True, the data will be subject to Robust Scaling before the\n        Gradient Boosting Classifier is trained. \n\n    log (bool, optional):\n        Apply `log1p` transformation on the data, unless it has already been log\n        transformed in which case set it to `False`. \n\n    stringentThreshold (bool, optional):\n        The Gaussian Mixture Model (GMM) is utilized to distinguish positive and \n        negative cells by utilizing csScores. The stringentThreshold can be utilized \n        to further refine the classification of positive and negative cells. \n        By setting it to True, cells with csScore below the mean of the negative \n        distribution and above the mean of the positive distribution will be \n        labeled as true negative and positive, respectively.\n\n    x_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        X coordinates for each cell. \n\n    y_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        Y coordinates for each cell.\n\n    imageid (str, optional):\n        The name of the column that holds the unique image ID. \n\n    random_state (int, optional):\n        Seed used by the random number generator. \n\n    rescaleMethod (string, optional):\n        Choose between `sigmoid` and `minmax`.\n\n    label (str, optional):\n        Assign a label for the object within `adata.uns` where the predictions\n        from CSPOT will be stored. \n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    projectDir (str, optional):\n        Provide the path to the output directory. The result will be located at\n        `projectDir/CSPOT/cspotOutput/`. \n\n    **kwargs (keyword parameters):\n        Additional arguments to pass to the `HistGradientBoostingClassifier()` function.\n\nReturns:\n    csObject (anndata):\n        If projectDir is provided the updated CSPOT Object will saved within the\n        provided projectDir.\n\nExample:\n        ```python\n\n        # set the working directory &amp; set paths to the example data\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n        csObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n\n        # Run the function\n        adata = cs.cspot ( csObject=csObject,\n                    csScore='csScore',\n                    minAbundance=0.005,\n                    percentiles=[1, 20, 80, 99],\n                    dropMarkers = None,\n                    RobustScale=False,\n                    log=True,\n                    x_coordinate='X_centroid',\n                    y_coordinate='Y_centroid',\n                    imageid='imageid',\n                    random_state=0,\n                    rescaleMethod='sigmoid',\n                    label='cspotOutput',\n                    verbose=False,\n                   projectDir=projectDir)\n\n\n        # Same function if the user wants to run it via Command Line Interface\n        python cspot.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n\n        ```\n\n    \"\"\"\n\n    # testing\n    #csObject= '/Users/aj/Dropbox (Partners HealthCare)/nirmal lab/resources/exemplarData/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n    #csScore='csScore'; minAbundance=0.005; percentiles=[1, 20, 80, 99]; dropMarkers = None\n    #RobustScale=False; log=True; stringentThreshold=False; x_coordinate='X_centroid'; y_coordinate='Y_centroid'\n    #imageid='imageid'; random_state=0; rescaleMethod='minmax'; label='cspotOutput'; verbose=True; projectDir=None\n\n    # Load the andata object\n    if isinstance(csObject, str):\n        adata = ad.read(csObject)\n        csObject = [csObject]\n        csObjectPath = [pathlib.Path(p) for p in csObject]\n    else:\n        adata = csObject.copy()\n\n    # break the function if csScore is not detectable\n    def check_key_exists(dictionary, key):\n        try:\n            # Check if the key exists in the dictionary\n            value = dictionary[key]\n        except KeyError:\n            # Return an error if the key does not exist\n            return \"Error: \" + str(csScore) + \" does not exist, please check!\"\n    # Test\n    check_key_exists(dictionary=adata.uns, key=csScore)\n\n\n    ###########################################################################\n    # SOME GENERIC FUNCTIONS\n    ###########################################################################\n\n    # used in (step 1)\n    def get_columns_with_low_values(df, minAbundance):\n        columns_to_keep = []\n        for column in df.columns:\n            num_rows_with_high_values = len(df[df[column] &gt; 0.6])\n            if num_rows_with_high_values / len(df) &lt; minAbundance:\n                columns_to_keep.append(column)\n        return columns_to_keep\n\n    # count the number of pos and neg elements in a list\n    def count_pos_neg(lst):\n        arr = np.array(lst)\n        result = {'pos': np.sum(arr == 'pos'), 'neg': np.sum(arr == 'neg')}\n        result['pos'] = result['pos'] if result['pos'] &gt; 0 else 0\n        result['neg'] = result['neg'] if result['neg'] &gt; 0 else 0\n        return result\n\n    # alternative to find if markers failed\n    def simpleGMM_failedMarkers (df, n_components, minAbundance, random_state):\n        # prepare data\n        columns_to_keep = []\n        for column in df.columns:\n            #print(str(column))\n            colValue = df[[column]].values  \n            colValue[0] = 0; colValue[1] = 1;   # force the model to converge from 0-1\n            gmm = GaussianMixture(n_components=n_components,  random_state=random_state)\n            gmm.fit(colValue)\n            predictions = gmm.predict(colValue)\n            # Get the mean of each Gaussian component\n            means = gmm.means_.flatten()\n            # Sort the mean values in ascending order\n            sorted_means = np.sort(means)\n            # Assign 'pos' to rows with higher mean distribution and 'neg' to rows with lower mean distribution\n            labels = np.where(predictions == np.argmax(means), 'pos', 'neg')\n            # count pos and neg\n            counts = count_pos_neg(labels)\n            # find if the postive cells is less than the user defined min abundance\n            if counts['pos'] / len(df)  &lt; minAbundance:\n                columns_to_keep.append(column)\n        return columns_to_keep\n\n\n    # preprocess data (step-4)\n    def pre_process (data, log=log):\n        # clip outliers\n        def clipping (x):\n            clip = x.clip(lower =np.percentile(x,0.01), upper=np.percentile(x,99.99)).tolist()\n            return clip\n        processsed_data = data.apply(clipping)\n        if log is True:\n            processsed_data = np.log1p(processsed_data)\n        return processsed_data\n    # preprocess data (step-5)\n    def apply_transformation (data):\n        # rescale the data\n        transformer = RobustScaler().fit(data)\n        processsed_data = pd.DataFrame(transformer.transform(data), columns = data.columns, index=data.index)\n        return processsed_data\n\n    # GMM\n    def simpleGMM (data, n_components, means_init, random_state):\n        gmm = GaussianMixture(n_components=n_components, means_init=means_init,  random_state=random_state)\n        gmm.fit(data)\n        # Predict the class labels for each sample\n        predictions = gmm.predict(data)\n        # Get the mean of each Gaussian component\n        means = gmm.means_.flatten()\n        # Sort the mean values in ascending order\n        sorted_means = np.sort(means)\n        # Assign 'pos' to rows with higher mean distribution and 'neg' to rows with lower mean distribution\n        labels = np.where(predictions == np.argmax(means), 'pos', 'neg')\n\n        return labels, sorted_means \n\n    # take in two list (ccategorical and numerical) and return mean values\n    def array_mean (labels, values):\n        # Create a defaultdict with an empty list as the default value\n        result = defaultdict(list)\n        # Iterate over the labels and values arrays\n        for label, value in zip(labels, values):\n            # Add the value to the list for the corresponding label\n            result[label].append(value)\n        # Calculate the mean for each label and store it in the dictionary\n        for label, value_list in result.items():\n            result[label] = np.mean(value_list)\n        return result\n\n    # match two arrays and return seperate lists\n    def array_match (labels, names):\n        # Create a defaultdict with an empty list as the default value\n        result = defaultdict(list)\n        # Iterate over the labels and names arrays\n        for label, name in zip(labels, names):\n            # Add the name to the list for the corresponding label\n            result[label].append(name)\n        return result\n\n    # return the mean between two percentiles\n    def meanPercentile (values, lowPercentile, highPercentile):\n        # Calculate the 1st percentile value\n        p1 = np.percentile(values, lowPercentile)\n        # Calculate the 20th percentile value\n        p20 = np.percentile(values, highPercentile)\n        # Select the values between the 1st and 20th percentile using numpy.where()\n        filtered_values = np.where((values &gt;= p1) &amp; (values &lt;= p20))\n        # Calculate the mean of the filtered values\n        meanVal = np.mean(values[filtered_values])\n        return meanVal\n\n    # return the mean between two percentiles\n    def indexPercentile (processed_data, marker, lowPercentile, highPercentile):\n        values = processed_data[marker].values\n        # Calculate the 1st percentile value\n        p1 = np.percentile(values, lowPercentile)\n        # Calculate the 20th percentile value\n        p20 = np.percentile(values, highPercentile)\n        # Select the values between the 1st and 20th percentile using numpy.where()\n        filtered_values = np.where((values &gt;= p1) &amp; (values &lt;= p20))\n        # Calculate the mean of the filtered values\n        idx = processed_data[marker].iloc[filtered_values].index\n        return idx\n\n    # Used for rescaling data\n    # used to find the mid point of GMM mixtures\n    def find_midpoint(data, labels):\n      # Convert data and labels to NumPy arrays\n      data = np.array(data)\n      labels = np.array(labels)\n      # Get the indices that would sort the data and labels arrays\n      sort_indices = np.argsort(data)\n      # Sort the data and labels arrays using the sort indices\n      sorted_data = data[sort_indices]\n      sorted_labels = labels[sort_indices]\n      # Convert labels to binary values (0 for 'neg', 1 for 'pos')\n      binary_labels = np.where(sorted_labels == 'pos', 1, 0)\n      # Compute cumulative sums\n      cumulative_pos = np.cumsum(binary_labels)\n      cumulative_neg = np.arange(1, len(sorted_labels) + 1) - cumulative_pos\n      # Compute the difference and find the index where the absolute difference is maximized\n      midpoint_index = np.argmax(np.abs(cumulative_pos - cumulative_neg))\n      # Find the index where the 'neg' and 'pos' labels meet\n      # midpoint_index = np.argmax(sorted_labels == 'pos') # old method\n      # Return the value at the midpoint index\n      return sorted_data[midpoint_index]\n\n    # Used for reassigning some of the wrong 'neg' and 'pos' within data given a midpoint\n    def modify_negatives_vectorized(data, labels, midpoint):\n      # Convert data and labels to NumPy arrays\n      data = np.array(data)\n      labels = np.array(labels)\n      # Calculating the mean of 'neg' instances  (used to replace wrongly assigned pos instances)\n      neg_mean = np.mean(data[labels == 'neg'])\n      # Get the indices that would sort the data and labels arrays\n      sort_indices = np.argsort(data)\n      # Sort the data and labels arrays using the sort indices\n      sorted_data = data[sort_indices]\n      sorted_labels = labels[sort_indices]\n      # Find the index where the sorted data is greater than or equal to the midpoint value\n      midpoint_index = np.argmax(sorted_data &gt;= midpoint)\n      # Find all the elements in the sorted labels array with a value of 'neg' after the midpoint index\n      neg_mask = np.logical_and(sorted_labels == 'neg', np.arange(len(sorted_data)) &gt;= midpoint_index)\n      # Modify the value of the elements to be equal to the midpoint value\n      sorted_data[neg_mask] = neg_mean\n      # Find all the elements in the sorted labels array with a value of 'pos' before the midpoint index\n      pos_mask = np.logical_and(sorted_labels == 'pos', np.arange(len(sorted_data)) &lt; midpoint_index)\n      # Modify the value of the elements to be equal to the midpoint value plus 0.1\n      sorted_data[pos_mask] = midpoint + 0.1\n      # Reorder the data array to the original order\n      reordered_data = sorted_data[np.argsort(sort_indices)]\n      # Return the modified data\n      return reordered_data\n\n# =============================================================================\n#     def modify_prediction_results(rawData, prediction_results, failedMarkersinData):\n#         # Identify the index of the maximum value for each column in rawData\n#         max_index = rawData.idxmax()\n#         # Iterate through the specified columns of rawData\n#         for col in failedMarkersinData:\n#             # Get the index of the maximum value\n#             max_row = max_index[col]\n#             # Modify the corresponding index in prediction_results\n#             prediction_results[col].at[max_row] = 'pos'\n# =============================================================================\n\n    def get_second_highest_values(df, failedMarkersinData):\n        # get the second largest value for each column in the list\n        second_highest_values = df[failedMarkersinData].max()\n        # convert the series to a dictionary\n        second_highest_values_dict = second_highest_values.to_dict()\n        return second_highest_values_dict\n\n\n    # sigmoid scaling to convert the data between 0-1 based on the midpoint\n    def sigmoid(x, midpoint):\n        return 1 / (1 + np.exp(-(x - midpoint)))\n\n    # rescale based on min-max neg -&gt; 0-4.9 and pos -&gt; 0.5-1\n    def scale_data(data, midpoint):\n        below_midpoint = data[data &lt;= midpoint]\n        above_midpoint = data[data &gt; midpoint]\n        indices_below = np.where(data &lt;= midpoint)[0]\n        indices_above = np.where(data &gt; midpoint)[0]\n\n        # Scale the group below the midpoint\n        min_below = below_midpoint.min()\n        max_below = below_midpoint.max()\n        range_below = max_below - min_below\n        below_midpoint = (below_midpoint - min_below) / range_below\n\n        # Scale the group above the midpoint\n        if len(above_midpoint) &gt; 0:\n            min_above = above_midpoint.min()\n            max_above = above_midpoint.max()\n            range_above = max_above - min_above\n            above_midpoint = (above_midpoint - min_above) / range_above\n        else:\n            above_midpoint = []\n\n        # Re-assemble the data in the original order by using the indices of the values in each group\n        result = np.empty(len(data))\n        result[indices_below] = below_midpoint * 0.499999999\n        if len(above_midpoint) &gt; 0:\n            result[indices_above] = above_midpoint * 0.50 + 0.50\n        return result\n\n    # classifies data based on a given midpoint\n    def classify_data(data, sorted_means):\n        data = np.array(data)\n        low = sorted_means[0]\n        high = sorted_means[1]\n        return np.where(data &lt; low, 'neg', np.where(data &gt; high, 'pos', 'unknown'))\n\n\n\n    ###########################################################################\n    # step-1 : Identify markers that have failed in this dataset\n    ###########################################################################\n    # 0ld thresholding method\n    #failed_markers = get_columns_with_low_values (df=adata.uns[csScore],minAbundance=minAbundance)\n\n    # New GMM method\n    failed_markers = simpleGMM_failedMarkers (df=adata.uns[csScore], \n                                              n_components=2, \n                                              minAbundance=minAbundance, \n                                              random_state=random_state)\n\n    # to store in adata\n    failed_markers_dict = {adata.obs[imageid].unique()[0] : failed_markers}\n\n    if verbose is True:\n        print('Failed Markers are: ' + \", \".join(str(x) for x in failed_markers))\n\n    ###########################################################################\n    # step-2 : Prepare DATA\n    ###########################################################################\n\n    rawData = pd.DataFrame(adata.raw.X, columns= adata.var.index, index = adata.obs.index)\n    rawprocessed = pre_process (rawData, log=log)\n    # drop user defined markers; note if a marker is dropped it will not be part of the\n    # final prediction too. Markers that failed although removed from prediction will\n    # still be included in the final predicted output as all negative.\n    if dropMarkers is not None:\n        if isinstance(dropMarkers, str):\n            dropMarkers = [dropMarkers]\n        pre_processed_data = rawprocessed.drop(columns=dropMarkers)\n    else:\n        pre_processed_data = rawprocessed.copy()\n\n    # also drop failed markers\n    failedMarkersinData = list(set(pre_processed_data.columns).intersection(failed_markers))\n\n    # final dataset that will be used for prediction\n    pre_processed_data = pre_processed_data.drop(columns=failedMarkersinData)\n\n    # isolate the unet probabilities\n    probQuant_data = adata.uns[csScore]\n\n    # list of markers to process: (combined should match data)\n    expression_unet_common = list(set(pre_processed_data.columns).intersection(set(probQuant_data.columns)))\n    only_expression = list(set(pre_processed_data.columns).difference(set(probQuant_data.columns)))\n\n\n    ###########################################################################\n    # step-4 : Identify a subset of true positive and negative cells\n    ###########################################################################\n\n    # marker = 'CD4'\n    def bonafide_cells (marker,\n                        expression_unet_common, only_expression,\n                        pre_processed_data, probQuant_data, random_state,\n                        percentiles):\n\n\n        if marker in expression_unet_common:\n            if verbose is True:\n                print(\"NN marker: \" + str(marker))\n            # run GMM on probQuant_data\n            X = probQuant_data[marker].values.reshape(-1,1)\n            # Fit the GMM model to the data\n            labels, sorted_means = simpleGMM (data=X, n_components=2, means_init=None, random_state=random_state)\n\n            # Identify cells that are above a certain threshold in the probability maps\n            if stringentThreshold is True:\n                labels = classify_data (data=probQuant_data[marker], sorted_means=sorted_means)\n\n            # find the mean of the pos and neg cells in expression data given the labels\n            values = pre_processed_data [marker].values\n            Pmeans = array_mean (labels, values)\n            # Format mean to pass into next GMM\n            Pmean = np.array([[ Pmeans.get('neg')], [Pmeans.get('pos')]])\n\n            # Now run GMM on the expression data\n            Y = pre_processed_data[marker].values.reshape(-1,1)\n            labelsE, sorted_meansE = simpleGMM (data=Y, n_components=2, means_init=Pmean, random_state=random_state)\n\n            # Match the labels and index names to identify which cells are pos and neg\n            expCells = array_match (labels=labels, names=pre_processed_data.index)\n            probCells = array_match (labels=labelsE, names=pre_processed_data.index)\n            # split it\n            expCellsPos = expCells.get('pos', []) ; expCellsNeg = expCells.get('neg', [])\n            probCellsPos = probCells.get('pos', []) ; probCellsNeg = probCells.get('neg', [])\n            # find common elements\n            pos = list(set(expCellsPos).intersection(set(probCellsPos)))\n            neg = list(set(expCellsNeg).intersection(set(probCellsNeg)))\n\n            # print no of cells\n            if verbose is True:\n                print(\"POS cells: {} and NEG cells: {}.\".format(len(pos), len(neg)))\n\n            # check if the length is less than 20 cells and if so add the marker to only_expression\n            if len(pos) &lt; 20 or len(neg) &lt; 20: ## CHECK!\n                only_expression.append(marker)\n                if verbose is True:\n                    print (\"As the number of POS/NEG cells is low for \" + str(marker) + \", GMM will fitted using only expression values.\")\n\n\n        if marker in only_expression:\n            if verbose is True:\n                print(\"Expression marker: \" + str(marker))\n            # Run GMM only on the expression data\n            Z = pre_processed_data[marker].values.reshape(-1,1)\n            # if user provides manual percentile, use it to intialize the GMM\n            if percentiles is not None:\n                percentiles.sort()\n                F = pre_processed_data[marker].values\n                # mean of cells within defined threshold\n                lowerPercent = meanPercentile (values=F, lowPercentile=percentiles[0], highPercentile=percentiles[1])\n                higherPercent = meanPercentile (values=F, lowPercentile=percentiles[2], highPercentile=percentiles[3])\n                # Format mean to pass into next GMM\n                Pmean = np.array([[lowerPercent], [higherPercent]])\n                labelsOE, sorted_meansOE = simpleGMM (data=Z, n_components=2, means_init=Pmean, random_state=random_state)\n            else:\n                labelsOE, sorted_meansOE = simpleGMM (data=Z, n_components=2, means_init=None, random_state=random_state)\n            # match labels with indexname\n            OEcells = array_match (labels=labelsOE, names=pre_processed_data.index)\n            # split it\n            pos = OEcells.get('pos', []) ; neg = OEcells.get('neg', [])\n\n            # randomly subset 70% of the data to return\n            random.seed(random_state); pos = random.sample(pos, k=int(len(pos) * 0.7))\n            random.seed(random_state); neg = random.sample(neg, k=int(len(neg) * 0.7))\n\n            # print no of cells\n            if verbose is True:\n                print(\"Defined POS cells is {} and NEG cells is {}.\".format(len(pos), len(neg)))\n\n            # What happens of POS/NEG is less than 20\n            # check if the length is less than 20 cells and if so add the marker to only_expression\n            if len(pos) &lt; 20 or len(neg) &lt; 20:  ## CHECK!\n                if percentiles is None:\n                    percentiles = [1,20,80,99]\n                neg = list(indexPercentile (pre_processed_data, marker, lowPercentile=percentiles[0], highPercentile=percentiles[1]))\n                pos = list(indexPercentile (pre_processed_data, marker, lowPercentile=percentiles[2], highPercentile=percentiles[3]))\n                if verbose is True:\n                    print (\"As the number of POS/NEG cells is low for \" + str(marker) + \", cells falling within the given percentile \" + str(percentiles) + ' was used.')\n\n        # return the output\n        return marker, pos, neg\n\n    # Run the function on all markers\n    if verbose is True:\n        print(\"Intial GMM Fitting\")\n    r_bonafide_cells = lambda x: bonafide_cells (marker=x,\n                                                expression_unet_common=expression_unet_common,\n                                                only_expression=only_expression,\n                                                pre_processed_data=pre_processed_data,\n                                                probQuant_data=probQuant_data,\n                                                random_state=random_state,\n                                                percentiles=percentiles)\n    bonafide_cells_result = list(map(r_bonafide_cells,  pre_processed_data.columns)) # Apply function\n\n\n\n    ###########################################################################\n    # step-5 : Generate training data for the Gradient Boost Classifier\n    ###########################################################################\n\n    # bonafide_cells_result = bonafide_cells_result[8]\n    def trainingData (bonafide_cells_result, pre_processed_data, RobustScale):\n        # uravel the data\n        marker = bonafide_cells_result[0]\n        pos = bonafide_cells_result[1]\n        neg = bonafide_cells_result[2]\n        PD = pre_processed_data.copy()\n\n        if verbose is True:\n            print('Processing: ' + str(marker))\n\n        # class balance the number of pos and neg cells based on the lowest denominator\n        if len(neg) &lt; len(pos):\n            pos = random.sample(pos, len(neg))\n        else:\n            neg = random.sample(neg, len(pos))\n\n        # processed data with pos and neg info\n\n\n        #PD['label'] = ['pos' if index in pos else 'neg' if index in neg else 'other' for index in PD.index]\n        PD['label'] = np.where(PD.index.isin(pos), 'pos', np.where(PD.index.isin(neg), 'neg', 'other'))\n        combined_data = PD.copy()\n\n        # scale data if requested\n        if RobustScale is True:\n            combined_data_labels = combined_data[['label']]\n            combined_data = combined_data.drop('label', axis=1)\n            combined_data = apply_transformation(combined_data)\n            combined_data = pd.concat ([combined_data, combined_data_labels], axis=1)\n\n        # return final output\n        return marker, combined_data\n\n    # Run the function\n    if verbose is True:\n        print(\"Building the Training Data\")\n    r_trainingData = lambda x: trainingData (bonafide_cells_result=x,\n                                                 pre_processed_data=pre_processed_data,\n                                                 RobustScale=RobustScale)\n    trainingData_result = list(map(r_trainingData, bonafide_cells_result)) # Apply function\n\n\n    ###########################################################################\n    # step-6 : Train and Predict on all cells\n    ###########################################################################\n\n    # trainingData_result = trainingData_result[2]\n    def csClassifier (trainingData_result,random_state):\n\n        #unravel data\n        marker = trainingData_result[0]\n        combined_data = trainingData_result[1]\n\n        # prepare the data for predicition\n        index_names_to_drop = [index for index in combined_data.index if 'npu' in index or 'nnu' in index]\n        predictionData = combined_data.drop(index=index_names_to_drop, inplace=False)\n        predictionData = predictionData.drop('label', axis=1)\n\n        if verbose is True:\n            print('classifying: ' + str(marker))\n\n        # shuffle the data\n        combined_data = combined_data.sample(frac=1) # shuffle it\n\n        # prepare the training data and training labels\n        to_train = combined_data.loc[combined_data['label'].isin(['pos', 'neg'])]\n        training_data = to_train.drop('label', axis=1)\n        training_labels = to_train[['label']]\n        trainD = training_data.values\n        trainL = training_labels.values\n        trainL = [item for sublist in trainL for item in sublist]\n\n\n        #start = time.time()\n\n        # Function for the classifier\n        #mlp = MLPClassifier(**kwargs) # CHECK\n        #model = GradientBoostingClassifier()\n\n        model = HistGradientBoostingClassifier(random_state=random_state, **kwargs)\n        #model = HistGradientBoostingClassifier(random_state=random_state)\n        model.fit(trainD, trainL)\n        # predict\n        pred = model.predict(predictionData.values)\n        prob = model.predict_proba(predictionData.values)\n        prob = [item[0] for item in prob]\n\n        #end = time.time()\n        #print(end - start)\n\n        # find the mid point based on the predictions (used for rescaling data later)\n        midpoint = find_midpoint(data=predictionData[marker].values, labels=pred)\n\n        # return\n        return marker, pred, prob, midpoint\n\n    # Run the function\n    if verbose is True:\n        print(\"Fitting model for classification:\")\n    r_csClassifier = lambda x: csClassifier (trainingData_result=x,random_state=random_state)\n    csClassifier_result = list(map(r_csClassifier, trainingData_result))\n\n    ###########################################################################\n    # step-7 : Consolidate the results into a dataframe\n    ###########################################################################\n\n    # consolidate results\n    markerOrder = []\n    for i in range(len(csClassifier_result)):\n        markerOrder.append(csClassifier_result[i][0])\n\n    prediction_results = []\n    for i in range(len(csClassifier_result)):\n        prediction_results.append(csClassifier_result[i][1])\n    prediction_results = pd.DataFrame(prediction_results, index=markerOrder, columns=pre_processed_data.index).T\n\n    probability_results = []\n    for i in range(len(csClassifier_result)):\n        probability_results.append(csClassifier_result[i][2])\n    probability_results = pd.DataFrame(probability_results, index=markerOrder, columns=pre_processed_data.index).T\n\n    midpoints_dict = {}\n    for i in range(len(csClassifier_result)):\n        midpoints_dict[markerOrder[i]] = csClassifier_result[i][3]\n\n    ###########################################################################\n    # step-8 : Final cleaning of predicted results with UNET results\n    ###########################################################################\n\n    # bonafide_cells_result_copy = bonafide_cells_result.copy()\n    # bonafide_cells_result = bonafide_cells_result[8]\n\n    def anomalyDetector (pre_processed_data, bonafide_cells_result, prediction_results):\n        # unravel data\n        marker = bonafide_cells_result[0]\n        pos = bonafide_cells_result[1]\n        neg = bonafide_cells_result[2]\n\n        if verbose is True:\n            print(\"Processing: \" + str(marker))\n        # prepare data\n        X = pre_processed_data.drop(marker, axis=1)\n        # scale data\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n\n        # model data\n        #model = LocalOutlierFactor(n_neighbors=20)\n        #model.fit(X_scaled)\n        #outlier_scores = model.negative_outlier_factor_\n        #outliers = pre_processed_data[outlier_scores &lt; -1].index\n\n        # Initialize LocalOutlierFactor with parallel processing\n        model = LocalOutlierFactor(n_neighbors=20, n_jobs=-1)\n\n        # Define batch size and prepare for batch processing\n        batch_size = 50000  # Adjust this based on your system's memory capacity\n        n_batches = int(np.ceil(X_scaled.shape[0] / batch_size))\n\n        outlier_scores = np.array([])\n\n        # Process in batches\n        for i in range(n_batches):\n            start_index = i * batch_size\n            end_index = start_index + batch_size\n            batch = X_scaled[start_index:end_index]\n\n            # Fit the model on the batch\n            model.fit(batch)\n\n            # Append the batch's outlier scores\n            batch_scores = model.negative_outlier_factor_\n            outlier_scores = np.concatenate((outlier_scores, batch_scores))\n\n        # Identifying outliers\n        threshold = -1 \n        outliers = pre_processed_data[outlier_scores &lt; threshold].index\n\n\n        # common elements betwenn outliers and true neg\n        posttoneg = list(set(outliers).intersection(set(neg)))\n        # similarly is there any cells in negative that needs to be relocated to positive?\n        negtopos = list(set(pos).intersection(set(prediction_results[prediction_results[marker]=='neg'].index)))\n\n        # mutate the prediction results\n        prediction_results.loc[negtopos, marker] = 'pos'\n        prediction_results.loc[posttoneg, marker] = 'neg'\n\n        # results\n        results = prediction_results[[marker]]\n        return results\n\n    # Run the function\n    if verbose is True:\n        print(\"Running Anomaly Detection\")\n    r_anomalyDetector = lambda x: anomalyDetector (bonafide_cells_result = x,\n                                                   pre_processed_data = pre_processed_data,\n                                                   prediction_results = prediction_results)\n\n    # as the Anomaly Detection uses the rest of the data it cannot be run on 1 marker\n    if len(bonafide_cells_result) &gt; 1:\n        anomalyDetector_result = list(map(r_anomalyDetector, bonafide_cells_result))\n        # final prediction\n        prediction_results = pd.concat(anomalyDetector_result, axis=1)\n\n\n    ###########################################################################\n    # step-9 : Reorganizing all predictions into a final dataframe\n    ###########################################################################\n\n    # re introduce failed markers\n    if len(failedMarkersinData) &gt; 0 :\n        for name in failedMarkersinData:\n            prediction_results[name] = 'neg'\n\n        # modify the highest value element to be pos\n        #modify_prediction_results(rawprocessed, prediction_results, failedMarkersinData)\n\n        # identify midpoints for the failed markers (second largest element)\n        max_values_dict = get_second_highest_values (rawprocessed, failedMarkersinData)\n\n        # update midpoints_dict\n        midpoints_dict.update(max_values_dict)\n\n        # add the column to pre_processed data for rescaling\n        columns_to_concat = rawprocessed[failedMarkersinData]\n        pre_processed_data = pd.concat([pre_processed_data, columns_to_concat], axis=1)\n\n\n    ###########################################################################\n    # step-10 : Rescale data\n    ###########################################################################\n\n    # marker = 'CD31'\n    def rescaleData (marker, pre_processed_data, prediction_results, midpoints_dict):\n        if verbose is True:\n            print(\"Processing: \" + str(marker))\n        # unravel data\n        data = pre_processed_data[marker].values\n        labels = prediction_results[marker].values\n        midpoint = midpoints_dict.get(marker)\n\n        # reformat data such that all negs and pos are sorted based on the midpoint\n        rescaled = modify_negatives_vectorized(data,\n                                               labels,\n                                               midpoint)\n\n        # sigmoid scaling to convert the data between 0-1 based on the midpoint\n        if rescaleMethod == 'sigmoid':\n            rescaled_data = sigmoid (rescaled, midpoint=midpoint)\n\n        if rescaleMethod == 'minmax':\n            rescaled_data = scale_data(rescaled, midpoint=midpoint)\n\n        # return\n        return rescaled_data\n\n    # Run the function\n    if verbose is True:\n        print(\"Rescaling the raw data\")\n    r_rescaleData = lambda x: rescaleData (marker=x,\n                                           pre_processed_data=pre_processed_data,\n                                           prediction_results=prediction_results,\n                                           midpoints_dict=midpoints_dict)\n    rescaleData_result = list(map(r_rescaleData, pre_processed_data.columns))\n    rescaledData = pd.DataFrame(rescaleData_result, index=pre_processed_data.columns, columns=pre_processed_data.index).T\n\n\n\n    ###########################################################################\n    # step-8 : create a new adata object with the results\n    ###########################################################################\n\n\n    final_markers = pre_processed_data.columns\n    intial_markers = rawData.columns\n    ordered_final_markers = [marker for marker in intial_markers if marker in final_markers]\n    # final raw data\n    rd = rawData[ordered_final_markers].reindex(adata.obs.index)\n    # final scaled data\n    rescaledData = rescaledData[ordered_final_markers].reindex(adata.obs.index)\n    # final pre-processed data\n    pre_processed_data = pre_processed_data[ordered_final_markers].reindex(adata.obs.index)\n\n\n    # reindex prediction results\n    prediction_results = prediction_results.reindex(adata.obs.index)\n    #probability_results = probability_results.reindex(adata.obs.index)\n\n    # create AnnData object\n    bdata = ad.AnnData(rd, dtype=np.float64)\n    bdata.obs = adata.obs\n    bdata.raw = bdata\n    bdata.X = rescaledData\n    # add the pre-processed data as a layer\n    bdata.layers[\"preProcessed\"] = pre_processed_data\n    bdata.uns = adata.uns\n    bdata.uns['failedMarkers'] = failed_markers_dict\n    bdata.uns['predictedGates'] = midpoints_dict\n\n    # save the prediction results in anndata object\n    bdata.uns[str(label)] = prediction_results\n    #bdata.uns[str(label)] = probability_results\n\n    # Save data if requested\n    if projectDir is not None:\n        finalPath = pathlib.Path(projectDir + '/CSPOT/cspotOutput')\n        if not os.path.exists(finalPath):\n            os.makedirs(finalPath)\n        if len(csObjectPath) &gt; 1:\n            imid = 'cspotOutput'\n        else:\n            imid = csObjectPath[0].stem\n        bdata.write(finalPath / f'{imid}.h5ad')\n\n    # Finish Job\n    if verbose is True:\n        if projectDir is not None:\n            print('CSPOT ran successfully, head over to \"' + str(projectDir) + '/CSPOT/cspotOutput\" to view results')\n\n    return bdata\n</code></pre>"},{"location":"Functions/generateCSScore/","title":"generateCSScore","text":"<p>Short Description</p> <p>The <code>generateCSScore</code> function calculates <code>CSPOT Score</code> for each cell by using  both the generated probability masks and pre-computed segmentation masks as inputs</p>"},{"location":"Functions/generateCSScore/#cspot.generateCSScore--function","title":"Function","text":""},{"location":"Functions/generateCSScore/#cspot.generateCSScore.generateCSScore","title":"<code>generateCSScore(probabilityMaskPath, segmentationMaskPath, feature='median', verbose=True, markerNames=None, projectDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>probabilityMaskPath</code> <code>str</code> <p>Supply the path of the probability map image produced by <code>dlModelPredict</code>.</p> required <code>segmentationMaskPath</code> <code>str</code> <p>Supply the path of the pre-computed segmentation mask.</p> required <code>feature</code> <code>str</code> <p>Calculates the <code>mean</code> or <code>median</code> CSPOT Score for each cell.</p> <code>'median'</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>markerNames</code> <code>list</code> <p>The program searches for marker names in the meta data (description section) of the tiff files created by <code>csPredict</code> by default. If the meta data is lost due to user modifications, provide the marker names for each channel/layer in the <code>probabilityMaskPath</code> here.</p> <code>None</code> <code>projectDir</code> <code>str</code> <p>Provide the path to the output directory. The result will be located at <code>projectDir/CSPOT/csScore/</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CSV</code> <code>dataframe</code> <p>The <code>.csv</code> file containing the <code>csScore</code> is stored in the provided projectDir.</p> Example <pre><code># global path\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# Path to all the files that are necessary files for running generateCSScore\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\nprobabilityMaskPath = projectDir + '/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif'\n\ncs.generateCSScore(probabilityMaskPath=probabilityMaskPath,\n              segmentationMaskPath=segmentationPath,\n              feature='median',\n              projectDir=projectDir)\n\n# Same function if the user wants to run it via Command Line Interface\npython generateCSScore.py             --probabilityMaskPath /Users/aj/Documents/cspotExampleData/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif             --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/generateCSScore.py</code> <pre><code>def generateCSScore (probabilityMaskPath,\n                         segmentationMaskPath,\n                         feature='median',\n                         verbose=True,\n                         markerNames=None,\n                         projectDir=None):\n\n    \"\"\"\nParameters:\n    probabilityMaskPath (str):\n        Supply the path of the probability map image produced by `dlModelPredict`.\n\n    segmentationMaskPath (str):\n        Supply the path of the pre-computed segmentation mask.\n\n    feature (str, optional):\n        Calculates the `mean` or `median` CSPOT Score for each cell.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    markerNames (list, optional):\n        The program searches for marker names in the meta data (description section)\n        of the tiff files created by `csPredict` by default. If the meta data\n        is lost due to user modifications, provide the marker names for each\n        channel/layer in the `probabilityMaskPath` here.\n\n    projectDir (str, optional):\n        Provide the path to the output directory. The result will be located at\n        `projectDir/CSPOT/csScore/`.\n\nReturns:\n    CSV (dataframe):\n        The `.csv` file containing the `csScore` is stored in the provided projectDir.\n\nExample:\n        ```python\n\n        # global path\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # Path to all the files that are necessary files for running generateCSScore\n        segmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n        probabilityMaskPath = projectDir + '/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif'\n\n        cs.generateCSScore(probabilityMaskPath=probabilityMaskPath,\n                      segmentationMaskPath=segmentationPath,\n                      feature='median',\n                      projectDir=projectDir)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python generateCSScore.py \\\n            --probabilityMaskPath /Users/aj/Documents/cspotExampleData/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif \\\n            --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n        ```\n\n    \"\"\"\n\n    # ERROS before parsing the entire image\n\n    # Ensure feature is valid\n    if feature not in ['median', 'mean']:\n        raise ValueError(\"Error: Invalid feature selection. Please choose feature='median' or 'mean'.\")\n\n    # check for marker names\n    # read the channel names from the tiffile\n    tiff = tifffile.TiffFile(probabilityMaskPath)\n    try:\n        root = ET.fromstring(tiff.pages[0].description)\n        # parse the ome XML\n        namespace = None\n        for elem in root.iter():\n            if \"Channel\" in elem.tag:\n                namespace = {\"ome\": elem.tag.split(\"}\")[0][1:]}\n                break\n        channel_names = [channel.get(\"Name\") for channel in root.findall(\".//ome:Channel\", namespace)]\n        #omexml_string = ast.literal_eval(tiff.pages[0].description)\n        #channel_names = omexml_string['Channel']['Name']\n    except:\n        pass\n\n    # check if channel_names has been defined\n    if markerNames is not None:\n        channel_names = markerNames\n    else:\n        #quantTable.columns = channel_names\n        channel_names = list(channel_names)\n        # Check if channel_names is empty or contains NaN/None values\n        if not channel_names or any(x is None or (isinstance(x, float) and np.isnan(x)) for x in channel_names):\n            raise ValueError(\n                \"Error: Unable to identify marker names from the image. \"\n                \"Please manually pass markerNames.\")\n\n\n    # NOW PARSE ENTIRE IMAGE\n\n    # read the seg mask\n    segM = tifffile.imread(pathlib.Path(segmentationMaskPath))\n    probM = tifffile.imread(pathlib.Path(probabilityMaskPath))\n\n    #probs = []\n    #for i in range(len(probM)):\n    #    pospix = len(probM[i][(probM[i] / 255) &gt; 0.5]) / (probM[i].shape[0] * probM[i].shape[1])\n    #    probs.append(pospix)\n\n    if len(probM.shape) &gt; 2:\n        probM = np.moveaxis(probM, 0, -1)\n\n    def median_intensity(mask, img):\n        return np.median(img[mask])\n\n\n    # quantify\n    if verbose is True:\n        print(\"Quantifying the probability masks\")\n    quantTable = pd.DataFrame(measure.regionprops_table(segM, intensity_image=probM,\n                                                        properties=['label','mean_intensity'],\n                                                        extra_properties=[median_intensity])).set_index('label')\n\n    # keep only median\n    if feature == 'median':\n        quantTable = quantTable.filter(regex='median')\n    if feature == 'mean':\n        quantTable = quantTable.filter(regex='mean')\n\n\n    # assign channel names\n    quantTable.columns = channel_names\n\n    # build a division vector\n    # this is to make sure the low probs are not amplified; chosing 154 as it is 0.6P\n    #div_val = []\n    #for i in quantTable.columns:\n    #    div_val.append(255 if quantTable[i].max() &lt; 154 else  quantTable[i].max())\n\n    # conver to prob\n    #quantTable = quantTable / div_val\n    quantTable = quantTable / 255\n\n    # identify markers that failed\n    #sns.distplot(quantTable['ECAD'])\n\n\n    # if projectDir is given\n    if projectDir is None:\n        projectDir = os.getcwd()\n\n    # final path to save results\n    finalPath = pathlib.Path(projectDir + '/CSPOT/csScore/')\n    if not os.path.exists(finalPath):\n        os.makedirs(finalPath)\n\n    # file name\n    file_name = pathlib.Path(probabilityMaskPath).stem + '.csv'\n    quantTable.to_csv(finalPath / file_name)\n\n    # Finish Job\n    if verbose is True:\n        print('csScore is ready, head over to' + str(projectDir) + '/CSPOT/csScore\" to view results')\n</code></pre>"},{"location":"Functions/generateThumbnails/","title":"generateThumbnails","text":"<p>Short Description</p> <p>The <code>generateThumbnails</code> function generates Thumbnails of positive and  negative cells for a specified marker. The Thumbnails will be used to train a deep learning model. Make sure to have  the raw image, computed single-cell spatial table, and markers.csv file  ready for input.</p>"},{"location":"Functions/generateThumbnails/#cspot.generateThumbnails--function","title":"Function","text":""},{"location":"Functions/generateThumbnails/#cspot.generateThumbnails.generateThumbnails","title":"<code>generateThumbnails(spatialTablePath, imagePath, markerChannelMapPath, markers, markerColumnName='marker', channelColumnName='channel', transformation=True, maxThumbnails=2000, random_state=0, localNorm=True, globalNorm=False, x_coordinate='X_centroid', y_coordinate='Y_centroid', percentiles=[2, 12, 88, 98], windowSize=64, restrictDensity=True, restrictDensityNumber=None, verbose=True, projectDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>spatialTablePath</code> <code>str</code> <p>Path to the single-cell spatial feature matrix.</p> required <code>imagePath</code> <code>str</code> <p>Path to the image file. Recognizes <code>.ome.tif</code> image file.</p> required <code>markerChannelMapPath</code> <code>str</code> <p>Path to a <code>markers.csv</code> file that maps the channel number with the marker information.  Create a .csv file with at least two columns named 'channel' and 'marker' that  map the channel numbers to their corresponding markers. The channel number  should use 1-based indexing.</p> required <code>markers</code> <code>list</code> <p>Markers for which <code>Thumbnails</code> need to be generated. The function looks for these listed names in the <code>single-cell spatial Table</code>.</p> required <code>markerColumnName</code> <code>str</code> <p>The name of the column in the <code>markers.csv</code> file that holds the marker information. </p> <code>'marker'</code> <code>channelColumnName</code> <code>str</code> <p>The name of the column in the <code>markers.csv</code> file that holds the channel information.  </p> <code>'channel'</code> <code>transformation</code> <code>bool</code> <p>Performs <code>arcsinh</code> transformation on the data. If the <code>single-cell spatial table</code> is already transformed (like log transformation), set this to <code>False</code>.</p> <code>True</code> <code>maxThumbnails</code> <code>int</code> <p>Maximum number of Thumbnails to generate. </p> <code>2000</code> <code>random_state</code> <code>int</code> <p>Seed used by the random number generator.</p> <code>0</code> <code>localNorm</code> <code>bool</code> <p>It creates a duplicate folder of the Thumbnails, with local normalization performed on the images. Local normalization is the process of dividing each pixel in a thumbnail by the maximum value across the entire thumbnail. This is helpful for visual supervised sorting of the Thumbnails.</p> <code>True</code> <code>globalNorm</code> <code>bool</code> <p>It creates a duplicate folder of the Thumbnails, with global normalization performed on the images. Global normalization is the process of dividing each pixel in a thumbnail by the maximum value of the given marker across the entire image.</p> <code>False</code> <code>x_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the X coordinates for each cell.</p> <code>'X_centroid'</code> <code>y_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the Y coordinates for each cell.</p> <code>'Y_centroid'</code> <code>percentiles</code> <code>list</code> <p>Specify the interval of percentile levels of the expression utilized to intialize the GMM. The cells falling within these percentiles are utilized to distinguish between negative cells (first two values) and positive cells (last two values).</p> <code>[2, 12, 88, 98]</code> <code>windowSize</code> <code>int</code> <p>Size of the Thumbnails.</p> <code>64</code> <code>restrictDensity</code> <code>bool</code> <p>This parameter is utilized to regulate the number of positive cells  observed in a given field of view. In the case of markers that do not  exhibit a distinct spatial pattern, such as immune cells, it is  recommended to train the model using sparse cells in the field of view.</p> <code>True</code> <code>restrictDensityNumber</code> <code>int</code> <p>This parameter is employed in conjunction with <code>restrictDensity</code>.  By default, the program attempts to automatically identify less dense  regions when restrictDensity is set to <code>True</code> using a GMM approach.  However, <code>restrictDensityNumber</code> can be utilized to exert greater  control over the process, allowing the user to limit the number of  positive cells they wish to observe within the field of view.  This parameter requires integers.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console.  </p> <code>True</code> <code>projectDir</code> <code>string</code> <p>Path to output directory. The result will be located at <code>projectDir/CSPOT/Thumbnails/</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Thumbnails</code> <code>image</code> <p>Saves Thumbnails of auto identified postive and negative cells the designated output directory.</p> Example <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Documents/cspotExampleData'\nimagePath = projectDir + '/image/exampleImage.tif'\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\nmarkerChannelMapPath = projectDir + '/markers.csv'\n\n# Run the function\ncs.generateThumbnails ( spatialTablePath=spatialTablePath, \n                imagePath=imagePath, \n                markerChannelMapPath=markerChannelMapPath,\n                markers=[\"ECAD\", \"CD3D\"], \n                markerColumnName='marker',\n                channelColumnName='channel',\n                transformation=True, \n                maxThumbnails=100, \n                random_state=0,\n                localNorm=True, \n                globalNorm=False,\n                x_coordinate='X_centroid', \n                y_coordinate='Y_centroid',\n                percentiles=[2, 12, 88, 98], \n                windowSize=64,\n                restrictDensity=True,\n                restrictDensityNumber=None,\n                verbose=True,\n                projectDir=cwd)\n\n# Same function if the user wants to run it via Command Line Interface\npython generateThumbnails.py             --spatialTablePath /Users/aj/Desktop/cspotExampleData/quantification/exampleSpatialTable.csv             --imagePath /Users/aj/Desktop/cspotExampleData/image/exampleImage.tif             --markerChannelMapPath /Users/aj/Desktop/cspotExampleData/markers.csv             --markers ECAD CD3D             --maxThumbnails 100             --projectDir /Users/aj/Desktop/cspotExampleData/\n</code></pre> Source code in <code>cspot/generateThumbnails.py</code> <pre><code>def generateThumbnails (spatialTablePath, \n                        imagePath, \n                        markerChannelMapPath,\n                        markers, \n                        markerColumnName='marker',\n                        channelColumnName='channel',\n                        transformation=True, \n                        maxThumbnails=2000, \n                        random_state=0,\n                        localNorm=True, \n                        globalNorm=False,\n                        x_coordinate='X_centroid', \n                        y_coordinate='Y_centroid',\n                        percentiles=[2, 12, 88, 98], \n                        windowSize=64,\n                        restrictDensity=True,\n                        restrictDensityNumber=None,\n                        verbose=True,\n                        projectDir=None):\n    \"\"\"\nParameters:\n    spatialTablePath (str):\n        Path to the single-cell spatial feature matrix.\n\n    imagePath (str):\n        Path to the image file. Recognizes `.ome.tif` image file.\n\n    markerChannelMapPath (str):\n        Path to a `markers.csv` file that maps the channel number with the marker information. \n        Create a .csv file with at least two columns named 'channel' and 'marker' that \n        map the channel numbers to their corresponding markers. The channel number \n        should use 1-based indexing.\n\n    markers (list):\n        Markers for which `Thumbnails` need to be generated. The function looks for\n        these listed names in the `single-cell spatial Table`.\n\n    markerColumnName (str):\n        The name of the column in the `markers.csv` file that holds the marker information. \n\n    channelColumnName (str):\n        The name of the column in the `markers.csv` file that holds the channel information.  \n\n    transformation (bool, optional):\n        Performs `arcsinh` transformation on the data. If the `single-cell spatial table`\n        is already transformed (like log transformation), set this to `False`.\n\n    maxThumbnails (int, optional):\n        Maximum number of Thumbnails to generate. \n\n    random_state (int, optional):\n        Seed used by the random number generator.\n\n    localNorm (bool, optional):\n        It creates a duplicate folder of the Thumbnails, with local normalization\n        performed on the images. Local normalization is the process of dividing\n        each pixel in a thumbnail by the maximum value across the entire thumbnail.\n        This is helpful for visual supervised sorting of the Thumbnails.\n\n    globalNorm (bool, optional):\n        It creates a duplicate folder of the Thumbnails, with global normalization\n        performed on the images. Global normalization is the process of dividing\n        each pixel in a thumbnail by the maximum value of the given marker across\n        the entire image.\n\n    x_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        X coordinates for each cell.\n\n    y_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        Y coordinates for each cell.\n\n    percentiles (list, optional):\n        Specify the interval of percentile levels of the expression utilized to intialize\n        the GMM. The cells falling within these percentiles are utilized to distinguish\n        between negative cells (first two values) and positive cells (last two values).\n\n    windowSize (int, optional):\n        Size of the Thumbnails.\n\n    restrictDensity (bool, optional):\n        This parameter is utilized to regulate the number of positive cells \n        observed in a given field of view. In the case of markers that do not \n        exhibit a distinct spatial pattern, such as immune cells, it is \n        recommended to train the model using sparse cells in the field of view.\n\n    restrictDensityNumber (int, optional):\n        This parameter is employed in conjunction with `restrictDensity`. \n        By default, the program attempts to automatically identify less dense \n        regions when restrictDensity is set to `True` using a GMM approach. \n        However, `restrictDensityNumber` can be utilized to exert greater \n        control over the process, allowing the user to limit the number of \n        positive cells they wish to observe within the field of view. \n        This parameter requires integers.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console.  \n\n    projectDir (string, optional):\n        Path to output directory. The result will be located at\n        `projectDir/CSPOT/Thumbnails/`.\n\nReturns:\n    Thumbnails (image):\n        Saves Thumbnails of auto identified postive and negative cells the\n        designated output directory.\n\nExample:\n        ```python\n\n        # set the working directory &amp; set paths to the example data\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n        imagePath = projectDir + '/image/exampleImage.tif'\n        spatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\n        markerChannelMapPath = projectDir + '/markers.csv'\n\n        # Run the function\n        cs.generateThumbnails ( spatialTablePath=spatialTablePath, \n                        imagePath=imagePath, \n                        markerChannelMapPath=markerChannelMapPath,\n                        markers=[\"ECAD\", \"CD3D\"], \n                        markerColumnName='marker',\n                        channelColumnName='channel',\n                        transformation=True, \n                        maxThumbnails=100, \n                        random_state=0,\n                        localNorm=True, \n                        globalNorm=False,\n                        x_coordinate='X_centroid', \n                        y_coordinate='Y_centroid',\n                        percentiles=[2, 12, 88, 98], \n                        windowSize=64,\n                        restrictDensity=True,\n                        restrictDensityNumber=None,\n                        verbose=True,\n                        projectDir=cwd)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python generateThumbnails.py \\\n            --spatialTablePath /Users/aj/Desktop/cspotExampleData/quantification/exampleSpatialTable.csv \\\n            --imagePath /Users/aj/Desktop/cspotExampleData/image/exampleImage.tif \\\n            --markerChannelMapPath /Users/aj/Desktop/cspotExampleData/markers.csv \\\n            --markers ECAD CD3D \\\n            --maxThumbnails 100 \\\n            --projectDir /Users/aj/Desktop/cspotExampleData/\n\n        ```\n    \"\"\"\n\n    # read the markers.csv\n    maper = pd.read_csv(pathlib.Path(markerChannelMapPath))\n    columnnames =  [word.lower() for word in maper.columns]\n    maper.columns = columnnames\n\n    # identify the marker column name (doing this to make it easier for people who confuse between marker and markers)\n    if markerColumnName not in columnnames:\n        if markerColumnName != 'marker':\n            raise ValueError('markerColumnName not found in markerChannelMap, please check')\n        if 'markers' in columnnames:\n            markerCol = 'markers'\n        else:\n            raise ValueError('markerColumnName not found in markerChannelMap, please check')\n    else: \n        markerCol = markerColumnName\n\n    # identify the channel column name (doing this to make it easier for people who confuse between channel and channels)\n    if channelColumnName not in columnnames:\n        if channelColumnName != 'channel':\n            raise ValueError('channelColumnName not found in markerChannelMap, please check')\n        if 'channels' in columnnames:\n            channelCol = 'channels'\n        else:\n            raise ValueError('channelColumnName not found in markerChannelMap, please check')\n    else: \n        channelCol = channelColumnName\n\n    # map the marker and channels\n    chmamap = dict(zip(maper[markerCol], maper[channelCol]))\n\n    # load the CSV to identify potential thumbnails\n    data = pd.read_csv(pathlib.Path(spatialTablePath))\n    #data.index = data.index.astype(str)\n\n    # subset the markers of interest\n    if isinstance (markers, str):\n        markers = [markers]\n\n    # find the corresponding channel names\n    markerChannels = [chmamap[key] for key in markers if key in chmamap]\n    # convert markerChannels to zero indexing\n    markerChannels = [x-1 for x in markerChannels]\n\n    # creat a dict of marker and corresponding marker channel\n    marker_map = dict(zip(markers,markerChannels))\n\n    # create folders if it does not exist\n    if projectDir is None:\n        projectDir = os.getcwd()\n\n    # TruePos folders\n    for i in markers:\n        pos_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(i) + '/TruePos')\n        neg_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(i) + '/TrueNeg')\n        pos2neg_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(i) + '/PosToNeg')\n        neg2pos_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(i) + '/NegToPos')\n        if not os.path.exists(pos_path):\n            os.makedirs(pos_path)\n        if not os.path.exists(neg_path):\n            os.makedirs(neg_path)\n        if not os.path.exists(pos2neg_path):\n            os.makedirs(pos2neg_path)\n        if not os.path.exists(neg2pos_path):\n            os.makedirs(neg2pos_path)\n        if localNorm is True:\n            local_pos_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(i) + '/TruePos')\n            local_neg_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(i) + '/TrueNeg')\n            local_pos2neg_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(i) + '/PosToNeg')\n            local_neg2pos_path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(i) + '/NegToPos')\n            if not os.path.exists(local_pos_path):\n                os.makedirs(local_pos_path)\n            if not os.path.exists(local_neg_path):\n                os.makedirs(local_neg_path)\n            if not os.path.exists(local_pos2neg_path):\n                os.makedirs(local_pos2neg_path)\n            if not os.path.exists(local_neg2pos_path):\n                os.makedirs(local_neg2pos_path)\n\n    marker_data = data[markers]\n    location = data[[x_coordinate,y_coordinate]]\n\n    # clip the data to drop outliers\n    def clipping (x):\n        clip = x.clip(lower =np.percentile(x,0.01), upper=np.percentile(x,99.99)).tolist()\n        return clip\n\n    # return the mean between two percentiles\n    def meanPercentile (values, lowPercentile, highPercentile):\n        # Calculate the 1st percentile value\n        p1 = np.percentile(values, lowPercentile)\n        # Calculate the 20th percentile value\n        p20 = np.percentile(values, highPercentile)\n        # Select the values between the 1st and 20th percentile using numpy.where()\n        filtered_values = np.where((values &gt;= p1) &amp; (values &lt;= p20))\n        # Calculate the mean of the filtered values\n        meanVal = np.mean(values[filtered_values])\n        return meanVal\n\n    # Function for GMM\n    def simpleGMM (data, n_components, means_init, random_state):\n        gmm = GaussianMixture(n_components=n_components, means_init=means_init,  random_state=random_state)\n        gmm.fit(data)\n        # Predict the class labels for each sample\n        predictions = gmm.predict(data)\n        # Get the mean of each Gaussian component\n        means = gmm.means_.flatten()\n        # Sort the mean values in ascending order\n        sorted_means = np.sort(means)\n        # Assign 'pos' to rows with higher mean distribution and 'neg' to rows with lower mean distribution\n        labels = np.where(predictions == np.argmax(means), 'pos', 'neg')\n        return labels\n\n    # match two arrays and return seperate lists\n    def array_match (labels, names):\n        # Create a defaultdict with an empty list as the default value\n        result = defaultdict(list)\n        # Iterate over the labels and names arrays\n        for label, name in zip(labels, names):\n            # Add the name to the list for the corresponding label\n            result[label].append(name)\n        return result\n\n\n\n    # clip data\n    marker_data = marker_data.apply(clipping)\n\n    # apply transformation if requested\n    if transformation is True:\n        marker_data = np.arcsinh(marker_data)\n        #marker_data = np.log1p(marker_data)\n\n    # combine data\n    combined_data = pd.concat([marker_data, location], axis=1)\n\n    # intialize the percentiles values\n    percentiles.sort()\n\n    # function to identify the corner of the thumbnails\n    def cornerFinder (centroid):\n        row_start = int(centroid - windowSize // 2)\n        row_end = row_start + windowSize\n        return [row_start, row_end]\n\n    # function to crop the image and save the image\n    def cropImage (rowIndex, corners, imgType, zimg, npercentile, m, maxpercentile, imname):\n        #print(str(rowIndex))\n        x_start = corners.loc[rowIndex]['x_start']; x_end = corners.loc[rowIndex]['x_end']\n        y_start = corners.loc[rowIndex]['y_start']; y_end = corners.loc[rowIndex]['y_end']\n        # cropping image\n        crop = zimg[y_start:y_end, x_start:x_end]\n        # check if image is the right size\n        if crop.shape == (windowSize,windowSize):\n            # convert the image to unit8\n            if globalNorm is True:\n                fullN = ((crop/npercentile)*255).clip(0, 255).astype('uint8')\n            else:\n                fullN = ((crop/maxpercentile)*255).clip(0, 255).astype('uint8')\n\n            # construct image filename with marker name prefix\n            prefixed_imname = f\"{m}_{imname}_{rowIndex}.tif\"\n            # save the cropped image\n            if imgType == 'pos':\n                path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(m) + '/TruePos/' + prefixed_imname)\n            elif imgType == 'neg':\n                path = pathlib.Path(projectDir + '/CSPOT/Thumbnails/' + str(m) + '/TrueNeg/' + prefixed_imname)                      \n            # write file\n            tifffile.imwrite(path,fullN)\n            # local normalization if requested\n            if localNorm is True:\n                localN = ((crop/(np.percentile(crop, 99.99)))*255).clip(0, 255).astype('uint8') #.compute()\n                # save image\n                if imgType == 'pos':\n                    Lpath = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(m) + '/TruePos/' + prefixed_imname)\n                elif imgType == 'neg':\n                    Lpath = pathlib.Path(projectDir + '/CSPOT/Thumbnails/localNorm/' + str(m) + '/TrueNeg/' + prefixed_imname)     \n                # write file\n                tifffile.imwrite(Lpath,localN)\n\n    # identify the cells of interest\n    def processMarker (marker):\n        if verbose is True:\n            print('Processing Marker: ' + str(marker))\n\n        moi = combined_data[marker].values\n\n        # figure out marker index or channel in image\n        markerIndex = marker_map[marker]\n\n        # mean of cells within defined threshold\n        lowerPercent = meanPercentile (values=moi, lowPercentile=percentiles[0], highPercentile=percentiles[1])\n        higherPercent = meanPercentile (values=moi, lowPercentile=percentiles[2], highPercentile=percentiles[3])\n        # Format mean to pass into next GMM\n        Pmean = np.array([[lowerPercent], [higherPercent]])\n\n        # perform GMM\n        labels = simpleGMM (data=moi.reshape(-1, 1), n_components=2, means_init=Pmean,  random_state=random_state)\n        # Match the labels and index names to identify which cells are pos and neg\n        expCells = array_match (labels=labels, names=data.index)\n        # split it\n        pos = expCells.get('pos', []) ; neg = expCells.get('neg', [])\n\n\n        # determine the percentiles value for the marker of interest\n        #low_a = np.percentile(moi, percentiles[0]); low_b = np.percentile(moi, percentiles[1])\n        #high_a = np.percentile(moi, percentiles[2]); high_b = np.percentile(moi, percentiles[3])\n\n        # identify the cells that fall within the determined range\n        #neg = np.where(moi.between(low_a, low_b))[0]\n        #pos = np.where(moi.between(high_a, high_b))[0]\n\n        # shuffle the cells\n        random.Random(random_state).shuffle(neg); random.Random(random_state).shuffle(pos)\n\n        # identify the location of pos and neg cells\n        neg_location_i = location.iloc[neg]\n        pos_location_i = location.iloc[pos]\n\n        # divide the pos cells into two bins based on the number of neighbours\n        # assumption is that we will find cells that are dense and sparse\n        if restrictDensity is True:\n            # identify postive cells that are densly packed if user requests\n            kdt = BallTree(pos_location_i[[x_coordinate,y_coordinate]], metric='euclidean') \n            ind = kdt.query_radius(pos_location_i[[x_coordinate,y_coordinate]], r=windowSize+5, return_distance=False)\n            for i in range(0, len(ind)): ind[i] = np.delete(ind[i], np.argwhere(ind[i] == i))#remove self\n            neigh_length = [len(subarray) for subarray in ind]\n            if restrictDensityNumber is None:\n                # GMM for auto detection\n                X = np.array(neigh_length).reshape(-1, 1)\n                gmm_neigh = GaussianMixture(n_components=2)\n                gmm_neigh.fit(X)\n                means = gmm_neigh.means_\n                index = np.argmin(means)\n                labels_neigh = gmm_neigh.predict(X)\n                lower_mean_indices = np.where(labels_neigh == index)[0]\n                # subset the postive cells based on the index\n                pos_location_i = pos_location_i.iloc[lower_mean_indices]\n            else:\n                lower_mean_indices = [i for i, x in enumerate(neigh_length) if x &lt; restrictDensityNumber]\n                pos_location_i = pos_location_i.iloc[lower_mean_indices]\n\n        # Find corner\n        # Negative cells\n        r_cornerFinder = lambda x: cornerFinder (centroid=x)\n        neg_x = pd.DataFrame(list(map(r_cornerFinder, neg_location_i[x_coordinate].values))) # x direction\n        neg_y = pd.DataFrame(list(map(r_cornerFinder, neg_location_i[y_coordinate].values))) # y direction\n        neg_x.columns = [\"x_start\", \"x_end\"]; neg_y.columns = [\"y_start\", \"y_end\"]\n        neg_location = pd.concat([neg_x, neg_y], axis=1)\n        neg_location.index = neg_location_i.index\n\n        # Positive cells\n        r_cornerFinder = lambda x: cornerFinder (centroid=x)\n        pos_x = pd.DataFrame(list(map(r_cornerFinder, pos_location_i[x_coordinate].values))) # x direction\n        pos_y = pd.DataFrame(list(map(r_cornerFinder, pos_location_i[y_coordinate].values))) # y direction\n        pos_x.columns = [\"x_start\", \"x_end\"]; pos_y.columns = [\"y_start\", \"y_end\"]\n        pos_location = pd.concat([pos_x, pos_y], axis=1)\n        pos_location.index = pos_location_i.index\n\n        # drop all coordinates with neg values (essentially edges of slide)\n        neg_location = neg_location[(neg_location &gt; 0).all(1)]\n        pos_location = pos_location[(pos_location &gt; 0).all(1)]\n\n        # subset max number of cells\n        if len(neg_location) &gt; maxThumbnails:\n            neg_location = neg_location[:maxThumbnails]\n        if len(pos_location) &gt; maxThumbnails:\n            pos_location = pos_location[:maxThumbnails]\n\n        # identify image name\n        imname = pathlib.Path(imagePath).stem\n\n        # load the image\n        zimg = tifffile.imread(str(pathlib.Path(imagePath)), level=0, key=markerIndex)\n        npercentile = np.percentile(zimg, 99.99)\n        maxpercentile = zimg.max()\n\n        # load the image (dask has issues)\n        #zimg = da.from_zarr(tifffile.imread(pathlib.Path(imagePath), aszarr=True, level=0, key=markerIndex))\n        #npercentile = np.percentile(zimg.compute(), 99.99)\n        #maxpercentile = zimg.max().compute()\n\n\n        # for older version f tifffile\n        #zimg = zarr.open(tifffile.imread(pathlib.Path(imagePath), aszarr=True, level=0, key=markerIndex))\n        # = np.percentile(zimg,  99.99)\n\n        # Cut images and write it out\n        # neg\n        r_cropImage = lambda x: cropImage (rowIndex=x, corners=neg_location, imgType='neg', zimg=zimg, npercentile=npercentile, maxpercentile=maxpercentile, m=marker, imname=imname)\n        process_neg = list(map(r_cropImage, list(neg_location.index)))\n        # pos\n        r_cropImage = lambda x: cropImage (rowIndex=x, corners=pos_location, imgType='pos', zimg=zimg, npercentile=npercentile, maxpercentile=maxpercentile, m=marker, imname=imname)\n        process_neg = list(map(r_cropImage, list(pos_location.index)))\n\n\n    # Run the function for each marker\n    r_processMarker = lambda x: processMarker (marker=x)\n    final = list(map(r_processMarker, markers))\n\n    # Finish Job\n    if verbose is True:\n        print('Thumbnails have been generated, head over to \"' + str(projectDir) + '/CSPOT/Thumbnails\" to view results')\n</code></pre>"},{"location":"Functions/generateTrainTestSplit/","title":"generateTrainTestSplit","text":"<p>Short Description</p> <p>The function generates a mask for the deep learning model training, using  automated approaches. Splitting the data into training, validation and  test sets is also included in the function, making it easier to feed the  data directly into the deep learning algorithm. Note that manually drawing  the mask on thumbnails is the ideal approach, however for scalability  purposes, automation is used.</p>"},{"location":"Functions/generateTrainTestSplit/#cspot.generateTrainTestSplit--function","title":"Function","text":""},{"location":"Functions/generateTrainTestSplit/#cspot.generateTrainTestSplit.generateTrainTestSplit","title":"<code>generateTrainTestSplit(thumbnailFolder, projectDir, file_extension=None, verbose=True, TruePos='TruePos', NegToPos='NegToPos', TrueNeg='TrueNeg', PosToNeg='PosToNeg')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>thumbnailFolder</code> <code>list</code> <p>List of folders that contains the human sorted Thumbnails that is to be used for generating training data and split them train test and validation cohorts.</p> required <code>projectDir</code> <code>str</code> <p>Path to output directory.</p> required <code>file_extension</code> <code>str</code> <p>If there are non-image files in the thumbnailFolder, the user can specify a file extension to only select those files for processing. The default is None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console. </p> <code>True</code> <code>TruePos</code> <code>str</code> <p>Name of the folder that holds the Thumbnails classified as True Positive. The default is 'TruePos'.</p> <code>'TruePos'</code> <code>NegToPos</code> <code>str</code> <p>Name of the folder that holds the Thumbnails classified as True Negative. The default is 'NegToPos'.</p> <code>'NegToPos'</code> <code>TrueNeg</code> <code>str</code> <p>Name of the folder that holds the Thumbnails that were moved from <code>True Positive</code> to <code>True Negative</code>. The default is 'TrueNeg'.</p> <code>'TrueNeg'</code> <code>PosToNeg</code> <code>str</code> <p>Name of the folder that holds the Thumbnails that were moved from <code>True Negative</code> to <code>True Positive</code>. The default is 'PosToNeg'.</p> <code>'PosToNeg'</code> <p>Returns:</p> Name Type Description <code>masks</code> <code>images</code> <p>Segmentation masks are generated for every Thumbnail and split into Train, Test and Validation cohorts.</p> Example <pre><code># High level working directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# Folder where the raw Thumbnails are stored\nthumbnailFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                   projectDir + '/CSPOT/Thumbnails/ECAD']\n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n# If you had deleted any of the folders and are not using them, replace the folder name with `None` in the parameter.\ncs.generateTrainTestSplit ( thumbnailFolder, \n                            projectDir=projectDir,\n                            file_extension=None,\n                            TruePos='TruePos', NegToPos='NegToPos',\n                            TrueNeg='TrueNeg', PosToNeg='PosToNeg')\n\n# Same function if the user wants to run it via Command Line Interface\npython generateTrainTestSplit.py             --thumbnailFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD             --projectDir /Users/aj/Desktop/cspotExampleData/\n</code></pre> Source code in <code>cspot/generateTrainTestSplit.py</code> <pre><code>def generateTrainTestSplit (thumbnailFolder, \n                            projectDir, \n                            file_extension=None,\n                            verbose=True,\n                            TruePos='TruePos', NegToPos='NegToPos',\n                            TrueNeg='TrueNeg', PosToNeg='PosToNeg'):\n    \"\"\"\nParameters:\n    thumbnailFolder (list):\n        List of folders that contains the human sorted Thumbnails that is to be used\n        for generating training data and split them train test and validation cohorts.\n\n    projectDir (str):\n        Path to output directory.\n\n    file_extension (str, optional):\n        If there are non-image files in the thumbnailFolder, the user can specify\n        a file extension to only select those files for processing. The default is None.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console. \n\n    TruePos (str, optional):\n        Name of the folder that holds the Thumbnails classified as True Positive.\n        The default is 'TruePos'.\n\n    NegToPos (str, optional):\n        Name of the folder that holds the Thumbnails classified as True Negative.\n        The default is 'NegToPos'.\n\n    TrueNeg (str, optional):\n        Name of the folder that holds the Thumbnails that were moved from `True Positive`\n        to `True Negative`. The default is 'TrueNeg'.\n\n    PosToNeg (str, optional):\n        Name of the folder that holds the Thumbnails that were moved from `True Negative`\n        to `True Positive`. The default is 'PosToNeg'.\n\nReturns:\n    masks (images):\n        Segmentation masks are generated for every Thumbnail and split into Train,\n        Test and Validation cohorts.\n\nExample:\n        ```python\n\n        # High level working directory\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # Folder where the raw Thumbnails are stored\n        thumbnailFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                           projectDir + '/CSPOT/Thumbnails/ECAD']\n\n        # The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n        # If you had deleted any of the folders and are not using them, replace the folder name with `None` in the parameter.\n        cs.generateTrainTestSplit ( thumbnailFolder, \n                                    projectDir=projectDir,\n                                    file_extension=None,\n                                    TruePos='TruePos', NegToPos='NegToPos',\n                                    TrueNeg='TrueNeg', PosToNeg='PosToNeg')\n\n        # Same function if the user wants to run it via Command Line Interface\n        python generateTrainTestSplit.py \\\n            --thumbnailFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD \\\n            --projectDir /Users/aj/Desktop/cspotExampleData/\n\n        ```\n\n    \"\"\"\n\n    # Function takes in path to two folders, processes the images in those folders,\n    # and saves them into a different folder that contains Train, Validation and Test samples\n    #TruePos='TruePos'; NegToPos='NegToPos'; TrueNeg='TrueNeg'; PosToNeg='PosToNeg'; verbose=True\n\n    # convert the folder into a list\n    if isinstance (thumbnailFolder, str):\n        thumbnailFolder = [thumbnailFolder]\n\n    # convert all path names to pathlib\n    thumbnailFolder = [pathlib.Path(p) for p in thumbnailFolder]\n    projectDir = pathlib.Path(projectDir)\n\n    # find all markers passed\n    all_markers = [i.stem for i in thumbnailFolder]\n\n    # create directories to save\n    for i in all_markers:\n        if not (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'training').exists ():\n            (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'training').mkdir(parents=True, exist_ok=True)\n\n        if not (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'validation').exists ():\n            (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'validation').mkdir(parents=True, exist_ok=True)\n\n        if not (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'test').exists ():\n            (projectDir / 'CSPOT/TrainingData/' / f\"{i}\" /  'test').mkdir(parents=True, exist_ok=True)\n\n    # standard format\n    if file_extension is None:\n        file_extension = '*'\n    else:\n        file_extension = '*' + str(file_extension)\n\n    # Filter on pos cells\n    def pos_filter (path):\n        image = cv.imread(str(path.resolve()), cv.IMREAD_GRAYSCALE)\n        blur = cv.GaussianBlur(image, ksize=(3,3), sigmaX=1, sigmaY=1)\n        ret3,th3 = cv.threshold(blur,0,1,cv.THRESH_OTSU)\n        mask = th3 + 1\n        return [mask, image]\n\n    # Filter on neg cells\n    def neg_filter (path):\n        image = cv.imread(str(path.resolve()), cv.IMREAD_GRAYSCALE)\n        mask = np.ones(image.shape, dtype=np.uint8)\n        return [mask, image]\n\n    # identify the files within all the 4 folders\n    def findFiles (folderIndex):\n        if verbose is True:\n            print ('Processing: ' + str(thumbnailFolder[folderIndex].stem))\n        marker_name = str(thumbnailFolder[folderIndex].stem)\n\n        baseFolder = thumbnailFolder[folderIndex]\n\n        if TruePos is not None:\n            pos = list(pathlib.Path.glob(baseFolder / TruePos, file_extension))\n        if NegToPos is not None:\n            negtopos = list(pathlib.Path.glob(baseFolder / NegToPos, file_extension))\n        positive_cells = pos + negtopos\n\n        if TrueNeg is not None:\n            neg = list(pathlib.Path.glob(baseFolder / TrueNeg, file_extension))\n        if PosToNeg is not None:\n            postoneg = list(pathlib.Path.glob(baseFolder / PosToNeg, file_extension))\n        negative_cells = neg + postoneg\n\n        # prepare the Training, Validataion and Test Cohorts\n        if len(positive_cells) &gt; 0:\n            train_pos = random.sample(positive_cells, round(len(positive_cells) * 0.6))\n            remanining_pos = list(set(positive_cells) - set(train_pos))\n            val_pos = random.sample(remanining_pos, round(len(remanining_pos) * 0.5)) # validation\n            test_pos = list(set(remanining_pos) - set(val_pos)) # test\n        else:\n            train_pos = []; val_pos = []; test_pos = []\n        if len(negative_cells) &gt; 0:\n            train_neg = random.sample(negative_cells, round(len(negative_cells) * 0.6))\n            remanining_neg = list(set(negative_cells) - set(train_neg))\n            val_neg = random.sample(remanining_neg, round(len(remanining_neg) * 0.5))\n            test_neg = list(set(remanining_neg) - set(val_neg))\n        else:\n            train_neg = []; val_neg = []; test_neg = []\n\n\n        # loop through training dataset and save images and masks\n        newname_train = list(range(len(train_pos) + len(train_neg))); random.shuffle(newname_train)\n        train_pos_name = newname_train[:len(train_pos)]; train_neg_name = newname_train[len(train_pos):]\n\n        if len (train_pos_name) &gt; 0:\n            for i, j in zip( train_pos_name, train_pos):\n                m, im = pos_filter (j)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'training' / f\"{i}_img.tif\"\n                tifffile.imwrite(fPath,im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'training' / f\"{i}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n        if len (train_neg_name) &gt; 0:\n            for k, l in zip( train_neg_name, train_neg):\n                m, im = neg_filter (l)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'training' / f\"{k}_img.tif\"\n                tifffile.imwrite(fPath, im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'training' / f\"{k}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n\n        # loop through validation dataset and save images and masks\n        newname_train = list(range(len(val_pos) + len(val_neg))); random.shuffle(newname_train)\n        train_pos_name = newname_train[:len(val_pos)]; train_neg_name = newname_train[len(val_pos):]\n\n        if len (train_pos_name) &gt; 0:\n            for i, j in zip( train_pos_name, val_pos):\n                m, im = pos_filter (j)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'validation' / f\"{i}_img.tif\"\n                tifffile.imwrite(fPath, im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'validation' / f\"{i}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n        if len (train_neg_name) &gt; 0:\n            for k, l in zip( train_neg_name, val_neg):\n                m, im = neg_filter (l)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'validation' / f\"{k}_img.tif\"\n                tifffile.imwrite(fPath, im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'validation' / f\"{k}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n\n        # loop through test dataset and save images and masks\n        newname_train = list(range(len(test_pos) + len(test_neg))); random.shuffle(newname_train)\n        train_pos_name = newname_train[:len(test_pos)]; train_neg_name = newname_train[len(test_pos):]\n\n        if len (train_pos_name) &gt; 0:\n            for i, j in zip( train_pos_name, test_pos):\n                m, im = pos_filter (j)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'test' / f\"{i}_img.tif\"\n                tifffile.imwrite(fPath, im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'test' / f\"{i}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n        if len (train_neg_name) &gt; 0:\n            for k, l in zip( train_neg_name, test_neg):\n                m, im = neg_filter (l)\n                # save image\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'test' / f\"{k}_img.tif\"\n                tifffile.imwrite(fPath, im)\n                # associated mask\n                fPath = projectDir / 'CSPOT/TrainingData/' / f\"{marker_name}\" / 'test' / f\"{k}_mask.tif\"\n                tifffile.imwrite(fPath, m)\n\n    # apply function to all folders\n    r_findFiles = lambda x: findFiles (folderIndex=x)\n    process_folders = list(map(r_findFiles, list(range(len(thumbnailFolder)))))\n\n    # Print\n    if verbose is True:\n        print('Training data has been generated, head over to \"' + str(projectDir) + '/CSPOT/TrainingData\" to view results')\n</code></pre>"},{"location":"Functions/mergecsObject/","title":"mergecsObject","text":"<p>Short Description</p> <p>Use <code>mergecsObject</code> to combine multiple csObjects into a dataset for  analysis when multiple images need to be analyzed.</p> <p>Note that merging <code>csObjects</code> requires merging multiple sections, not  simple concatenation. Use parameters to specify which parts of the  <code>csObjects</code> to merge.</p>"},{"location":"Functions/mergecsObject/#cspot.mergecsObject--function","title":"Function","text":""},{"location":"Functions/mergecsObject/#cspot.mergecsObject.mergecsObject","title":"<code>mergecsObject(csObjects, fileName='mergedCSObject', layers=['preProcessed'], uns=['cspotOutput', 'csScore', 'failedMarkers'], verbose=True, projectDir=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObjects</code> <code>list</code> <p>A collection of CSPOT Objects to combine into one object, which can include both CSPOT Objects stored in memory and those accessed via file path.</p> required <code>fileName</code> <code>str</code> <p>Designate a Name for the resulting combined CSPOT object.</p> <code>'mergedCSObject'</code> <code>layers</code> <code>list</code> <p>The <code>.layers</code> section within the CSPOT Objects to be merged together.</p> <code>['preProcessed']</code> <code>uns</code> <code>list</code> <p>The <code>.uns</code> section within the CSPOT Objects to be merged together.</p> <code>['cspotOutput', 'csScore', 'failedMarkers']</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed information about the process to the console. </p> <code>True</code> <code>projectDir</code> <code>str</code> <p>Provide the path to the output directory. The result will be located at <code>projectDir/CSPOT/mergedCSObject/</code>. </p> <code>None</code> <p>Returns:</p> Name Type Description <code>csObject</code> <code>anndata</code> <p>If <code>projectDir</code> is provided the merged CSPOT Object will saved within the provided projectDir.</p> Example <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\ncsObjects = [projectDir + '/CSPOT/csOutput/exampleImage_cspotPredict.ome.h5ad',\n             projectDir + '/CSPOT/csOutput/exampleImage_cspotPredict.ome.h5ad']\n\n# For this tutorial, supply the same csObject twice for merging, but multiple csObjects can be merged in ideal conditions.\nadata = cs.mergecsObject ( csObjects=csObjects,\n                      fileName='mergedcspotObject',\n                      layers=['preProcessed'],\n                      uns= ['cspotOutput','csScore'],\n                      projectDir=projectDir)\n\n# Same function if the user wants to run it via Command Line Interface\npython mergecsObject.py             --csObjects /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad             --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre> Source code in <code>cspot/mergecsObject.py</code> <pre><code>def mergecsObject (csObjects,\n                      fileName='mergedCSObject',\n                      layers=['preProcessed'],\n                      uns= ['cspotOutput','csScore','failedMarkers'],\n                      verbose=True,\n                      projectDir=None):\n    \"\"\"\nParameters:\n    csObjects (list):\n       A collection of CSPOT Objects to combine into one object, which can\n       include both CSPOT Objects stored in memory and those accessed\n       via file path.\n\n    fileName (str, optional):\n        Designate a Name for the resulting combined CSPOT object.\n\n    layers (list, optional):\n        The `.layers` section within the CSPOT Objects to be merged together.\n\n    uns (list, optional):\n        The `.uns` section within the CSPOT Objects to be merged together.\n\n    verbose (bool, optional):\n        If True, print detailed information about the process to the console. \n\n    projectDir (str, optional):\n        Provide the path to the output directory. The result will be located at\n        `projectDir/CSPOT/mergedCSObject/`. \n\nReturns:\n    csObject (anndata):\n        If `projectDir` is provided the merged CSPOT Object will saved within the\n        provided projectDir.\n\nExample:\n        ```python.\n\n        # set the working directory &amp; set paths to the example data\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        csObjects = [projectDir + '/CSPOT/csOutput/exampleImage_cspotPredict.ome.h5ad',\n                     projectDir + '/CSPOT/csOutput/exampleImage_cspotPredict.ome.h5ad']\n\n        # For this tutorial, supply the same csObject twice for merging, but multiple csObjects can be merged in ideal conditions.\n        adata = cs.mergecsObject ( csObjects=csObjects,\n                              fileName='mergedcspotObject',\n                              layers=['preProcessed'],\n                              uns= ['cspotOutput','csScore'],\n                              projectDir=projectDir)\n\n        # Same function if the user wants to run it via Command Line Interface\n        python mergecsObject.py \\\n            --csObjects /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n\n\n        ```\n    \"\"\"\n\n    # Convert to list of anndata objects\n    if isinstance (csObjects, list):\n        csObjects = csObjects\n    else:\n        csObjects = [csObjects]\n\n    # converting other parameters to list\n    if isinstance (layers, str):\n        layers = [layers]\n    if isinstance (uns, str):\n        uns = [uns]\n\n    # Things to process\n    process_layers = ['rawData', 'scaledData', 'obs']\n    if layers is not None:\n        process_layers.extend(layers)\n    if uns is not None:\n        process_layers.extend(uns)\n\n\n    # for expression, uns and layers\n    def processX (csObject, process_layers):\n        if isinstance(csObject, str):\n            # start with raw data\n            adata = ad.read(csObject)\n        else:\n            adata = csObject.copy()\n\n        # print\n        if verbose is True:\n            print (\"Extracting data from: \" + str( adata.obs['imageid'].unique()[0]) )\n\n        # process the data\n        rawData = pd.DataFrame(adata.raw.X, index=adata.obs.index, columns=adata.var.index)\n\n        # scaled data\n        scaledData = pd.DataFrame(adata.X, index=adata.obs.index, columns=adata.var.index)\n\n        # obs\n        obs = adata.obs.copy()\n\n        # Process layers\n        if layers is not None:\n            for i in layers:\n                exec(f\"{i} = pd.DataFrame(adata.layers[i],index=adata.obs.index, columns=adata.var.index)\")\n\n        # Process uns\n        if uns is not None:\n            for j in uns:\n                exec(f\"{j} = adata.uns[j]\")\n\n        # return the results\n        objects = []\n        for name in process_layers:\n            exec(f\"objects.append({name})\")\n\n        return objects\n\n    # Run the function\n    # Run the function:\n    if verbose is True:\n        print(\"Extracting data\")\n    r_processX = lambda x: processX (csObject=x, process_layers=process_layers)\n    processX_result = list(map(r_processX, csObjects)) # Apply function\n\n    # combine all the data\n    # create a dictinoary between index and data type\n    mapping = {i: element for i, element in enumerate(process_layers)}\n\n\n    # create an empty dictionary to store the final dataframes\n    final_data = {}\n    # get the number of lists in the data list\n    num_lists = len(processX_result)\n    # iterate over the mapping dictionary\n    for key, value in mapping.items():\n        # create an empty list to store the dataframes\n        df_list = []\n        # iterate over the data lists\n        for i in range(num_lists):\n            # retrieve the dataframe from the current list\n            df = processX_result[i][key]\n            # resolve dict independently\n            if isinstance(df, dict):\n                df = pd.DataFrame.from_dict(df, orient='index', columns=df[list(df.keys())[0]]).applymap(lambda x: 1)   \n            # add the dataframe to the df_list\n            df_list.append(df)\n        # concatenate the dataframes in the df_list\n        df = pd.concat(df_list)\n        # add the resulting dataframe to the final_data dictionary\n        final_data[value] = df\n\n\n    # create the combined anndata object\n    bdata = ad.AnnData(final_data.get(\"rawData\"), dtype=np.float64)\n    bdata.obs = final_data.get(\"obs\")\n    bdata.raw = bdata\n    bdata.X = final_data.get(\"scaledData\")\n    # add layers\n    if layers is not None:\n        for i in layers:\n            tmp = final_data.get(i)\n            bdata.layers[i] = tmp\n    # add uns\n    if uns is not None:\n        for i in uns:\n            tmp = final_data.get(i)\n            bdata.uns[i] = tmp\n\n    # last resolve all_markers if it exisits\n    if isinstance(csObjects[0], str):\n        # start with raw data\n        adata = ad.read(csObjects[0])\n    else:\n        adata = csObjects[0].copy()\n    if hasattr(adata, 'uns') and 'all_markers' in adata.uns:\n        # the call to adata.uns['all_markers'] is valid\n        bdata.uns['all_markers'] = adata.uns['all_markers']\n\n    # write the output\n    if projectDir is not None:\n        finalPath = pathlib.Path(projectDir + '/CSPOT/mergedcsObject')\n        if not os.path.exists(finalPath):\n            os.makedirs(finalPath)\n        bdata.write(finalPath / f'{fileName}.h5ad')\n        # Print\n        if verbose is True:\n            print('Given csObjects have been merged, head over to \"' + str(projectDir) + '/CSPOT/mergedcsObject\" to view results')\n\n\n    # return data\n    return bdata\n</code></pre>"},{"location":"Functions/scatterPlot/","title":"scatterPlot","text":"<p>Short Description</p> <p>The scatterPlot function can be used to create scatter plots of single-cell spatial data.  This function can be used to visualize the spatial distribution of positive  cells for a given marker, providing a quick and intuitive way to view the final predictions.</p>"},{"location":"Functions/scatterPlot/#cspot.scatterPlot--function","title":"Function","text":""},{"location":"Functions/scatterPlot/#cspot.scatterPlot.scatterPlot","title":"<code>scatterPlot(csObject, markers=None, cspotOutput='cspotOutput', x_coordinate='X_centroid', y_coordinate='Y_centroid', poscellsColor='#78290f', negcellsColor='#e5e5e5', s=None, ncols=5, alpha=1, dpi=200, figsize=(5, 5), invert_yaxis=True, outputDir=None, outputFileName='cspotPlot.png', **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>csObject</code> <code>anndata</code> <p>Pass the <code>csObject</code> loaded into memory or a path to the <code>csObject</code>  file (.h5ad).</p> required <code>markers</code> <code>str or list of str</code> <p>The name(s) of the markers to plot. If not provided, all markers will be plotted.</p> <code>None</code> <code>cspotOutput</code> <code>str</code> <p>The label underwhich the CSPOT output is stored within the object.</p> <code>'cspotOutput'</code> <code>x_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the X coordinates for each cell. </p> <code>'X_centroid'</code> <code>y_coordinate</code> <code>str</code> <p>The column name in <code>single-cell spatial table</code> that records the Y coordinates for each cell.</p> <code>'Y_centroid'</code> <code>poscellsColor</code> <code>str</code> <p>The color of positive cells.</p> <code>'#78290f'</code> <code>negcellsColor</code> <code>str</code> <p>The color of negative cells. </p> <code>'#e5e5e5'</code> <code>s</code> <code>float</code> <p>The size of the markers.</p> <code>None</code> <code>ncols</code> <code>int</code> <p>The number of columns in the final plot when multiple makers are plotted. </p> <code>5</code> <code>alpha</code> <code>float</code> <p>The alpha value of the points (controls opacity).</p> <code>1</code> <code>dpi</code> <code>int</code> <p>The DPI of the figure.</p> <code>200</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure.</p> <code>(5, 5)</code> <code>invert_yaxis</code> <code>bool</code> <p>Invert the Y-axis of the plot. </p> <code>True</code> <code>outputDir</code> <code>str</code> <p>The directory to save the output plot. </p> <code>None</code> <code>outputFileName</code> <code>str</code> <p>The name of the output file. Use desired file format as  suffix (e.g. <code>.png</code> pr <code>.pdf</code>)</p> <code>'cspotPlot.png'</code> <code>**kwargs</code> <code>keyword parameters</code> <p>Additional arguments to pass to the <code>matplotlib.scatter</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Plot</code> <code>image</code> <p>If <code>outputDir</code> is provided the plot will saved within the provided outputDir.</p> Example <pre><code># Prohect directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# path to the final output\ncsObject = '/Users/aj/Desktop/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n# Plot image to console\ncs.scatterPlot(csObject,\n    markers=['ECAD', 'CD8A', 'CD45'],\n    poscellsColor='#78290f',\n    negcellsColor='#e5e5e5',\n    s=3,\n    ncols=3,\n    dpi=90,\n    figsize=(4, 4),\n    outputDir=None,\n    outputFileName='cspotplot.png')\n\n# Same function if the user wants to run it via Command Line Interface\npython scatterPlot.py --csObject /Users/aj/Desktop/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad                             --markers ECAD CD8A                             --outputDir /Users/aj/Desktop/cspotExampleData/CSPOT\n</code></pre> Source code in <code>cspot/scatterPlot.py</code> <pre><code>def scatterPlot (csObject, \n                 markers=None, \n                 cspotOutput='cspotOutput',\n                 x_coordinate='X_centroid',\n                 y_coordinate='Y_centroid',\n                 poscellsColor='#78290f',\n                 negcellsColor='#e5e5e5',\n                 s=None,\n                 ncols=5,\n                 alpha=1,\n                 dpi=200,\n                 figsize=(5, 5),\n                 invert_yaxis=True,\n                 outputDir=None,\n                 outputFileName='cspotPlot.png',\n                 **kwargs):\n    \"\"\"\nParameters:\n    csObject (anndata):\n        Pass the `csObject` loaded into memory or a path to the `csObject` \n        file (.h5ad).\n\n    markers (str or list of str, optional): \n        The name(s) of the markers to plot. If not provided, all markers will be plotted.\n\n    cspotOutput (str, optional): \n        The label underwhich the CSPOT output is stored within the object.\n\n    x_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        X coordinates for each cell. \n\n    y_coordinate (str, optional):\n        The column name in `single-cell spatial table` that records the\n        Y coordinates for each cell.\n\n    poscellsColor (str, optional): \n        The color of positive cells.\n\n    negcellsColor (str, optional): \n        The color of negative cells. \n\n    s (float, optional): \n        The size of the markers.\n\n    ncols (int, optional): \n        The number of columns in the final plot when multiple makers are plotted. \n\n    alpha (float, optional): \n        The alpha value of the points (controls opacity).\n\n    dpi (int, optional): \n        The DPI of the figure.\n\n    figsize (tuple, optional): \n        The size of the figure.\n\n    invert_yaxis (bool, optional):  \n        Invert the Y-axis of the plot. \n\n    outputDir (str, optional): \n        The directory to save the output plot. \n\n    outputFileName (str, optional): \n        The name of the output file. Use desired file format as \n        suffix (e.g. `.png` pr `.pdf`)\n\n    **kwargs (keyword parameters):\n        Additional arguments to pass to the `matplotlib.scatter` function.\n\n\nReturns:\n    Plot (image):\n        If `outputDir` is provided the plot will saved within the\n        provided outputDir.\n\nExample:\n        ```python\n\n        # Prohect directory\n        projectDir = '/Users/aj/Documents/cspotExampleData'\n\n        # path to the final output\n        csObject = '/Users/aj/Desktop/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n        # Plot image to console\n        cs.scatterPlot(csObject,\n            markers=['ECAD', 'CD8A', 'CD45'],\n            poscellsColor='#78290f',\n            negcellsColor='#e5e5e5',\n            s=3,\n            ncols=3,\n            dpi=90,\n            figsize=(4, 4),\n            outputDir=None,\n            outputFileName='cspotplot.png')\n\n        # Same function if the user wants to run it via Command Line Interface\n        python scatterPlot.py --csObject /Users/aj/Desktop/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n                            --markers ECAD CD8A \\\n                            --outputDir /Users/aj/Desktop/cspotExampleData/CSPOT\n        ```\n\n    \"\"\"\n\n    # Load the andata object\n    if isinstance(csObject, str):\n        adata = ad.read(csObject)\n    else:\n        adata = csObject.copy()\n\n    # break the function if cspotOutput is not detectable\n    def check_key_exists(dictionary, key):\n        try:\n            # Check if the key exists in the dictionary\n            value = dictionary[key]\n        except KeyError:\n            # Return an error if the key does not exist\n            return \"Error: \" + str(cspotOutput) + \" does not exist, please check!\"\n    # Test\n    check_key_exists(dictionary=adata.uns, key=cspotOutput)\n\n\n    # convert marter to list\n    if markers is None:\n        markers = list(adata.uns[cspotOutput].columns)\n    if isinstance (markers, str):\n        markers = [markers]\n\n    # identify the x and y coordinates\n    x = adata.obs[x_coordinate]\n    y = adata.obs[y_coordinate]\n\n    # subset the cspotOutput with the requested markers\n    subset = adata.uns[cspotOutput][markers]\n    # get the list of columns to plot\n    cols_to_plot = subset.columns\n\n    # identify the number of columns to plot\n    ncols = min(ncols, len(cols_to_plot))\n\n    # calculate the number of rows needed for the subplot\n    nrows = (len(cols_to_plot) - 1) // ncols + 1\n\n    # resolve figsize\n    figsize = (figsize[0]*ncols, figsize[1]*nrows)\n\n    # Estimate point size\n    if s is None:\n        s = (100000 / adata.shape[0]) / len(cols_to_plot)\n\n    # FIIGURE\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=dpi)\n    for i, col in enumerate(cols_to_plot):\n        # get the classes for the current column\n        classes = list(subset[col])\n\n        # get the current subplot axes\n        if nrows==1 and ncols==1:\n            ax = axs\n        elif nrows==1 or ncols==1:\n            ax = axs[i]\n        else:\n            ax = axs[i // ncols, i % ncols]\n\n        # invert y-axis\n        if invert_yaxis is True:\n            ax.invert_yaxis()\n\n        # set the title of the subplot to the current column name\n        ax.set_title(col)\n\n        # plot the 'neg' points with a small size\n        neg_x = [x[j] for j in range(len(classes)) if classes[j] == 'neg']\n        neg_y = [y[j] for j in range(len(classes)) if classes[j] == 'neg']\n        #ax.scatter(x=neg_x, y=neg_y, c=negcellsColor, s=s, alpha=alpha)\n        ax.scatter(x=neg_x, y=neg_y, c=negcellsColor, s=s, linewidth=0, alpha=alpha, **kwargs)\n\n        # plot the 'pos' points on top of the 'neg' points with a larger size\n        pos_x = [x[j] for j in range(len(classes)) if classes[j] == 'pos']\n        pos_y = [y[j] for j in range(len(classes)) if classes[j] == 'pos']\n        #ax.scatter(x=pos_x, y=pos_y, c=poscellsColor, s=s, alpha=alpha)\n        ax.scatter(x=pos_x, y=pos_y, c=poscellsColor, s=s, linewidth=0, alpha=alpha, **kwargs)\n\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # remove any unused subplots\n    for i in range(len(cols_to_plot), nrows * ncols):\n        fig.delaxes(axs[i // ncols, i % ncols])\n\n    plt.tight_layout()\n\n    # save figure\n    if outputDir is not None:\n        plt.savefig(pathlib.Path(outputDir) / outputFileName)\n</code></pre>"},{"location":"Tutorials/md/BuildCSPOTModel/","title":"\ud83c\udfaf Build a CSPOT Model","text":"<p>The executable notebook can be downloaded here </p> <p>When following the tutorial, it is crucial to read the text as simply running the cells will not work!</p> <p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <p>Training a CSPOT Model involves the following steps: - For any given marker: Identify a image that could be used to generate postive and negative thumbnails - Run the <code>generateThumbnails</code> function on the image to auto generate postive and negative thumbnails - Go through the auto generated  thumbnails and remove any wrong assignments - On the user sorted set of thumbnails run the <code>generateTrainTestSplit</code> function to prepare the data for training a deep learning model - Lastly, run <code>csTrain</code> </p> <pre><code># import packages in jupyter notebook (not needed for command line interface users)\nimport cspot as cs\n</code></pre> <p>CSPOT auto generates subfolders and so always set a single folder as <code>projectDir</code> and cspot will use that for all subsequent steps. In this case we will set the downloaded sample data as our <code>projectDir</code>. My sample data is on my desktop as seen below.</p> <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Documents/cspotExampleData'\nimagePath = projectDir + '/image/exampleImage.tif'\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\nmarkerChannelMapPath = projectDir + '/markers.csv'\n</code></pre>"},{"location":"Tutorials/md/BuildCSPOTModel/#step-1-generate-thumbnails-for-training-data","title":"Step-1: Generate Thumbnails for Training Data","text":"<p>The first step would be to train a model to recognize the marker of interest. In this example the data contains 11 channels <code>DNA1, ECAD, CD45, CD4, CD3D, CD8A, CD45R, KI67</code> and as we are not interested in training a model to recognize DNA or background (<code>DNA1</code>), we will only need to generate training data for  <code>ECAD1, CD45, CD4, CD3D, CD8A &amp; KI67</code>. However for proof of concept, let us just train a model for <code>ECAD</code> and <code>CD3D</code>.</p> <p>To do so, the first step is to create examples of <code>postive</code> and <code>negative</code> examples for each marker of interest. To facilitate this process, we can use the <code>generateThumbnails</code> function in <code>CSPOT</code>. Under the hood the function auto identifies the cells that has high and low expression of the marker of interest and cuts out small thumbnails from the image.</p> <pre><code>cs.generateThumbnails ( spatialTablePath=spatialTablePath, \n                        imagePath=imagePath, \n                        markerChannelMapPath=markerChannelMapPath,\n                        markers=[\"ECAD\", \"CD3D\"], \n                        markerColumnName='marker',\n                        channelColumnName='channel',\n                        transformation=True, \n                        maxThumbnails=100, \n                        random_state=0,\n                        localNorm=True, \n                        globalNorm=False,\n                        x_coordinate='X_centroid', \n                        y_coordinate='Y_centroid',\n                        percentiles=[2, 12, 88, 98], \n                        windowSize=64,\n                        projectDir=projectDir)\n</code></pre> <pre><code>Processing Marker: ECAD\nProcessing Marker: CD3D\nThumbnails have been generated, head over to \"/Users/aj/Documents/cspotExampleData/CSPOT/Thumbnails\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python generateThumbnails.py \\\n            --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv \\\n            --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif \\\n            --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv \\\n            --markers ECAD CD3D \\\n            --maxThumbnails 100 \\\n            --projectDir /Users/aj/Documents/cspotExampleData/\n</code></pre></p> <p>The output from the above function will be stored under <code>CSPOT/Thumbnails/</code>. </p> <p>There are a number of parameters that function need to provided as seen above. Detailed explanations are avaialable in the documentation. Briefly, the function takes in the single-cell table (<code>spatialTablePath</code>) with X and Y coordinates, the full image (<code>imagePath</code>) and lastly a list of <code>markers</code> for which thumbnails need to be generated. Please note as the program does not know which channels in the image corresponds to the <code>markers</code>, hence, the <code>markerChannelMapPath</code> is used to supply a <code>.csv</code> file that maps the channels to the marker information. The <code>markerChannelMap</code> follow 1-indexing convention- so the first channel is represented by the number <code>1</code>. </p> <p>You would have also notices that I have set <code>maxThumbnails=100</code>. This basically means that even if more than 100 cells are identified, only 100 random cells will be used to generate the thumbnails. I generally generate about <code>2000</code> cells, however based on our estimates about 250 postive and 250 negative examples should be suffcient. As this is for illustration purpose only, I have set it to <code>100</code>.  </p> <p>Now that the thumbnails are generated, one would manually go through the <code>TruePos</code> folder and <code>TrueNeg</code> folder and move files around as necessary. If there are any truly negative thumbnails in the <code>TruePos</code> folder, move it to <code>PosToNeg</code> folder. Similarly, if there are any truly positive thumbnails in <code>TrueNeg</code> folder, move it to <code>NegToPos</code> folder. You will often notice that imaging artifacts are captured in the <code>TruePos</code> folder and there will also likely be a number of true positives in the <code>TrueNeg</code> folder as the field of view (64x64) is larger than what the program used to identify those thumbnails (just the centroids of single cells at the center of that thumbnail).    </p> <p>While you are manually sorting the postives and negative thumbnails, please keep in mind that you are looking for high-confident positives and high-confident negatives. It is absolutely okay to delete off majority of the thumbnails that you are not confident about. This infact makes it easy and fast as you are looking to only keep only thumbnails that are readily sortable.  </p> <p>Lastly, I generally use a whole slide image to generate these thumbnails as there will be enough regions with high expression and no expression of the marker of interest. If you look at the thumbnails of this dummy example, you will notice that most thumbnails of <code>TrueNeg</code> for <code>ECAD</code> does contain some level of <code>ECAD</code> as there is not enough regions to sample from. </p>"},{"location":"Tutorials/md/BuildCSPOTModel/#step-1a-optional","title":"Step-1a (optional)","text":"<p>You might have noticed in the above example, I had set <code>localNorm=True</code>, which is on by default. This parameter essentially creates a mirror duplicate copy of all the thumbnails and saves it under a folder named <code>localNorm</code>. The difference being that each thumbnail is normalized to the maximum intensity pixel in that thumbnail. It helps to visually sort out the true positives and negatives faster and more reliably. As we will not use the thumbnails in the <code>localNorm</code> for training the deep learning model, we want to make sure all the manual sorting that we did in the <code>localNorm</code> folder is copied over to the real training data. I have written an additional function to help with this. Any moving or deleting of files that you did in the <code>localNorm</code> folder will be copied over to the real training data.  </p> <p>Randomly shift and delete some files from <code>TruePos</code> -&gt; <code>PosToNeg</code> and <code>TrueNeg</code> -&gt; <code>NegToPos</code>   for   <code>CD3D</code> for the purpose of illustration and run the  <code>cloneFolder</code> function to see what happens.</p> <pre><code># list of folders to copy settings from\ncopyFolder = [projectDir + '/CSPOT/Thumbnails/localNorm/CD3D',\n              projectDir + '/CSPOT/Thumbnails/localNorm/ECAD']\n# list of folders to apply setting to\napplyFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n               projectDir + '/CSPOT/Thumbnails/ECAD']\n# note: Every copyFolder should have a corresponding applyFolder. The order matters! \n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\ncs.cloneFolder (copyFolder, \n                applyFolder, \n                TruePos='TruePos', TrueNeg='TrueNeg', \n                PosToNeg='PosToNeg', NegToPos='NegToPos')\n</code></pre> <pre><code>Processing: CD3D\nProcessing: ECAD\nCloning Folder is complete, head over to /CSPOT/Thumbnails\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python cloneFolder.py \\\n            --copyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/localNorm/ECAD \\\n            --applyFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD\n</code></pre></p> <p>If you head over to the training data thumbails you will notice that the files have been shifited around exactly as in the <code>localNorm</code> folder.</p>"},{"location":"Tutorials/md/BuildCSPOTModel/#step-2-generate-masks-for-training-data","title":"Step-2: Generate Masks for Training Data","text":"<p>To train the deep learning model, in addition to the raw thumbnails a mask is needed. The mask lets the model know where the cell is located. Ideally one would manually draw on the thumbnails to locate where the positive cells are, however for the pupose of scalability we will use automated approaches to generate the Mask for us. The following function will generate the mask and split the data into <code>training, validation and test</code> that can be directly fed into the deep learning algorithm.</p> <pre><code>thumbnailFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                   projectDir + '/CSPOT/Thumbnails/ECAD']\n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n# If you had deleted any of the folders and are not using them, replace the folder name with `None` in the parameter.\ncs.generateTrainTestSplit ( thumbnailFolder, \n                            projectDir=projectDir,\n                            file_extension=None,\n                            TruePos='TruePos', NegToPos='NegToPos',\n                            TrueNeg='TrueNeg', PosToNeg='PosToNeg')\n</code></pre> <pre><code>Processing: CD3D\nProcessing: ECAD\nTraining data has been generated, head over to \"/Users/aj/Documents/cspotExampleData/CSPOT/TrainingData\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python generateTrainTestSplit.py \\\n            --thumbnailFolder /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/CD3D /Users/aj/Desktop/cspotExampleData/CSPOT/Thumbnails/ECAD \\\n            --projectDir /Users/aj/Desktop/cspotExampleData/\n</code></pre></p> <p>If you head over to <code>CSPOT/TrainingData/</code>, you will notice that each of the supplied marker above will have a folder with the associated <code>training, validataion and test</code> data that is required by the deep-learning algorithm to generate the model. </p>"},{"location":"Tutorials/md/BuildCSPOTModel/#step-3-train-the-cspot-model","title":"Step-3: Train the CSPOT Model","text":"<p>The function trains a deep learning model for each marker in the provided training data. To train the <code>cspotModel</code>, simply direct the function to the <code>TrainingData</code> folder. To train only specific models, specify the folder names using the <code>trainMarkers</code> parameter. The 'outputDir' remains constant and the program will automatically create subfolders to save the trained models.</p> <pre><code>trainingDataPath = projectDir + '/CSPOT/TrainingData'\n\ncs.csTrain(trainingDataPath=trainingDataPath,\n               projectDir=projectDir,\n               trainMarkers=None,\n               artefactPath=None,\n               imSize=64,\n               nChannels=1,\n               nClasses=2,\n               nExtraConvs=0,\n               nLayers=3,\n               featMapsFact=2,\n               downSampFact=2,\n               ks=3,\n               nOut0=16,\n               stdDev0=0.03,\n               batchSize=16,\n               epochs=1)\n</code></pre> <pre><code>WARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\n/Users/aj/Documents/cspotExampleData/CSPOT/TrainingData/CD3D/training\nTraining for 8 steps\nFound 122 training images\nFound 41 validation images\nFound 41 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 16.0 and 0.0 for global max and min intensities.\nClass balance ratio is 18.959115759448537\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nstep 00000, e: 0.506249, epoch: 1\nModel saved in file: /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nstep 00001, e: 0.500656, epoch: 1\nstep 00002, e: 0.481774, epoch: 1\nstep 00003, e: 0.466444, epoch: 1\nstep 00004, e: 0.441243, epoch: 1\nstep 00005, e: 0.492766, epoch: 1\nstep 00006, e: 0.522770, epoch: 2\nstep 00007, e: 0.508577, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nModel restored.\n/Users/aj/Documents/cspotExampleData/CSPOT/TrainingData/ECAD/training\nTraining for 8 steps\nFound 120 training images\nFound 40 validation images\nFound 40 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 70.0 and 6.0 for global max and min intensities.\nClass balance ratio is 6.6801200018750295\nstep 00000, e: 0.498890, epoch: 1\nModel saved in file: /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nstep 00001, e: 0.494717, epoch: 1\nstep 00002, e: 0.510676, epoch: 1\nstep 00003, e: 0.480390, epoch: 1\nstep 00004, e: 0.478806, epoch: 1\nstep 00005, e: 0.516272, epoch: 1\nstep 00006, e: 0.512058, epoch: 2\nstep 00007, e: 0.480931, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nModel restored.\nCSPOT Models have been generated, head over to \"/Users/aj/Documents/cspotExampleData/CSPOT/cspotModel\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csTrain.py \\\n        --trainingDataPath /Users/aj/Documents/cspotExampleData/CSPOT/TrainingData \\\n        --projectDir /Users/aj/Documents/cspotExampleData/ \\\n        --epochs 1\n</code></pre></p> <pre><code># this tutorial ends here. Move to the Run CSPOT Algorithm Tutorial\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/DownloadTutorialData/","title":"\ud83c\udfaf CSPOT Exemplar Data","text":"<p>A short guide to downloading our exemplar data for trying out the CSPOT algorithm.</p> <p>If you want to try out the CSPOT algorithm, follow these simple steps to download our exemplar data:</p> <ol> <li> <p>Navigate to the Harvard Dataverse website and download the exemplar data (see screenshot below!)</p> <p>Data: https://doi.org/10.7910/DVN/C45JWT</p> </li> </ol> <p></p> <ol> <li>Click on the \"Access Dataset\" \u2192 \"Original Format ZIP\" button.</li> <li>Wait for the download to complete. The size of the file is approximately 40 MB.</li> <li>Unzip the downloaded file to access the data.</li> </ol> <p>To ensure that you can follow the tutorial correctly, please make sure that the following folder structure (screenshot below) is maintained:</p> <p></p>"},{"location":"Tutorials/md/DownloadTutorialData/#models-used-in-the-manuscript","title":"Models used in the Manuscript","text":"<p>Please note that the purpose of this tutorial is to demonstrate how to run the cspot algorithm and should not be expected to produce meaningful results, especially in the model training section.</p> <p>For your reference, we have included the models used in the associated manuscript in the <code>manuscriptModels</code> folder.</p> <p>If you have any questions or encounter any issues while following the tutorial, please don't hesitate to contact us for assistance.</p> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/HelperFunctions/","title":"\ud83c\udfaf CSPOT Helper Functions","text":""},{"location":"Tutorials/md/HelperFunctions/#export-the-results-to-csv","title":"Export the results to <code>.csv</code>","text":"<p>Once the CSPOT pipeline has been executed, all the output is stored within the csObject. An efficient way to export the results of csScore, cspotOutput, and the rescaled data is by using a function that saves them as a CSV file. This allows for easy sharing and analysis of the data in other programs.</p> <pre><code># import packages\nimport cspot as cs\n</code></pre> <pre><code># path to files needed for csExport\nprojectDir = '/Users/aj/Documents/cspotExampleData'\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n</code></pre> <pre><code>cs.csExport(csObject,\n               projectDir,\n               fileName=None,\n               raw=False,\n               CellID='CellID',\n               verbose=True)\n</code></pre> <pre><code>Contents of the csObject have been exported to \"/Users/aj/Documents/cspotExampleData/CSPOT/csExport\"\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csExport.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p>"},{"location":"Tutorials/md/HelperFunctions/#we-also-provide-a-helper-functions-to-vizualize-the-identified-postive-and-negative-cells-for-each-marker","title":"We also provide a helper functions to vizualize the identified postive and negative cells for each marker.","text":"<p>The <code>addPredictions</code> function serves as a link between <code>cspot</code> and <code>scimap</code> package. It's useful for evaluating model performance. The function transforms results stored in <code>anndata.uns</code> to <code>anndata.obs</code> so they can be visualized using the <code>scimap</code> package's <code>sm.pl.image viewer</code> function. This displays <code>positive</code> and <code>negative</code> cells overlaid on the raw image.</p> <p>The <code>addPredictions</code> function can take in two methods.  <code>cspotOutput</code> displays the result of running the <code>cspot</code> function,  while <code>csScore</code> shows the raw output produced by the <code>csScore</code>  function, which returns a probability score. The <code>midpoint</code> parameter,  with a default value of 0.5, can be adjusted to define what is considered a <code>positive</code> result, when method is set to <code>csScore</code>.</p> <pre><code># Path to csObject\ncsObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\nadata = cs.addPredictions (csObject, \n                    method='cspotOutput',\n                    cspotOutput='cspotOutput',\n                    csScore='csScore', \n                    midpoint=0.5)\n</code></pre> <pre><code># check the results\nadata.obs.columns\n</code></pre> <pre><code>Index(['X_centroid', 'Y_centroid', 'Area', 'MajorAxisLength',\n       'MinorAxisLength', 'Eccentricity', 'Solidity', 'Extent', 'Orientation',\n       'CellID', 'imageid', 'p_ECAD', 'p_CD45', 'p_CD4', 'p_CD3D', 'p_CD8A',\n       'p_CD45R', 'p_KI67'],\n      dtype='object')\n</code></pre> <p>As it can be seen the addition of <code>p_CD45, p_CD4, p_CD8A, p_CD45R, p_KI67, p_ECAD, p_CD3D</code> to <code>adata.obs</code>. These columns can be vizualized with <code>scimap</code>. </p>"},{"location":"Tutorials/md/HelperFunctions/#we-recommend-creating-a-new-environment-to-install-scimap","title":"We recommend creating a new environment to install scimap","text":"<p>Download and install the scimap package. We recommend creating a new conda/python environment</p> <pre><code># create new conda env (assuming you have conda installed): executed in the conda command prompt or terminal\nconda create --name scimap -y python=3.8\nconda activate scimap\n</code></pre> <p>Install <code>scimap</code> within the conda environment.</p> <pre><code>pip install scimap\n\n# install jupyter notebook if you want to simply execute this notebook.\npip install notebook\n</code></pre> <p>Once <code>scimap</code> is installed the following function can be used to vizualize the results</p> <pre><code># import\nimport scimap as sm\nimport anndata as ad\n\n# import the csObject\ncwd = '/Users/aj/Desktop/cspotExampleData'\ncsObject = cwd + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\nadata = ad.read(csObject)\n\n# Path to the raw image\nimage_path = '/Users/aj/Documents/cspotExampleData/image/exampleImage.tif'\nsm.image_viewer(image_path, adata, overlay='p_CD45')\n</code></pre>"},{"location":"Tutorials/md/InstallationandSetUp/","title":"\ud83c\udfaf Setting up CSPOT","text":""},{"location":"Tutorials/md/InstallationandSetUp/#kindly-note-that-cspot-is-not-a-plug-and-play-solution-rather-its-a-framework-that-requires-significant-upfront-investment-of-time-from-potential-users-for-training-and-validating-deep-learning-models-which-can-then-be-utilized-in-a-plug-and-play-manner-for-processing-large-volumes-of-similar-multiplexed-imaging-data","title":"Kindly note that CSPOT is not a plug-and-play solution, rather it's a framework that requires significant upfront investment of time from potential users for training and validating deep learning models, which can then be utilized in a plug-and-play manner for processing large volumes of similar multiplexed imaging data.","text":"<p>There are two ways to set it up based on how you would like to run the program - Using an interactive environment like Jupyter Notebooks - Using Command Line Interface</p> <p>Before we set up CSPOT, we highly recommend using a environment manager like Conda. Using an environment manager like Conda allows you to create and manage isolated environments with specific package versions and dependencies. </p> <p>Download and Install the right conda based on the opertating system that you are using</p>"},{"location":"Tutorials/md/InstallationandSetUp/#lets-create-a-new-conda-environment-and-install-cspot","title":"Let's create a new conda environment and install CSPOT","text":"<p>use the terminal (mac/linux) and anaconda promt (windows) to run the following command <pre><code>conda create --name cspot -y python=3.9\n</code></pre></p> <p>Install <code>CSPOT</code> within the conda environment.</p> <pre><code>conda activate cspot\npip install cspot\n</code></pre>"},{"location":"Tutorials/md/InstallationandSetUp/#if-you-would-like-cspot-to-use-gpu","title":"If you would like CSPOT to use GPU:","text":"<p>cspot uses Tensorflow. Please install necessary packages for tensorflow to recogonise your specific GPU.  </p>"},{"location":"Tutorials/md/InstallationandSetUp/#download-the-exemplar-dataset","title":"Download the Exemplar Dataset","text":"<p>To help you get used to the program we have provided some dummy data.  Download link to the exemplar dataset provided here. All of the following files are mandatory for running cspot, but <code>phenotype_workflow.csv</code> is optional and can be skipped if single cell phenotyping is not required. <code>manuscriptModels</code> is provided explicitly for going through this tutorial.  <pre><code>cspotExampleData/\n\u251c\u2500\u2500 image\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 exampleImage.tif\n\u251c\u2500\u2500 manuscriptModels\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CD3D\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CD4\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CD45\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CD8A\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ECAD\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 KI67\n\u251c\u2500\u2500 markers.csv\n\u251c\u2500\u2500 phenotype_workflow.csv\n\u251c\u2500\u2500 quantification\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 exampleSpatialTable.csv\n\u2514\u2500\u2500 segmentation\n    \u2514\u2500\u2500 exampleSegmentationMask.tif\n</code></pre></p>"},{"location":"Tutorials/md/InstallationandSetUp/#method-1-set-up-jupyter-notebook-if-you-would-like-to-run-cspot-in-an-interactive-setting","title":"Method 1: Set up Jupyter Notebook (If you would like to run CSPOT in an interactive setting)","text":"<p>Install jupyter notebook within the conda environment <pre><code>conda activate cspot\npip install notebook\n</code></pre> After installation, open Jupyter Notebook by typing the following command in the terminal, ensuring that the cspot environment is activated and you are within the environment before executing the jupyter notebook command. <pre><code>jupyter notebook\n</code></pre> We will talk about how to run cspot in the next tutorial.</p>"},{"location":"Tutorials/md/InstallationandSetUp/#method-2-set-up-command-line-interface-if-you-like-to-run-cspot-in-the-cli-hpc-etc","title":"Method 2: Set up Command Line Interface (If you like to run CSPOT in the CLI, HPC, etc)","text":"<p>Activate the conda environment that you created earlier</p> <pre><code>conda activate cspot\n</code></pre> <p>MAC / LINUX / WSL If you have git installed you can clone the repo with the following command <pre><code>git clone https://github.com/nirmallab/cspot\ncd cspot/cspot/\n</code></pre></p> <p>OR </p> <pre><code>wget https://github.com/nirmalLab/cspot/archive/main.zip\nunzip main.zip \ncd cspot-main/cspot \n</code></pre> <p>WINDOWS If you have git installed you can pull the repo with the following command <pre><code>git clone https://github.com/nirmallab/cspot\n</code></pre></p> <p>OR </p> <p>Head over to https://github.com/nirmallab/cspot in your browser and download the repo.</p>"},{"location":"Tutorials/md/InstallationandSetUp/#method-3-set-up-docker","title":"Method 3: Set up Docker","text":"<p>Follow the docker installation guide to install docker: https://docs.docker.com/engine/install/</p> <p>Download CSPOT from Docker Hub <pre><code>docker pull nirmallab/cspot:latest\n</code></pre></p> <p>There is a special tutorial on how to run cspot with docker, please refer to that for further instructions.</p> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/PhenotypeCells/","title":"\ud83c\udfaf CSPOT Phenotyping","text":"<p>Assign phenotypes to each cell. Clustering data may not always be ideal, so we developed a cell type assignment algorithm that does a hierarchical assignment process iteratively.</p> <p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <p>Download executable notebook here.</p> <p>Make sure you have completed <code>Build cepot Model</code> and <code>Run cspot Algorithm</code> Tutorial before you try to execute this Jupyter Notebook!</p> <pre><code># import packages\nimport cspot as cs\nimport pandas as pd\n</code></pre> <p>We need <code>two</code> basic inputs to perform phenotyping with CSPOT - The cspot Object - A Phenotyping workflow based on prior knowledge</p> <pre><code># set the Project directory\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n# Path to the CSPOT Object\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n</code></pre> <pre><code># load the phenotyping workflow\nphenotype = pd.read_csv(str(projectDir) + '/phenotype_workflow.csv')\n# view the table:\nphenotype.style.format(na_rep='')\n</code></pre> Unnamed: 0 Unnamed: 1 ECAD CD45 CD4 CD3D CD8A KI67 0 all Immune anypos anypos anypos anypos 1 all ECAD+ pos 2 ECAD+ KI67+ ECAD+ pos 3 Immune CD4+ T allpos allpos 4 Immune CD8+ T allpos allpos 5 Immune Non T CD4+ cells pos neg <p>As it can be seen from the table above, (1) The <code>first column</code> has to contain the cell that are to be classified. (2) The <code>second column</code> indicates the phenotype a particular cell will be assigned if it satifies the conditions in the row. (3) <code>Column three</code> and onward represent protein markers. If the protein marker is known to be expressed for that cell type, then it is denoted by either <code>pos</code>, <code>allpos</code>. If the protein marker is known to not express for a cell type it can be denoted by <code>neg</code>, <code>allneg</code>. If the protein marker is irrelevant or uncertain to express for a cell type, then it is left empty. <code>anypos</code> and <code>anyneg</code> are options for using a set of markers and if any of the marker is positive or negative, the cell type is denoted accordingly.</p> <p>To give users maximum flexibility in identifying desired cell types, we have implemented various classification arguments as described above for strategical classification. They include</p> <ul> <li>allpos</li> <li>allneg</li> <li>anypos</li> <li>anyneg</li> <li>pos</li> <li>neg</li> </ul> <p><code>pos</code> : \"Pos\" looks for cells positive for a given marker. If multiple markers are annotated as <code>pos</code>, all must be positive to denote the cell type. For example, a Regulatory T cell can be defined as <code>CD3+CD4+FOXP3+</code> by passing <code>pos</code> to each marker. If one or more markers don't meet the criteria (e.g. CD4-), the program will classify it as <code>Likely-Regulatory-T cell</code>, pending user confirmation. This is useful in cases of technical artifacts or when cell types (such as cancer cells) are defined by marker loss (e.g. T-cell Lymphomas).</p> <p><code>neg</code> : Same as <code>pos</code> but looks for negativity of the defined markers. </p> <p><code>allpos</code> : \"Allpos\" requires all defined markers to be positive. Unlike <code>pos</code>, it doesn't classify cells as <code>Likely-cellType</code>, but strictly annotates cells positive for all defined markers.</p> <p><code>allneg</code> : Same as <code>allpos</code> but looks for negativity of the defined markers. </p> <p><code>anypos</code> : \"Anypos\" requires only one of the defined markers to be positive. For example, to define macrophages, a cell could be designated as such if any of <code>CD68</code>, <code>CD163</code>, or <code>CD206</code> is positive.</p> <p><code>anyneg</code> : Same as <code>anyneg</code> but looks for negativity of the defined markers. </p> <pre><code>adata = cs.csPhenotype ( csObject=csObject,\n                            phenotype=phenotype,\n                            midpoint = 0.5,\n                            label=\"phenotype\",\n                            imageid='imageid',\n                            pheno_threshold_percent=None,\n                            pheno_threshold_abs=None,\n                            fileName=None,\n                            projectDir=projectDir)\n</code></pre> <pre><code>Phenotyping Immune\nPhenotyping ECAD+\n-- Subsetting ECAD+\nPhenotyping KI67+ ECAD+\n-- Subsetting Immune\nPhenotyping CD4+ T\nPhenotyping CD8+ T\nPhenotyping Non T CD4+ cells\nConsolidating the phenotypes across all groups\nModified csObject is stored at \"/Users/aj/Documents/cspotExampleData/CSPOT/csPhenotype\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:259: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:259: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csPhenotype.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad \\\n            --phenotype /Users/aj/Documents/cspotExampleData/phenotype_workflow.csv \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>If you had provided <code>projectDir</code> the modified csObject would be stored in <code>CSPOT/csPhenotype/</code>, else, the object will be returned to memory.</p> <pre><code># check the identified phenotypes\nadata.obs['phenotype'].value_counts()\n</code></pre> <pre><code>KI67+ ECAD+    6159\nCD4+ T         5785\nCD8+ T          816\nName: phenotype, dtype: int64\n</code></pre> <pre><code># Tutorial Ends here (check out some of the helper functions!)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/Quick%20Start%20Guide/","title":"\ud83c\udfaf Quick Start Guide","text":""},{"location":"Tutorials/md/Quick%20Start%20Guide/#getting-started-with-cspot","title":"\ud83d\udc0a Getting Started with CSPOT","text":"<p>Kindly note that CSPOT is not a plug-and-play solution. It's a framework that requires significant upfront investment of time from potential users for training and validating deep learning models, which can then be utilized in a plug-and-play manner for processing large volumes of similar multiplexed imaging data.  </p> <p>Before we set up CSPOT, we highly recommend using a environment manager like Conda. Using an environment manager like Conda allows you to create and manage isolated environments with specific package versions and dependencies. </p> <p>Download and Install the right conda based on the opertating system that you are using</p>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#create-a-new-conda-environment","title":"Create a new conda environment","text":"<pre><code># use the terminal (mac/linux) and anaconda promt (windows) to run the following command\nconda create --name cspot -y python=3.9\nconda activate cspot\n</code></pre> <p>Install <code>cspot</code> within the conda environment.</p> <pre><code>pip install cspot\npip install notebook\n\n# after it is installed open the notebook by typing the following in the terminal:\njupyter notebook\n</code></pre>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#build-a-cspot-model","title":"Build a CSPOT Model","text":"<p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <pre><code># import packages in jupyter notebook (not needed for command line interface users)\nimport cspot as cs\n</code></pre> <p>CSPOT auto generates subfolders and so always set a single folder as <code>projectDir</code> and cspot will use that for all subsequent steps. In this case we will set the downloaded sample data as our <code>projectDir</code>. My sample data is on my desktop as seen below.</p> <pre><code># set the working directory &amp; set paths to the example data\nprojectDir = '/Users/aj/Downloads/cspotExampleData'\nimagePath = projectDir + '/image/exampleImage.tif'\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\nmarkerChannelMapPath = projectDir + '/markers.csv'\n</code></pre>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#step-1-generate-thumbnails-for-training-data","title":"Step-1: Generate Thumbnails for Training Data","text":"<p>The first step would be to train a model to recognize the marker of interest. In this example the data contains 11 channels <code>DNA1, ECAD, CD45, CD4, CD3D, CD8A, CD45R, KI67</code> and as we are not interested in training a model to recognize DNA or background (<code>DNA1</code>), we will only need to generate training data for  <code>ECAD1, CD45, CD4, CD3D, CD8A &amp; KI67</code>. However for proof of concept, let us just train a model for <code>ECAD</code> and <code>CD3D</code>.</p> <p>To do so, the first step is to create examples of <code>postive</code> and <code>negative</code> examples for each marker of interest. To facilitate this process, we can use the <code>generateThumbnails</code> function in <code>CSPOT</code>. Under the hood the function auto identifies the cells that has high and low expression of the marker of interest and cuts out small thumbnails from the image.</p> <pre><code>cs.generateThumbnails ( spatialTablePath=spatialTablePath, \n                        imagePath=imagePath, \n                        markerChannelMapPath=markerChannelMapPath,\n                        markers=[\"ECAD\", \"CD3D\"], \n                        markerColumnName='marker',\n                        channelColumnName='channel',\n                        transformation=True, \n                        maxThumbnails=100, \n                        random_state=0,\n                        localNorm=True, \n                        globalNorm=False,\n                        x_coordinate='X_centroid', \n                        y_coordinate='Y_centroid',\n                        percentiles=[2, 12, 88, 98], \n                        windowSize=64,\n                        projectDir=projectDir)\n</code></pre> <pre><code>Processing Marker: ECAD\nProcessing Marker: CD3D\nThumbnails have been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/Thumbnails\" to view results\n</code></pre> <p>The output from the above function will be stored under <code>CSPOT/Thumbnails/</code>. </p> <p>Now that the thumbnails are generated, one would manually go through the <code>TruePos</code> folder and <code>TrueNeg</code> folder and move files around as necessary. If there are any truly negative thumbnails in the <code>TruePos</code> folder, move it to <code>PosToNeg</code> folder. Similarly, if there are any truly positive thumbnails in <code>TrueNeg</code> folder, move it to <code>NegToPos</code> folder. You will often notice that imaging artifacts are captured in the <code>TruePos</code> folder and there will also likely be a number of true positives in the <code>TrueNeg</code> folder as the field of view (64x64) is larger than what the program used to identify those thumbnails (just the centroids of single cells at the center of that thumbnail).    </p> <p>While you are manually sorting the postives and negative thumbnails, please keep in mind that you are looking for high-confident positives and high-confident negatives. It is absolutely okay to delete off majority of the thumbnails that you are not confident about. This infact makes it easy and fast as you are looking to only keep only thumbnails that are readily sortable.  </p> <p>Lastly, I generally use a whole slide image to generate these thumbnails as there will be enough regions with high expression and no expression of the marker of interest. If you look at the thumbnails of this dummy example, you will notice that most thumbnails of <code>TrueNeg</code> for <code>ECAD</code> does contain some level of <code>ECAD</code> as there is not enough regions to sample from. </p>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#step-2-generate-masks-for-training-data","title":"Step-2: Generate Masks for Training Data","text":"<p>To train the deep learning model, in addition to the raw thumbnails a mask is needed. The mask lets the model know where the cell is located. Ideally one would manually draw on the thumbnails to locate where the positive cells are, however for the pupose of scalability we will use automated approaches to generate the Mask for us. The following function will generate the mask and split the data into <code>training, validation and test</code> that can be directly fed into the deep learning algorithm.</p> <pre><code>thumbnailFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                   projectDir + '/CSPOT/Thumbnails/ECAD']\n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n# If you had deleted any of the folders and are not using them, replace the folder name with `None` in the parameter.\ncs.generateTrainTestSplit ( thumbnailFolder, \n                            projectDir=projectDir,\n                            file_extension=None,\n                            TruePos='TruePos', NegToPos='NegToPos',\n                            TrueNeg='TrueNeg', PosToNeg='PosToNeg')\n</code></pre> <pre><code>Processing: CD3D\nProcessing: ECAD\nTraining data has been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData\" to view results\n</code></pre> <p>If you head over to <code>CSPOT/TrainingData/</code>, you will notice that each of the supplied marker above will have a folder with the associated <code>training, validataion and test</code> data that is required by the deep-learning algorithm to generate the model. </p>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#step-3-train-the-cspot-model","title":"Step-3: Train the CSPOT Model","text":"<p>The function trains a deep learning model for each marker in the provided training data. To train the <code>cspotModel</code>, simply direct the function to the <code>TrainingData</code> folder. To train only specific models, specify the folder names using the <code>trainMarkers</code> parameter. The 'outputDir' remains constant and the program will automatically create subfolders to save the trained models.</p> <pre><code>trainingDataPath = projectDir + '/CSPOT/TrainingData'\n\ncs.csTrain(trainingDataPath=trainingDataPath,\n               projectDir=projectDir,\n               trainMarkers=None,\n               artefactPath=None,\n               imSize=64,\n               nChannels=1,\n               nClasses=2,\n               nExtraConvs=0,\n               nLayers=3,\n               featMapsFact=2,\n               downSampFact=2,\n               ks=3,\n               nOut0=16,\n               stdDev0=0.03,\n               batchSize=16,\n               epochs=1)\n</code></pre> <pre><code>WARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\n/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData/CD3D/training\nTraining for 8 steps\nFound 120 training images\nFound 40 validation images\nFound 40 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 16.0 and 0.0 for global max and min intensities.\nClass balance ratio is 15.749131057043549\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nstep 00000, e: 0.524932, epoch: 1\nModel saved in file: /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nstep 00001, e: 0.393462, epoch: 1\nstep 00002, e: 0.706428, epoch: 1\nstep 00003, e: 0.483075, epoch: 1\nstep 00004, e: 0.619706, epoch: 1\nstep 00005, e: 0.435708, epoch: 1\nstep 00006, e: 0.453731, epoch: 2\nstep 00007, e: 0.338719, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nModel restored.\n/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData/ECAD/training\nTraining for 8 steps\nFound 120 training images\nFound 40 validation images\nFound 40 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 63.0 and 0.0 for global max and min intensities.\nClass balance ratio is 6.543625397117731\nstep 00000, e: 0.480363, epoch: 1\nModel saved in file: /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nstep 00001, e: 0.499994, epoch: 1\nstep 00002, e: 0.507299, epoch: 1\nstep 00003, e: 0.496521, epoch: 1\nstep 00004, e: 0.494144, epoch: 1\nstep 00005, e: 0.541110, epoch: 1\nstep 00006, e: 0.499454, epoch: 2\nstep 00007, e: 0.492319, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nModel restored.\nCSPOT Models have been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel\" to view results\n</code></pre>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#step-4-predict-on-a-new-image","title":"Step-4: Predict on a new image","text":"<p>Now that the model is trained, we can predict the expression of the trained markers on a new image that the model has not seen before. </p> <pre><code># cspotpredict related paths\ncsModelPath = projectDir + '/manuscriptModels/'\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n</code></pre> <pre><code># Run the pipeline (For function specific parameters, check the documentation)\ncs.csPipeline(   \n                    # parameters for cspotPredict function\n                    imagePath=imagePath,\n                    csModelPath=csModelPath,\n                    markerChannelMapPath=markerChannelMapPath,\n\n                    # parameters for cspotCScore function\n                    segmentationMaskPath=segmentationPath,\n\n                    # parameters for cspotObject function\n                    spatialTablePath=spatialTablePath,\n\n                    # common parameters\n                    verbose=False,\n                    projectDir=projectDir)\n</code></pre> <pre><code>loading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/manuscriptModels/ECAD/model.ckpt\nModel restored.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\nInference...\nloading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/manuscriptModels/CD3D/model.ckpt\nModel restored.\nInference...\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/cspot.py:386: RuntimeWarning: divide by zero encountered in divide\n  below_midpoint = (below_midpoint - min_below) / range_below\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/cspot.py:386: RuntimeWarning: invalid value encountered in divide\n  below_midpoint = (below_midpoint - min_below) / range_below\n</code></pre> <p>Head over to <code>CSPOT/cspotOutput</code> to view results</p>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#visualize-the-results","title":"Visualize the results","text":"<p>Let us visualize the marker postivity of three markers using a helper plotting function provided within CSPOT.</p> <pre><code>csObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n# Plot image to console\ncs.scatterPlot(csObject,\n            markers=['ECAD', 'CD8A', 'CD45'],\n            poscellsColor='#78290f',\n            negcellsColor='#e5e5e5',\n            s=3,\n            ncols=3,\n            dpi=90,\n            figsize=(4, 4),\n            outputDir=None,\n            outputFileName='cspotplot.png')\n</code></pre> <p></p>"},{"location":"Tutorials/md/Quick%20Start%20Guide/#step-5-cspot-phenotyping","title":"Step-5: CSPOT Phenotyping","text":"<p>Assign phenotypes to each cell. Clustering data may not always be ideal, so we developed a cell type assignment algorithm that does a hierarchical assignment process iteratively.</p> <p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <pre><code># import packages\nimport pandas as pd\n</code></pre> <p>We need <code>two</code> basic inputs to perform phenotyping with CSPOT - The cspot Object - A Phenotyping workflow based on prior knowledge</p> <pre><code># Path to the CSPOT Object\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n</code></pre> <pre><code># load the phenotyping workflow\nphenotype = pd.read_csv(str(projectDir) + '/phenotype_workflow.csv')\n# view the table:\nphenotype.style.format(na_rep='')\n</code></pre> Unnamed: 0 Unnamed: 1 ECAD CD45 CD4 CD3D CD8A KI67 0 all Immune anypos anypos anypos anypos 1 all ECAD+ pos 2 ECAD+ KI67+ ECAD+ pos 3 Immune CD4+ T allpos allpos 4 Immune CD8+ T allpos allpos 5 Immune Non T CD4+ cells pos neg <p>As it can be seen from the table above, (1) The <code>first column</code> has to contain the cell that are to be classified. (2) The <code>second column</code> indicates the phenotype a particular cell will be assigned if it satifies the conditions in the row. (3) <code>Column three</code> and onward represent protein markers. If the protein marker is known to be expressed for that cell type, then it is denoted by either <code>pos</code>, <code>allpos</code>. If the protein marker is known to not express for a cell type it can be denoted by <code>neg</code>, <code>allneg</code>. If the protein marker is irrelevant or uncertain to express for a cell type, then it is left empty. <code>anypos</code> and <code>anyneg</code> are options for using a set of markers and if any of the marker is positive or negative, the cell type is denoted accordingly.</p> <p>To give users maximum flexibility in identifying desired cell types, we have implemented various classification arguments as described above for strategical classification. They include</p> <ul> <li>allpos</li> <li>allneg</li> <li>anypos</li> <li>anyneg</li> <li>pos</li> <li>neg</li> </ul> <p><code>pos</code> : \"Pos\" looks for cells positive for a given marker. If multiple markers are annotated as <code>pos</code>, all must be positive to denote the cell type. For example, a Regulatory T cell can be defined as <code>CD3+CD4+FOXP3+</code> by passing <code>pos</code> to each marker. If one or more markers don't meet the criteria (e.g. CD4-), the program will classify it as <code>Likely-Regulatory-T cell</code>, pending user confirmation. This is useful in cases of technical artifacts or when cell types (such as cancer cells) are defined by marker loss (e.g. T-cell Lymphomas).</p> <p><code>neg</code> : Same as <code>pos</code> but looks for negativity of the defined markers. </p> <p><code>allpos</code> : \"Allpos\" requires all defined markers to be positive. Unlike <code>pos</code>, it doesn't classify cells as <code>Likely-cellType</code>, but strictly annotates cells positive for all defined markers.</p> <p><code>allneg</code> : Same as <code>allpos</code> but looks for negativity of the defined markers. </p> <p><code>anypos</code> : \"Anypos\" requires only one of the defined markers to be positive. For example, to define macrophages, a cell could be designated as such if any of <code>CD68</code>, <code>CD163</code>, or <code>CD206</code> is positive.</p> <p><code>anyneg</code> : Same as <code>anyneg</code> but looks for negativity of the defined markers. </p> <pre><code>adata = cs.csPhenotype ( csObject=csObject,\n                            phenotype=phenotype,\n                            midpoint = 0.5,\n                            label=\"phenotype\",\n                            imageid='imageid',\n                            pheno_threshold_percent=None,\n                            pheno_threshold_abs=None,\n                            fileName=None,\n                            projectDir=projectDir)\n</code></pre> <pre><code>Phenotyping Immune\nPhenotyping ECAD+\n-- Subsetting ECAD+\nPhenotyping KI67+ ECAD+\n-- Subsetting Immune\nPhenotyping CD4+ T\nPhenotyping CD8+ T\nPhenotyping Non T CD4+ cells\nConsolidating the phenotypes across all groups\nModified csObject is stored at \"/Users/aj/Downloads/cspotExampleData/CSPOT/csPhenotype\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:262: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:262: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n</code></pre> <p>If you had provided <code>projectDir</code> the modified csObject would be stored in <code>CSPOT/csPhenotype/</code>, else, the object will be returned to memory.</p> <pre><code># check the identified phenotypes\nadata.obs['phenotype'].value_counts()\n</code></pre> <pre><code>KI67+ ECAD+    6159\nCD4+ T         5785\nCD8+ T          816\nName: phenotype, dtype: int64\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/ReviewQuickStartGuide/","title":"\ud83c\udfaf Review: Quick Start Guide","text":""},{"location":"Tutorials/md/ReviewQuickStartGuide/#getting-started-with-cspot","title":"\ud83d\udc0a Getting Started with CSPOT","text":"<p>Kindly note that CSPOT is not a plug-and-play solution. It's a framework that requires significant upfront investment of time from potential users for training and validating deep learning models, which can then be utilized in a plug-and-play manner for processing large volumes of similar multiplexed imaging data.  </p> <p>Before we set up CSPOT, we highly recommend using a environment manager like Conda. Using an environment manager like Conda allows you to create and manage isolated environments with specific package versions and dependencies. </p> <p>Download and Install the right conda based on the opertating system that you are using</p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#create-a-new-conda-environment","title":"Create a new conda environment","text":"<pre><code># use the terminal (mac/linux) and anaconda promt (windows) to run the following command\nconda create --name cspot -y python=3.9\nconda activate cspot\n</code></pre> <p>Install <code>cspot</code> within the conda environment.</p> <pre><code>pip install cspot\npip install notebook\n\n# after it is installed open the notebook by typing the following in the terminal:\njupyter notebook\n</code></pre>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#build-a-cspot-model","title":"Build a CSPOT Model","text":"<p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <pre><code>%%time\n# import packages in jupyter notebook (not needed for command line interface users)\nimport cspot as cs\n</code></pre> <pre><code>WARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nCPU times: user 3.8 s, sys: 2.74 s, total: 6.54 s\nWall time: 3.37 s\n</code></pre> <p>CSPOT auto generates subfolders and so always set a single folder as <code>projectDir</code> and cspot will use that for all subsequent steps. In this case we will set the downloaded sample data as our <code>projectDir</code>. My sample data is on my desktop as seen below.</p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#change-this-to-your-local-directory-where-the-example-data-has-been-saved","title":"Change this to your local directory where the example data has been saved","text":"<pre><code>projectDir = '/Users/aj/Downloads/cspotExampleData' #CHANGE\n</code></pre> <pre><code>%%time\n# set the working directory &amp; set paths to the example data\nimagePath = projectDir + '/image/exampleImage.tif'\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\nmarkerChannelMapPath = projectDir + '/markers.csv'\n</code></pre> <pre><code>CPU times: user 2 \u00b5s, sys: 0 ns, total: 2 \u00b5s\nWall time: 2.86 \u00b5s\n</code></pre>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#step-1-generate-thumbnails-for-training-data","title":"Step-1: Generate Thumbnails for Training Data","text":"<p>The first step would be to train a model to recognize the marker of interest. In this example the data contains 11 channels <code>DNA1, ECAD, CD45, CD4, CD3D, CD8A, CD45R, KI67</code> and as we are not interested in training a model to recognize DNA or background (<code>DNA1</code>), we will only need to generate training data for  <code>ECAD1, CD45, CD4, CD3D, CD8A &amp; KI67</code>. However for proof of concept, let us just train a model for <code>ECAD</code> and <code>CD3D</code>.</p> <p>To do so, the first step is to create examples of <code>postive</code> and <code>negative</code> examples for each marker of interest. To facilitate this process, we can use the <code>generateThumbnails</code> function in <code>CSPOT</code>. Under the hood the function auto identifies the cells that has high and low expression of the marker of interest and cuts out small thumbnails from the image.</p> <pre><code>%%time\ncs.generateThumbnails ( spatialTablePath=spatialTablePath, \n                        imagePath=imagePath, \n                        markerChannelMapPath=markerChannelMapPath,\n                        markers=[\"ECAD\", \"CD3D\"], \n                        markerColumnName='marker',\n                        channelColumnName='channel',\n                        transformation=True, \n                        maxThumbnails=100, \n                        random_state=0,\n                        localNorm=True, \n                        globalNorm=False,\n                        x_coordinate='X_centroid', \n                        y_coordinate='Y_centroid',\n                        percentiles=[2, 12, 88, 98], \n                        windowSize=64,\n                        projectDir=projectDir)\n</code></pre> <pre><code>Processing Marker: ECAD\nProcessing Marker: CD3D\nThumbnails have been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/Thumbnails\" to view results\nCPU times: user 2.83 s, sys: 6.38 s, total: 9.21 s\nWall time: 1.19 s\n</code></pre> <p>The output from the above function will be stored under <code>CSPOT/Thumbnails/</code>. </p> <p>Now that the thumbnails are generated, one would manually go through the <code>TruePos</code> folder and <code>TrueNeg</code> folder and move files around as necessary. If there are any truly negative thumbnails in the <code>TruePos</code> folder, move it to <code>PosToNeg</code> folder. Similarly, if there are any truly positive thumbnails in <code>TrueNeg</code> folder, move it to <code>NegToPos</code> folder. You will often notice that imaging artifacts are captured in the <code>TruePos</code> folder and there will also likely be a number of true positives in the <code>TrueNeg</code> folder as the field of view (64x64) is larger than what the program used to identify those thumbnails (just the centroids of single cells at the center of that thumbnail).    </p> <p>While you are manually sorting the postives and negative thumbnails, please keep in mind that you are looking for high-confident positives and high-confident negatives. It is absolutely okay to delete off majority of the thumbnails that you are not confident about. This infact makes it easy and fast as you are looking to only keep only thumbnails that are readily sortable.  </p> <p>Lastly, I generally use a whole slide image to generate these thumbnails as there will be enough regions with high expression and no expression of the marker of interest. If you look at the thumbnails of this dummy example, you will notice that most thumbnails of <code>TrueNeg</code> for <code>ECAD</code> does contain some level of <code>ECAD</code> as there is not enough regions to sample from. </p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#step-2-generate-masks-for-training-data","title":"Step-2: Generate Masks for Training Data","text":"<p>To train the deep learning model, in addition to the raw thumbnails a mask is needed. The mask lets the model know where the cell is located. Ideally one would manually draw on the thumbnails to locate where the positive cells are, however for the pupose of scalability we will use automated approaches to generate the Mask for us. The following function will generate the mask and split the data into <code>training, validation and test</code> that can be directly fed into the deep learning algorithm.</p> <pre><code>%%time\nthumbnailFolder = [projectDir + '/CSPOT/Thumbnails/CD3D',\n                   projectDir + '/CSPOT/Thumbnails/ECAD']\n\n# The function accepts the four pre-defined folders. If you had renamed them, please change it using the parameter below.\n# If you had deleted any of the folders and are not using them, replace the folder name with `None` in the parameter.\ncs.generateTrainTestSplit ( thumbnailFolder, \n                            projectDir=projectDir,\n                            file_extension=None,\n                            TruePos='TruePos', NegToPos='NegToPos',\n                            TrueNeg='TrueNeg', PosToNeg='PosToNeg')\n</code></pre> <pre><code>Processing: CD3D\nProcessing: ECAD\nTraining data has been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData\" to view results\nCPU times: user 193 ms, sys: 276 ms, total: 469 ms\nWall time: 218 ms\n</code></pre> <p>If you head over to <code>CSPOT/TrainingData/</code>, you will notice that each of the supplied marker above will have a folder with the associated <code>training, validataion and test</code> data that is required by the deep-learning algorithm to generate the model. </p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#step-3-train-the-cspot-model","title":"Step-3: Train the CSPOT Model","text":"<p>The function trains a deep learning model for each marker in the provided training data. To train the <code>cspotModel</code>, simply direct the function to the <code>TrainingData</code> folder. To train only specific models, specify the folder names using the <code>trainMarkers</code> parameter. The 'outputDir' remains constant and the program will automatically create subfolders to save the trained models.</p> <pre><code>%%time\ntrainingDataPath = projectDir + '/CSPOT/TrainingData'\n\ncs.csTrain(trainingDataPath=trainingDataPath,\n               projectDir=projectDir,\n               trainMarkers=None,\n               artefactPath=None,\n               imSize=64,\n               nChannels=1,\n               nClasses=2,\n               nExtraConvs=0,\n               nLayers=3,\n               featMapsFact=2,\n               downSampFact=2,\n               ks=3,\n               nOut0=16,\n               stdDev0=0.03,\n               batchSize=16,\n               epochs=1)\n</code></pre> <pre><code>WARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData/CD3D/training\nTraining for 8 steps\nFound 124 training images\nFound 42 validation images\nFound 41 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 15.0 and 0.0 for global max and min intensities.\nClass balance ratio is 15.352093476144109\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nstep 00000, e: 0.526497, epoch: 1\nModel saved in file: /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nstep 00001, e: 0.480866, epoch: 1\nstep 00002, e: 0.533251, epoch: 1\nstep 00003, e: 0.519985, epoch: 1\nstep 00004, e: 0.501891, epoch: 1\nstep 00005, e: 0.507230, epoch: 1\nstep 00006, e: 0.465878, epoch: 2\nstep 00007, e: 0.505098, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/CD3D/model.ckpt\nModel restored.\n/Users/aj/Downloads/cspotExampleData/CSPOT/TrainingData/ECAD/training\nTraining for 9 steps\nFound 142 training images\nFound 48 validation images\nFound 47 test images\nOf these, 0 are artefact training images\n and  0 artefact validation images\nUsing 0 and 1 for mean and standard deviation.\nsaving data\nsaving data\nUsing 76.0 and 0.0 for global max and min intensities.\nClass balance ratio is 5.747231179027057\nstep 00000, e: 0.516712, epoch: 1\nModel saved in file: /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nstep 00001, e: 0.512037, epoch: 1\nstep 00002, e: 0.517852, epoch: 1\nstep 00003, e: 0.558351, epoch: 1\nstep 00004, e: 0.508853, epoch: 1\nstep 00005, e: 0.547349, epoch: 1\nstep 00006, e: 0.506477, epoch: 1\nstep 00007, e: 0.537210, epoch: 2\nstep 00008, e: 0.486268, epoch: 2\nsaving data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel/ECAD/model.ckpt\nModel restored.\nCSPOT Models have been generated, head over to \"/Users/aj/Downloads/cspotExampleData/CSPOT/cspotModel\" to view results\nCPU times: user 8.7 s, sys: 1.77 s, total: 10.5 s\nWall time: 3.72 s\n</code></pre>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#step-4-predict-on-a-new-image","title":"Step-4: Predict on a new image","text":"<p>Now that the model is trained, we can predict the expression of the trained markers on a new image that the model has not seen before. </p> <pre><code>%%time\n# cspotpredict related paths\ncsModelPath = projectDir + '/manuscriptModels/'\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n</code></pre> <pre><code>CPU times: user 1e+03 ns, sys: 1 \u00b5s, total: 2 \u00b5s\nWall time: 3.81 \u00b5s\n</code></pre> <pre><code>%%time\n# Run the pipeline (For function specific parameters, check the documentation)\ncs.csPipeline(   \n                    # parameters for cspotPredict function\n                    imagePath=imagePath,\n                    csModelPath=csModelPath,\n                    markerChannelMapPath=markerChannelMapPath,\n\n                    # parameters for cspotCScore function\n                    segmentationMaskPath=segmentationPath,\n\n                    # parameters for cspotObject function\n                    spatialTablePath=spatialTablePath,\n\n                    # common parameters\n                    verbose=False,\n                    projectDir=projectDir)\n</code></pre> <pre><code>loading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/manuscriptModels/ECAD/model.ckpt\nModel restored.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\nInference...\nloading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Downloads/cspotExampleData/manuscriptModels/CD3D/model.ckpt\nModel restored.\nInference...\nCPU times: user 1min 10s, sys: 25.5 s, total: 1min 36s\nWall time: 15.9 s\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/cspot.py:393: RuntimeWarning: invalid value encountered in divide\n  below_midpoint = (below_midpoint - min_below) / range_below\n</code></pre> <p>Head over to <code>CSPOT/cspotOutput</code> to view results</p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#visualize-the-results","title":"Visualize the results","text":"<p>Let us visualize the marker postivity of three markers using a helper plotting function provided within CSPOT.</p> <pre><code>%%time\ncsObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n# Plot image to console\ncs.scatterPlot(csObject,\n            markers=['ECAD', 'CD8A', 'CD45'],\n            poscellsColor='#78290f',\n            negcellsColor='#e5e5e5',\n            s=3,\n            ncols=3,\n            dpi=90,\n            figsize=(4, 4),\n            outputDir=None,\n            outputFileName='cspotplot.png')\n</code></pre> <pre><code>CPU times: user 383 ms, sys: 4.62 ms, total: 388 ms\nWall time: 197 ms\n</code></pre> <p></p>"},{"location":"Tutorials/md/ReviewQuickStartGuide/#step-5-cspot-phenotyping","title":"Step-5: CSPOT Phenotyping","text":"<p>Assign phenotypes to each cell. Clustering data may not always be ideal, so we developed a cell type assignment algorithm that does a hierarchical assignment process iteratively.</p> <p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <pre><code>%%time\n# import packages\nimport pandas as pd\n</code></pre> <pre><code>CPU times: user 3 \u00b5s, sys: 1e+03 ns, total: 4 \u00b5s\nWall time: 4.05 \u00b5s\n</code></pre> <p>We need <code>two</code> basic inputs to perform phenotyping with CSPOT - The cspot Object - A Phenotyping workflow based on prior knowledge</p> <pre><code>%%time\n# Path to the CSPOT Object\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n</code></pre> <pre><code>CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 \u00b5s\nWall time: 4.29 \u00b5s\n</code></pre> <pre><code>%%time\n# load the phenotyping workflow\nphenotype = pd.read_csv(str(projectDir) + '/phenotype_workflow.csv')\n# view the table:\nphenotype.style.format(na_rep='')\n</code></pre> <pre><code>CPU times: user 46.1 ms, sys: 2.16 ms, total: 48.3 ms\nWall time: 16.9 ms\n</code></pre> Unnamed: 0 Unnamed: 1 ECAD CD45 CD4 CD3D CD8A KI67 0 all Immune anypos anypos anypos anypos 1 all ECAD+ pos 2 ECAD+ KI67+ ECAD+ pos 3 Immune CD4+ T allpos allpos 4 Immune CD8+ T allpos allpos 5 Immune Non T CD4+ cells pos neg <p>As it can be seen from the table above, (1) The <code>first column</code> has to contain the cell that are to be classified. (2) The <code>second column</code> indicates the phenotype a particular cell will be assigned if it satifies the conditions in the row. (3) <code>Column three</code> and onward represent protein markers. If the protein marker is known to be expressed for that cell type, then it is denoted by either <code>pos</code>, <code>allpos</code>. If the protein marker is known to not express for a cell type it can be denoted by <code>neg</code>, <code>allneg</code>. If the protein marker is irrelevant or uncertain to express for a cell type, then it is left empty. <code>anypos</code> and <code>anyneg</code> are options for using a set of markers and if any of the marker is positive or negative, the cell type is denoted accordingly.</p> <p>To give users maximum flexibility in identifying desired cell types, we have implemented various classification arguments as described above for strategical classification. They include</p> <ul> <li>allpos</li> <li>allneg</li> <li>anypos</li> <li>anyneg</li> <li>pos</li> <li>neg</li> </ul> <p><code>pos</code> : \"Pos\" looks for cells positive for a given marker. If multiple markers are annotated as <code>pos</code>, all must be positive to denote the cell type. For example, a Regulatory T cell can be defined as <code>CD3+CD4+FOXP3+</code> by passing <code>pos</code> to each marker. If one or more markers don't meet the criteria (e.g. CD4-), the program will classify it as <code>Likely-Regulatory-T cell</code>, pending user confirmation. This is useful in cases of technical artifacts or when cell types (such as cancer cells) are defined by marker loss (e.g. T-cell Lymphomas).</p> <p><code>neg</code> : Same as <code>pos</code> but looks for negativity of the defined markers. </p> <p><code>allpos</code> : \"Allpos\" requires all defined markers to be positive. Unlike <code>pos</code>, it doesn't classify cells as <code>Likely-cellType</code>, but strictly annotates cells positive for all defined markers.</p> <p><code>allneg</code> : Same as <code>allpos</code> but looks for negativity of the defined markers. </p> <p><code>anypos</code> : \"Anypos\" requires only one of the defined markers to be positive. For example, to define macrophages, a cell could be designated as such if any of <code>CD68</code>, <code>CD163</code>, or <code>CD206</code> is positive.</p> <p><code>anyneg</code> : Same as <code>anyneg</code> but looks for negativity of the defined markers. </p> <pre><code>%%time\nadata = cs.csPhenotype ( csObject=csObject,\n                            phenotype=phenotype,\n                            midpoint = 0.5,\n                            label=\"phenotype\",\n                            imageid='imageid',\n                            pheno_threshold_percent=None,\n                            pheno_threshold_abs=None,\n                            fileName=None,\n                            projectDir=projectDir)\n</code></pre> <pre><code>Phenotyping Immune\nPhenotyping ECAD+\n-- Subsetting ECAD+\nPhenotyping KI67+ ECAD+\n-- Subsetting Immune\nPhenotyping CD4+ T\nPhenotyping CD8+ T\nPhenotyping Non T CD4+ cells\nConsolidating the phenotypes across all groups\nModified csObject is stored at \"/Users/aj/Downloads/cspotExampleData/CSPOT/csPhenotype\nCPU times: user 243 ms, sys: 9.24 ms, total: 252 ms\nWall time: 91.9 ms\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:261: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/csPhenotype.py:261: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  allpos_score['score'] = allpos_score.max(axis=1)\n</code></pre> <p>If you had provided <code>projectDir</code> the modified csObject would be stored in <code>CSPOT/csPhenotype/</code>, else, the object will be returned to memory.</p> <pre><code>%%time\n# check the identified phenotypes\nadata.obs['phenotype'].value_counts()\n</code></pre> <pre><code>CPU times: user 3.43 ms, sys: 513 \u00b5s, total: 3.95 ms\nWall time: 907 \u00b5s\n\n\n\n\n\nKI67+ ECAD+    6159\nCD4+ T         5785\nCD8+ T          816\nName: phenotype, dtype: int64\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/","title":"\ud83c\udfaf Run the CSPOT Prediction Algorithm on new images","text":"<p>Download the executable notebook and trained models. </p> <p>For the purpose of this tutorial, we will use the <code>manuscriptModels</code> as opposed to the models that you trained in the previous tutorial.</p> <p>Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <p>Running the CSPOT Prediction Algorithm involves the following steps: - Run the <code>csPredict</code> function on a new image. It will produce an image with probability masks - Run the <code>generateCSScore</code> function on the probability masks to generate the <code>cspotScores</code> - Run the <code>csObject</code> to create an anndata object with the <code>cspotScores</code> and pre-computed <code>single-cell table</code> - Lastly, run <code>cspot</code>  on the csObject</p> <p>Note: To make things easy, all of the above steps can be run with a single command <code>csPipeline</code>. Typically, in production settings, <code>csPipeline</code> would be utilized, whereas step-by-step analysis would be employed for troubleshooting, model validation, and similar tasks that necessitate greater granularity or control.</p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#single-command-execution-of-the-entire-cspot-prediction-algorithm-using-the-cspipeline-function","title":"Single command execution of the entire CSPOT Prediction Algorithm using the <code>csPipeline</code> function","text":"<pre><code># import packages in jupyter notebook (not needed for command line interface users)\nimport cspot as cs\n</code></pre> <pre><code># Path to all the files that are necessary files for running the CSPOT Prediction Algorithm (broken down based on sub functions)\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# cspotPredict related paths\nimagePath = projectDir + '/image/exampleImage.tif'\nmarkerChannelMapPath = projectDir + '/markers.csv'\ncsModelPath = projectDir + '/manuscriptModels/'\n\n# Generate generatecspotScore related paths\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\n\n# cspotObject related paths\nspatialTablePath = projectDir + '/quantification/exampleSpatialTable.csv'\n</code></pre> <pre><code># Run the pipeline (For function specific parameters, check the documentation)\ncs.csPipeline(   \n                    # parameters for cspotPredict function\n                    imagePath=imagePath,\n                    csModelPath=csModelPath,\n                    markerChannelMapPath=markerChannelMapPath,\n\n                    # parameters for generatecspotScore function\n                    segmentationMaskPath=segmentationPath,\n\n                    # parameters for cspotObject function\n                    spatialTablePath=spatialTablePath,\n\n                    # parameters to run cspot function\n                    # ..\n\n                    # common parameters\n                    verbose=False,\n                    projectDir=projectDir)\n</code></pre> <pre><code>loading data\nWARNING:tensorflow:From /Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/manuscriptModels/ECAD/model.ckpt\nModel restored.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\nInference...\nloading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/manuscriptModels/CD3D/model.ckpt\nModel restored.\nInference...\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/cspot.py:383: RuntimeWarning: invalid value encountered in divide\n  below_midpoint = (below_midpoint - min_below) / range_below\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csPipeline.py \\\n                --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif \\\n                --csModelPath /Users/aj/Documents/cspotExampleData/CSPOT/cspotModel/ \\\n                --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv \\\n                --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif \\\n                --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv \\\n                --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>Head over to <code>CSPOT/cspotOutput</code> to view results</p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#visualize-the-results","title":"Visualize the results","text":"<p>Let us visualize the marker postivity of three markers using a helper plotting function provided within CSPOT.</p> <pre><code>csObject = projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad'\n\n# Plot image to console\ncs.scatterPlot(csObject,\n            markers=['ECAD', 'CD8A', 'CD45'],\n            poscellsColor='#78290f',\n            negcellsColor='#e5e5e5',\n            s=3,\n            ncols=3,\n            dpi=90,\n            figsize=(4, 4),\n            outputDir=None,\n            outputFileName='cspotplot.png')\n</code></pre> <pre><code>/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/anndata/_core/anndata.py:121: ImplicitModificationWarning: Transforming to str index.\n  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n</code></pre> <p></p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-by-step-execution-of-the-cspot-prediction-algorithm-in-contrast-to-pipeline","title":"Step by step execution of the CSPOT Prediction Algorithm (in contrast to pipeline)","text":"<pre><code># Path to all the files that are necessary files for running csPredict\nprojectDir = '/Users/aj/Documents/cspotExampleData'\n\n# csPredict related paths\nimagePath = projectDir + '/image/exampleImage.tif'\nmarkerChannelMapPath = projectDir + '/markers.csv'\ncsModelPath = projectDir + '/manuscriptModels/'\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-1-apply-the-generated-models-on-the-image-of-interest-pixel-level","title":"Step-1: Apply the generated Models on the Image of interest (Pixel Level)","text":"<p>The function <code>csPredict</code> is employed to make predictions about the expression of a specified marker on cells in new images using the models generated by <code>csTrain</code>. This calculation is done at the pixel level, resulting in an output image where the number of channels corresponds to the number of models applied to the input image. The parameter <code>markerChannelMapPath</code> is used to associate the image channel number with the relevant model to be applied.</p> <pre><code>cs.csPredict( imagePath=imagePath,\n                 csModelPath=csModelPath,\n                 projectDir=projectDir,\n                 markerChannelMapPath=markerChannelMapPath, \n                 markerColumnName='marker', \n                 channelColumnName='channel', \n                 modelColumnName='cspotmodel')\n</code></pre> <pre><code>Running CSPOT model ECAD on channel 2 corresponding to marker ECAD\nloading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/manuscriptModels/ECAD/model.ckpt\nModel restored.\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:137: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  bn = tf.nn.leaky_relu(tf.layers.batch_normalization(c00+shortcut, training=UNet2D.tfTraining))\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:159: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  lbn = tf.nn.leaky_relu(tf.layers.batch_normalization(\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:162: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n  return tf.layers.dropout(lbn, 0.15, training=UNet2D.tfTraining)\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:224: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  tf.layers.batch_normalization(tf.nn.conv2d(cc, luXWeights2, strides=[1, 1, 1, 1], padding='SAME'),\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/cspot/UNet.py:245: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n  return tf.layers.batch_normalization(\n\n\nInference...\nRunning CSPOT model CD3D on channel 5 corresponding to marker CD3D\nloading data\nloading data\nloading data\nINFO:tensorflow:Restoring parameters from /Users/aj/Documents/cspotExampleData/manuscriptModels/CD3D/model.ckpt\nModel restored.\nInference...\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csPredict.py \\\n            --imagePath /Users/aj/Documents/cspotExampleData/image/exampleImage.tif \\\n            --csModelPath /Users/aj/Documents/cspotExampleData/manuscriptModels \\\n            --projectDir /Users/aj/Documents/cspotExampleData \\\n            --markerChannelMapPath /Users/aj/Documents/cspotExampleData/markers.csv\n</code></pre></p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-2-calculate-the-cspotscore-single-cell-level","title":"Step-2: Calculate the cspotScore (Single-cell Level)","text":"<p>After calculating pixel-level probability scores, the next step is to aggregate them to the single-cell level. This can be done by computing the mean or median probability scores using pre-computed segmentation masks. The marker names, if available, should already be included in the probabilityMask image. If the marker names are lost due to file manipulation, the user can provide them through the markerNames parameter.</p> <pre><code># Path to all the files that are necessary files for running generateCSScore\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\nprobabilityMaskPath = projectDir + '/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif'\n</code></pre> <pre><code>cs.generateCSScore(probabilityMaskPath=probabilityMaskPath,\n                      segmentationMaskPath=segmentationPath,\n                      feature='median',\n                      projectDir=projectDir)\n</code></pre> <pre><code>Quantifying the probability masks\ncsScore is ready, head over to/Users/aj/Documents/cspotExampleData/CSPOT/csScore\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python generateCSScore.py \\\n            --probabilityMaskPath /Users/aj/Documents/cspotExampleData/CSPOT/csPredict/exampleImage_cspotPredict.ome.tif \\\n            --segmentationMaskPath /Users/aj/Documents/cspotExampleData/segmentation/exampleSegmentationMask.tif \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>If you head over to <code>CSPOT/csScore/</code>, you will find the <code>.csv</code> file with the csScores for every cell.</p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-3-create-a-cspot-object","title":"Step-3: Create a CSPOT object","text":"<p>We'll use the anndata framework to create a cspot object to store all information in one file, making it easier to keep track of intermediate files generated in subsequent steps.  This helps streamline the data analysis process and reduces the risk of losing or misplacing information.</p> <pre><code># Path to all the files that are necessary files for running csObject function\nsegmentationPath = projectDir + '/segmentation/exampleSegmentationMask.tif'\ncsScorePath = projectDir + '/CSPOT/csScore/exampleImage_cspotPredict.ome.csv'\n</code></pre> <pre><code># please note that there are a number of defaults in the below function that assumes certain structure within the spatialTable.\n# Please confirm it is similar with user data or modifiy the parameters accordingly\n# check out the documentation for further details\nadata = cs.csObject (spatialTablePath=spatialTablePath,\n                        csScorePath=csScorePath,\n                        CellId='CellID',\n                        uniqueCellId=True,\n                        split='X_centroid',\n                        removeDNA=True,\n                        remove_string_from_name=None,\n                        log=True,\n                        dropMarkers=None,\n                        projectDir=projectDir)\n</code></pre> <pre><code>Loading exampleSpatialTable.csv\nCSPOT Object has been created, head over to/Users/aj/Documents/cspotExampleData/CSPOT/csObject\" to view results\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python csObject.py \\\n            --spatialTablePath /Users/aj/Documents/cspotExampleData/quantification/exampleSpatialTable.csv \\\n            --csScorePath /Users/aj/Documents/cspotExampleData/CSPOT/csScore/exampleImage_cspotPredict.ome.csv \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>If you had provided <code>projectDir</code> the object would be stored in <code>CSPOT/csObject/</code>, else, the object will be returned to memory</p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-4-run-the-final-cspot-algorithm","title":"Step-4: Run the final CSPOT Algorithm","text":"<p>The <code>cspot</code> algorithm is ready to run after pre-processing. To get optimal results, consider adjusting the following parameters:</p> <ol> <li>The <code>minAbundance</code> parameter determines the minimum percentage of a marker's abundance to consider it a failure.</li> <li>It is suggested to drop background markers with the <code>dropMarkers</code> option as they can interfere with classifiers.</li> <li><code>RobustScale</code>: Scaling the data before training the classifier model has been shown to improve results. However, in our experience a simple log transformation was found to be work best. </li> </ol> <pre><code># Path to all the files that are necessary files for running cspot function\ncsObject = projectDir + '/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad'\n</code></pre> <pre><code>adata = cs.cspot ( csObject=csObject,\n                    csScore='csScore',\n                    minAbundance=0.005,\n                    percentiles=[1, 20, 80, 99],\n                    dropMarkers = None,\n                    RobustScale=False,\n                    log=True,\n                    x_coordinate='X_centroid',\n                    y_coordinate='Y_centroid',\n                    imageid='imageid',\n                    random_state=0,\n                    rescaleMethod='sigmoid',\n                    label='cspotOutput',\n                    verbose=False,\n                   projectDir=projectDir)\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python cspot.py \\\n            --csObject /Users/aj/Documents/cspotExampleData/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>If <code>projectDir</code> is provided, modified anndata object with results (stored in <code>adata.uns['cspotOutput']</code>) will be saved in <code>CSPOT/cspotOutput/</code>. The cspot-scaled data (stored in <code>adata.X</code>) considers cells above 0.5 as positive and below 0.5 as negative for the marker.</p>"},{"location":"Tutorials/md/RunCSPOTAlgorithm/#step-5-merge-multiple-cspot-objects-optional","title":"Step-5: Merge multiple CSPOT objects (optional)","text":"<p>Use <code>mergecspotObject</code> to combine multiple csObjects into a dataset for analysis when multiple images need to be analyzed.</p> <p>Note that merging csObjects requires merging multiple sections, not simple concatenation. Use parameters to specify which parts of the csObjects to merge.</p> <pre><code># set the working directory &amp; set paths to the example data\ncsObjects = [projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad',\n            projectDir + '/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad']\n</code></pre> <pre><code># For this tutorial, supply the same cspotObject twice for merging, but multiple cspotObjects can be merged in ideal conditions.\nadata = cs.mergecsObject ( csObjects=csObjects,\n                              fileName='mergedcspotObject',\n                              layers=['preProcessed'],\n                              uns= ['cspotOutput','csScore'],\n                              projectDir=projectDir)\n</code></pre> <pre><code>Extracting data\nExtracting data from: exampleSpatialTable\nExtracting data from: exampleSpatialTable\nGiven csObjects have been merged, head over to \"/Users/aj/Documents/cspotExampleData/CSPOT/mergedcsObject\" to view results\n\n\n/Users/aj/miniconda3/envs/cspot/lib/python3.9/site-packages/anndata/_core/anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n  utils.warn_names_duplicates(\"obs\")\n</code></pre> <p>Same function if the user wants to run it via Command Line Interface <pre><code>python mergecsObject.py \\\n            --csObjects /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad /Users/aj/Documents/cspotExampleData/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n            --projectDir /Users/aj/Documents/cspotExampleData\n</code></pre></p> <p>If <code>projectDir</code> is provided, modified anndata object with results will be saved in <code>CSPOT/mergedcsObject/</code>.</p> <pre><code># this tutorial ends here. Move to the Phenotyping cells Tutorial\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/","title":"\ud83c\udfaf Run CSPOT with Docker","text":"<ol> <li>Install Docker on your local machine if you haven't already done so.</li> <li>Open a terminal or command prompt on your machine.</li> </ol>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#download-cspot-from-docker-hub","title":"Download CSPOT from Docker Hub","text":"<pre><code>docker pull nirmallab/cspot:latest\n</code></pre> <p>Run Docker Running cspot via docker follows the same principles as running cspot via Command Line Interface. </p> <p>If you are comfortable using Docker and would like to execute the commands in your preferred way, please feel free to do so. However, if you are new to Docker and would like step-by-step instructions, please follow the tutorial below.</p> <p>Download the sample data. Please keep in mind that the sample data is used for demonstration purposes only and has been simplified and reduced in size. It is solely intended for educational purposes on how to execute <code>cspot</code> and will not yeild any meaningful results.</p> <p>The purpose of this tutorial is solely to demonstrate how to run cspot using Docker. If you require detailed explanations of each step, please refer to the other tutorials.  Please note the following is for running docker on a mac. The semantics will defer if you are using windows terminal or powershell (with regards to declaring projectDir and so on)</p>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-1-generate-thumbnails-for-training-data","title":"Step-1: Generate Thumbnails for Training Data","text":"<p>To use your own data, it is recommended to follow the same folder structure as the sample data. However, if that is not possible, you should place all the required data within a single folder. This is because we need to tell Docker where to find all the raw data, and specifying a single directory makes it easier to manage the data within the container.</p> <pre><code># specify the directory where the sample data lives and Run the docker command\nexport projectDir=\"/Users/aj/Documents/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                nirmallab/cspot:latest \\\n                python /app/generateThumbnails.py \\\n                --spatialTablePath $projectDir/quantification/exampleSpatialTable.csv \\\n                --imagePath $projectDir/image/exampleImage.tif \\\n                --markerChannelMapPath $projectDir/markers.csv \\\n                --markers ECAD CD3D \\\n                --maxThumbnails 100 \\\n                --projectDir $projectDir\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-2-generate-masks-for-training-data","title":"Step-2: Generate Masks for Training Data","text":"<pre><code>export projectDir=\"/Users/aj/Documents/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                nirmallab/cspot:latest \\\n                python /app/generateTrainTestSplit.py \\\n                --thumbnailFolder $projectDir/CSPOT/Thumbnails/CD3D $projectDir/CSPOT/Thumbnails/ECAD\\\n                --projectDir $projectDir\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-3-train-the-cspot-model","title":"Step-3: Train the CSPOT Model","text":"<pre><code>export projectDir=\"/Users/aj/Documents/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                nirmallab/cspot:latest \\\n                python /app/csTrain.py \\\n                --trainingDataPath $projectDir/CSPOT/TrainingData \\\n                --projectDir $projectDir \\\n                --epochs=1\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-4-run-the-cspot-algorithm","title":"Step-4: Run the CSPOT Algorithm","text":"<p>Note that the <code>markers.csv</code> requests to predict on all markers and so replace the current <code>cspotModel</code> folder with these models that are available for download.   </p> <p>To keep things simple, we're running the entire pipeline with a single command instead of going through the step-by-step process. Nevertheless, you can apply the same principles to each function separately.</p> <pre><code>export projectDir=\"/Users/aj/Documents/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                nirmallab/cspot:latest \\\n                python /app/csPipeline.py \\\n                --imagePath $projectDir/image/exampleImage.tif \\\n                --csModelPath $projectDir/CSPOT/cspotModel/ \\\n                --markerChannelMapPath $projectDir/markers.csv \\\n                --segmentationMaskPath $projectDir/segmentation/exampleSegmentationMask.tif \\\n                --spatialTablePath $projectDir/quantification/exampleSpatialTable.csv \\\n                --projectDir $projectDir \\\n                --verbose False\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-5-merge-multiple-cspot-objects-optional","title":"Step-5: Merge multiple CSPOT objects (optional)","text":"<pre><code>export projectDir=\"/Users/aj/Documents/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                nirmallab/cspot:latest \\\n                python /app/mergecsObject.py \\\n                --csObjects $projectDir/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad $projectDir/CSPOT/cspotOutput/exampleImage_cspotPredict.ome.h5ad \\\n                --projectDir $projectDir\n</code></pre>"},{"location":"Tutorials/md/RunCSPOTwithDocker/#step-6-cspot-phenotyping","title":"Step-6: CSPOT Phenotyping","text":"<pre><code>export projectDir=\"/Users/aj/Dcouments/cspotExampleData\"\ndocker run -it --mount type=bind,source=$projectDir,target=/$projectDir \\\n                            nirmallab/cspot:latest \\\n                            python /app/csPhenotype.py \\\n                            --csObject $projectDir/CSPOT/csObject/exampleImage_cspotPredict.ome.h5ad \\\n                            --phenotype $projectDir/phenotype_workflow.csv \\\n                            --projectDir $projectDir\n</code></pre> <pre><code># Tutorial ends here. Refer to other tutorials for detailed explanation of each step!\n</code></pre> <pre><code>\n</code></pre>"}]}